<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maciej Ceglowski on Perl.com - programming news, code and culture</title>
    <link>http://localhost:1313/authors/maciej-ceglowski/</link>
    <description>Recent content in Maciej Ceglowski on Perl.com - programming news, code and culture</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Apr 2004 00:00:00 -0800</lastBuildDate>
    <atom:link href="/authors/maciej-ceglowski/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Using Bloom Filters</title>
      <link>http://localhost:1313/pub/2004/04/08/bloom_filters.html/</link>
      <pubDate>Thu, 08 Apr 2004 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2004/04/08/bloom_filters.html/</guid>
      <description>

&lt;p&gt;Anyone who has used Perl for any length of time is familiar with the lookup hash, a handy idiom for doing existence tests:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foreach my $e ( @things ) { $lookup{$e}++ }

sub check {
    my ( $key ) = @_;
    print &amp;quot;Found $key!&amp;quot; if exists( $lookup{ $key } );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As useful as the lookup hash is, it can become unwieldy for very large lists or in cases where the keys themselves are large. When a lookup hash grows too big, the usual recourse is to move it to a database or flat file, perhaps keeping a local cache of the most frequently used keys to improve performance.&lt;/p&gt;

&lt;p&gt;Many people don&amp;rsquo;t realize that there is an elegant alternative to the lookup hash, in the form of a venerable algorithm called a &lt;em&gt;Bloom filter&lt;/em&gt;. Bloom filters allow you to perform membership tests in just a fraction of the memory you&amp;rsquo;d need to store a full list of keys, so you can avoid the performance hit of having to use a disk or database to do your lookups. As you might suspect, the savings in space comes at a price: you run an adjustable risk of false positives, and you can&amp;rsquo;t remove a key from a filter once you&amp;rsquo;ve added it in. But in the many cases where those constraints are acceptable, a Bloom filter can make a useful tool.&lt;/p&gt;

&lt;p&gt;For example, imagine you run a high-traffic online music store along the lines of iTunes, and you want to minimize the stress on your database by only fetching song information when you know the song exists in your collection. You can build a Bloom filter at startup, and then use it as a quick existence check before trying to perform an expensive fetching operation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use Bloom::Filter;

my $filter = Bloom::Filter-&amp;gt;new( error_rate =&amp;gt; 0.01, capacity =&amp;gt; $SONG_COUNT );
open my $fh, &amp;quot;enormous_list_of_titles.txt&amp;quot; or die &amp;quot;Failed to open: $!&amp;quot;;

while (&amp;lt;$fh&amp;gt;) {
    chomp;
    $filter-&amp;gt;add( $_ );
}

sub lookup_song {
    my ( $title ) = @_;
    return unless $filter-&amp;gt;check( $title );
    return expensive_db_query( $title ) or undef;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, there&amp;rsquo;s a 1% chance that the test will give a false positive, which means the program will perform the expensive fetch operation and eventually return a null result. Still, you&amp;rsquo;ve managed to avoid the expensive query 99% of the time, using only a fraction of the memory you would have needed for a lookup hash. As we&amp;rsquo;ll see further on, a filter with a 1% error rate requires just under 2 bytes of storage per key. That&amp;rsquo;s far less memory than you would need for a lookup hash.&lt;/p&gt;

&lt;p&gt;Bloom filters are named after Burton Bloom, who first described them in a 1970 paper entitled &lt;a href=&#34;http://portal.acm.org/citation.cfm?id=362692&amp;amp;dl=ACM&amp;amp;coll=portal&#34;&gt;Space/time trade-offs in hash coding with allowable errors&lt;/a&gt;. In those days of limited memory, Bloom filters were prized primarily for their compactness; in fact, one of their earliest applications was in spell checkers. However, there are less obvious features of the algorithm that make it especially well-suited to applications in social software.&lt;/p&gt;

&lt;p&gt;Because Bloom filters use one-way hashing to store their data, it is impossible to reconstruct the list of keys in a filter without doing an exhaustive search of the keyspace. Even that is unlikely to be of much help, since the false positives from an exhaustive search will swamp the list of real keys. Bloom filters therefore make it possible to share information about what you have without broadcasting a complete list of it to the world. For that reason, they may be especially valuable in peer-to-peer applications, where both size and privacy are important constraints.&lt;/p&gt;

&lt;h3 id=&#34;span-id-how-bloom-filters-work-how-bloom-filters-work-span&#34;&gt;&lt;span id=&#34;how_bloom_filters_work&#34;&gt;How Bloom Filters Work&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;A Bloom filter consists of two components: a set of &lt;code&gt;k&lt;/code&gt; hash functions and a bit vector of a given length. We choose the length of the bit vector and the number of hash functions depending on how many keys we want to add to the set and how high an error rate we are willing to put up with &amp;ndash; more on that a little bit further on.&lt;/p&gt;

&lt;p&gt;All of the hash functions in a Bloom filter are configured so that their range matches the length of the bit vector. For example, if a vector is 200 bits long, the hash functions return a value between 1 and 200. It&amp;rsquo;s important to use high-quality hash functions in the filter to guarantee that output is equally distributed over all possible values &amp;ndash; &amp;ldquo;hot spots&amp;rdquo; in a hash function would increase our false-positive rate.&lt;/p&gt;

&lt;p&gt;To enter a key into a Bloom filter, we run it through each one of the &lt;code&gt;k&lt;/code&gt; hash functions and treat the result as an offset into the bit vector, turning on whatever bit we find at that position. If the bit is already set, we leave it on. There&amp;rsquo;s no mechanism for turning bits off in a Bloom filter.&lt;/p&gt;

&lt;p&gt;As an example, let&amp;rsquo;s take a look at a Bloom filter with three hash functions and a bit vector of length 14. We&amp;rsquo;ll use spaces and asterisks to represent the bit vector, to make it easier to follow along. As you might expect, an empty Bloom filter starts out with all the bits turned off, as seen in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_1.gif&#34; alt=&#34;an empty Bloom filter&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 1. An empty Bloom filter.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s now add the string &lt;code&gt;apples&lt;/code&gt; into our filter. To do so, we hash &lt;code&gt;apples&lt;/code&gt; through each of our three hash functions and collect the output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hash1(&amp;quot;apples&amp;quot;) = 3
hash2(&amp;quot;apples&amp;quot;) = 12
hash3(&amp;quot;apples&amp;quot;) = 11
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we turn on the bits at the corresponding positions in the vector &amp;ndash; in this case bits 3, 11, and 12, as shown in Figure 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_2.gif&#34; alt=&#34;a Bloom filter with three bits enabled&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 2. A Bloom filter with three bits enabled.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To add another key, such as &lt;code&gt;plums&lt;/code&gt;, we repeat the hashing procedure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hash1(&amp;quot;plums&amp;quot;) = 11
hash2(&amp;quot;plums&amp;quot;) = 1
hash3(&amp;quot;plums&amp;quot;) = 8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And again turn on the appropriate bits in the vector, as shown with highlights in Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_3.gif&#34; alt=&#34;the Bloom filter after adding a second key&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 3. The Bloom filter after adding a second key.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Notice that the bit at position 11 was already turned on &amp;ndash; we had set it when we added &lt;code&gt;apples&lt;/code&gt; in the previous step. Bit 11 now does double duty, storing information for both &lt;code&gt;apples&lt;/code&gt; and &lt;code&gt;plums&lt;/code&gt;. As we add more keys, it may store information for some of them as well. This overlap is what makes Bloom filters so compact &amp;ndash; any one bit may be encoding multiple keys simultaneously. This overlap also means that you can never take a key out of a filter, because you have no guarantee that the bits you turn off don&amp;rsquo;t carry information for other keys. If we tried to remove &lt;code&gt;apples&lt;/code&gt; from the filter by reversing the procedure we used to add it in, we would inadvertently turn off one of the bits that encodes &lt;code&gt;plums&lt;/code&gt;. The only way to strip a key out of a Bloom filter is to rebuild the filter from scratch, leaving out the offending key.&lt;/p&gt;

&lt;p&gt;Checking to see whether a key already exists in a filter is exactly analogous to adding a new key. We run the key through our set of hash functions, and then check to see whether the bits at those offsets are all turned on. If any of the bits is off, we know for certain the key is not in the filter. If all of the bits are on, we know the key is probably there.&lt;/p&gt;

&lt;p&gt;I say &amp;ldquo;probably&amp;rdquo; because there&amp;rsquo;s a certain chance our key might be a false positive. For example, let&amp;rsquo;s see what happens when we test our filter for the string &lt;code&gt;mango&lt;/code&gt;. We run &lt;code&gt;mango&lt;/code&gt; through the set of hash functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hash1(&amp;quot;mango&amp;quot;) = 8
hash2(&amp;quot;mango&amp;quot;) = 3
hash3(&amp;quot;mango&amp;quot;) = 12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then examine the bits at those offsets, as shown in Figure 4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_4.gif&#34; alt=&#34;a false positive in the Bloom filter&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 4. A false positive in the Bloom filter.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All of the bits at positions 3, 8, and 12 are on, so our filter will report that &lt;code&gt;mango&lt;/code&gt; is a valid key.&lt;/p&gt;

&lt;p&gt;Of course, &lt;code&gt;mango&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; a valid key &amp;ndash; the filter we built contains only &lt;code&gt;apples&lt;/code&gt; and &lt;code&gt;plums&lt;/code&gt;. The fact that the offsets for &lt;code&gt;mango&lt;/code&gt; point to enabled bits is just coincidence. We have found a false positive &amp;ndash; a key that seems to be in the filter, but isn&amp;rsquo;t really there.&lt;/p&gt;

&lt;p&gt;As you might expect, the false-positive rate depends on the bit vector length and the number of keys stored in the filter. The roomier the bit vector, the smaller the probability that all &lt;code&gt;k&lt;/code&gt; bits we check will be on, unless the key actually exists in the filter. The relationship between the number of hash functions and the false-positive rate is more subtle. If you use too few hash functions, there won&amp;rsquo;t be enough discrimination between keys; but if you use too many, the filter will be very dense, increasing the probability of collisions. You can calculate the false-positive rate for any filter using the formula:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c = ( 1 - e(-kn/m) )k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;c&lt;/code&gt; is the false positive rate, &lt;code&gt;k&lt;/code&gt; is the number of hash functions, &lt;code&gt;n&lt;/code&gt; is the number of keys in the filter, and &lt;code&gt;m&lt;/code&gt; is the length of the filter in bits.&lt;/p&gt;

&lt;p&gt;When using Bloom filters, we very frequently have a desired false-positive rate in mind and we are also likely to have a rough idea of how many keys we want to add to the filter. We need some way of finding out how large a bit vector is to make sure the false-positive rate never exceeds our limit. The following equation will give us vector length from the error rate and number of keys:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;m = -kn / ( ln( 1 - c ^ 1/k ) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll notice another free variable here: &lt;code&gt;k&lt;/code&gt;, the number of hash functions. It&amp;rsquo;s possible to use calculus to find a minimum for &lt;code&gt;k&lt;/code&gt;, but there&amp;rsquo;s a lazier way to do it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub calculate_shortest_filter_length {
    my ( $num_keys, $error_rate ) = @_;
    my $lowest_m;
    my $best_k = 1;

    foreach my $k ( 1..100 ) {
        my $m = (-1 * $k * $num_keys) /
            ( log( 1 - ($error_rate ** (1/$k))));

        if ( !defined $lowest_m or ($m &amp;lt; $lowest_m) ) {
            $lowest_m = $m;
            $best_k   = $k;
        }
    }
    return ( $lowest_m, $best_k );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To give you a sense of how error rate and number of keys affect the storage size of Bloom filters, Table 1 lists some sample vector sizes for a variety of capacity/error rate combinations.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Error Rate&lt;/th&gt;
&lt;th&gt;Keys&lt;/th&gt;
&lt;th&gt;Required Size&lt;/th&gt;
&lt;th&gt;Bytes/Key&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1%&lt;/td&gt;
&lt;td&gt;1K&lt;/td&gt;
&lt;td&gt;1.87 K&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.1%&lt;/td&gt;
&lt;td&gt;1K&lt;/td&gt;
&lt;td&gt;2.80 K&lt;/td&gt;
&lt;td&gt;2.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;1K&lt;/td&gt;
&lt;td&gt;3.74 K&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;10K&lt;/td&gt;
&lt;td&gt;37.4 K&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;100K&lt;/td&gt;
&lt;td&gt;374 K&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;1M&lt;/td&gt;
&lt;td&gt;3.74 M&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.001%&lt;/td&gt;
&lt;td&gt;1M&lt;/td&gt;
&lt;td&gt;4.68 M&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.0001%&lt;/td&gt;
&lt;td&gt;1M&lt;/td&gt;
&lt;td&gt;5.61 M&lt;/td&gt;
&lt;td&gt;5.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can find further lookup tables for various combinations of error rate, filter size, and number of hash functions at &lt;a href=&#34;http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html#tab:bf-config-1&#34;&gt;Bloom Filters &amp;ndash; the math&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;span-id-building-a-bloom-filter-in-perl-building-a-bloom-filter-in-perl-span&#34;&gt;&lt;span id=&#34;building_a_bloom_filter_in_perl&#34;&gt;Building a Bloom Filter in Perl&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;To make a working Bloom filter, we need a good set of hash functions These are easy to come by &amp;ndash; there are several excellent hashing algorithms available on CPAN. For our purposes, a good choice is &lt;code&gt;Digest::SHA1&lt;/code&gt;, a cryptographically strong hash with a fast C implementation. We can use the module to create as many hash functions as we like by salting the input with a list of distinct values. Here&amp;rsquo;s a subroutine that builds a list of unique hash functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use Digest::SHA1 qw/sha1/;

sub make_hashing_functions {
    my ( $count ) = @_;
    my @functions;

    for my $salt (1..$count ) {
        push @functions, sub { sha1( $salt, $_[0] ) };
    }

    return @functions;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be able to use these hash functions, we have to find a way to control their range. &lt;code&gt;Digest::SHA1&lt;/code&gt; returns an embarrassingly lavish 160 bits of hashed output, useful only in the unlikely case that our vector is 2&lt;sup&gt;160&lt;/sup&gt; bits long. We&amp;rsquo;ll use a combination of bit chopping and division to scale the output down to a more usable size.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a subroutine that takes a key, runs it through a list of hash functions, and returns a bitmask of length &lt;code&gt;$FILTER_LENGTH&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub make_bitmask {
    my ( $key ) = @_;
    my $mask    = pack( &amp;quot;b*&amp;quot;, &#39;0&#39; x $FILTER_LENGTH);

    foreach my $hash_function ( @functions ){

        my $hash       = $hash_function-&amp;gt;($key);
        my $chopped    = unpack(&amp;quot;N&amp;quot;, $hash );
        my $bit_offset = $result % $FILTER_LENGTH;

        vec( $mask, $bit_offset, 1 ) = 1;
    }
    return $mask;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a dense stretch of code, so let&amp;rsquo;s look at it line by line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $mask = pack( &amp;quot;b*&amp;quot;, &#39;0&#39; x $FILTER_LENGTH);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We start by using Perl&amp;rsquo;s &lt;code&gt;pack&lt;/code&gt; operator to create a zeroed bit vector that is &lt;code&gt;$FILTER_LENGTH&lt;/code&gt; bits long. &lt;code&gt;pack&lt;/code&gt; takes two arguments, a template and a value. The &lt;code&gt;b&lt;/code&gt; in our template tells &lt;code&gt;pack&lt;/code&gt; that we want it to interpret the value as bits, and the &lt;code&gt;*&lt;/code&gt; indicates &amp;ldquo;repeat as often as necessary,&amp;rdquo; just like in a regular expression. Perl will actually pad our bit vector to make its length a multiple of eight, but we&amp;rsquo;ll ignore those superfluous bits.&lt;/p&gt;

&lt;p&gt;With a blank bit vector in hand, we&amp;rsquo;re ready to start running our key through the hash functions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $hash = $hash_function-&amp;gt;($key);
my $chopped = unpack(&amp;quot;N&amp;quot;, $hash );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re keeping the first 32 bits of the output and discarding the rest. This prevents us from having to require &lt;code&gt;BigInt&lt;/code&gt; support further along. The second line does the actual bit chopping. The &lt;code&gt;N&lt;/code&gt; in the template tells &lt;code&gt;unpack&lt;/code&gt; to extract a 32-bit integer in network byte order. Because we don&amp;rsquo;t provide any quantifier in the template, &lt;code&gt;unpack&lt;/code&gt; will extract just one integer and then stop.&lt;/p&gt;

&lt;p&gt;If you are extra, super paranoid about bit chopping, you could split the hash into five 32-bit pieces and XOR them together, preserving all the information in the original hash:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $chopped = pack( &amp;quot;N&amp;quot;, 0 );
my @pieces  =  map { pack( &amp;quot;N&amp;quot;, $_ ) } unpack(&amp;quot;N*&amp;quot;, $hash );
$chopped    = $_ ^ $chopped foreach @pieces;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this is probably overkill.&lt;/p&gt;

&lt;p&gt;Now that we have a list of 32-bit integer outputs from our hash functions, all we have to do is scale them down with the modulo operator so they fall in the range (1..&lt;code&gt;$FILTER_LENGTH&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $bit_offset = $chopped % $FILTER_LENGTH;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;ve turned our key into a list of bit offsets, which is exactly what we were after.&lt;/p&gt;

&lt;p&gt;The only thing left to do is to set the bits using &lt;code&gt;vec&lt;/code&gt;, which takes three arguments: the vector itself, a starting position, and the number of bits to set. We can assign a value to &lt;code&gt;vec&lt;/code&gt; like we would to a variable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vec( $mask, $bit_offset, 1 ) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After we&amp;rsquo;ve set all the bits, we wind up with a bitmask that is the same length as our Bloom filter. We can use this mask to add the key into the filter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub add {
    my ( $key, $filter ) = @_;

    my $mask = make_bitmask( $key );
    $filter  = $filter | $mask;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or we can use it to check whether the key is already present:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub check {
    my ( $key, $filter ) = @_;
    my $mask  = make_bitmask( $key );
    my $found = ( ( $filter &amp;amp; $mask ) eq $mask );
    return $found;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that those are the bitwise OR (&lt;code&gt;|&lt;/code&gt;) and AND (&lt;code&gt;&amp;amp;&lt;/code&gt;) operators, not the more commonly used logical OR (&lt;code&gt;||&lt;/code&gt;) and AND ( &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; ) operators. Getting the two mixed up can lead to hours of interesting debugging. The first example ORs the mask against the bit vector, turning on any bits that aren&amp;rsquo;t already set. The second example compares the mask to the corresponding positions in the filter &amp;ndash; if all of the on bits in the mask are also on in the filter, we know we&amp;rsquo;ve found a match.&lt;/p&gt;

&lt;p&gt;Once you get over the intimidation factor of using &lt;code&gt;vec&lt;/code&gt;, &lt;code&gt;pack&lt;/code&gt;, and the bitwise operators, Bloom filters are actually quite straightforward. &lt;a href=&#34;http://localhost:1313/media/_pub_2004_04_08_bloom_filters/Filter.pm&#34;&gt;Listing 1&lt;/a&gt; shows a complete object-oriented implementation called &lt;code&gt;Bloom::Filter&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;span-id-bloom-filters-in-distributed-social-networks-bloom-filters-in-distributed-social-networks-span&#34;&gt;&lt;span id=&#34;bloom_filters_in_distributed_social_networks&#34;&gt;Bloom Filters in Distributed Social Networks&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;One drawback of existing social network schemes is that they require participants to either divulge their list of contacts to a central server (Orkut, Friendster) or publish it to the public Internet (FOAF), in both cases sacrificing a great deal of privacy. By exchanging Bloom filters instead of explicit lists of contacts, users can participate in social networking experiments without having to admit to the world who their friends are. A Bloom filter encoding someone&amp;rsquo;s contact information can be checked to see whether it contains a given name or email address, but it can&amp;rsquo;t be coerced into revealing the full list of keys that were used to build it. It&amp;rsquo;s even possible to turn the false-positive rate, which may not sound like a feature, into a powerful tool.&lt;/p&gt;

&lt;p&gt;Suppose that I am very concerned about people trying to reverse-engineer my social network by running a dictionary attack against my Bloom filter. I can build my filter with a prohibitively high false-positive rate (50%, for example) and then arrange to send multiple copies of my Bloom filter to friends, varying the hash functions I use to build each filter. The more filters my friends collect, the lower the false-positive rate they will see. For example, with five filters the false-positive rate will be (0.5)&lt;sup&gt;5&lt;/sup&gt;, or 3% &amp;ndash; and I can reduce the rate further by sending out more filters.&lt;/p&gt;

&lt;p&gt;If any one of the filters is intercepted, it will register the full 50% false-positive rate. So I am able to hedge my privacy risk across several interactions, and have some control over how accurately other people can see my network. My friends can be sure with a high degree of certainty whether someone is on my contact list, but someone who manages to snag just one or two of my filters will learn almost nothing about me.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a Perl function that checks a key against a set of noisy filters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use Bloom::Filter;

sub check_noisy_filters {
    my ( $key, @filters ) = @_;
    foreach my $filter ( @filters ) {
        return 0 unless $filter-&amp;gt;check( $key );
    }
    return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you and your friends agree to use the same filter length and set of hash functions, you can also use bitwise comparisons to estimate the degree of overlap between your social networks. The number of shared on bits in two Bloom filters will give a usable measure of the distance between them.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub shared_on_bits {
    my ( $filter_1, $filter_2 ) = @_;
    return unpack( &amp;quot;%32b*&amp;quot;,  $filter_1 &amp;amp; $filter_2 )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, you can combine two Bloom filters that have the same length and hash functions with the bitwise OR operator to create a composite filter. For example, if you participate in a small mailing list and want to create a whitelist from the address books of everyone in the group, you can have each participant create a Bloom filter individually and then OR the filters together into a Voltron-like master list. None of the members of the group will know who the other members&amp;rsquo; contacts are, and yet the filter will exhibit the correct behavior.&lt;/p&gt;

&lt;p&gt;There are sure to be other neat Bloom filter tricks with potential applications to social networking and distributed applications. The references below list a few good places to start mining.&lt;/p&gt;

&lt;h3 id=&#34;span-id-references-references-span&#34;&gt;&lt;span id=&#34;references&#34;&gt;References&lt;/span&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2ecs_2ewisc_2eedu_2f_7ecao_2fpape&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html&#34;&gt;Bloom Filters &amp;ndash; the math&lt;/a&gt;&lt;/strong&gt;. A good place to start for an overview of the math behind Bloom filters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2ecap_2dlore_2ecom_2fcode_2fbloom&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.cap-lore.com/code/BloomTheory.html&#34;&gt;Some Motley Bloom Tricks&lt;/a&gt;&lt;/strong&gt;. Handy filter tricks and theory page.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2eeecs_2eharvard_2eedu_2f_7emicha&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.eecs.harvard.edu/~michaelm/NEWWORK/postscripts/BloomFilterSurvey.pdf&#34;&gt;Bloom Filter Survey&lt;/a&gt;&lt;/strong&gt;. A handy survey article on Bloom filter network applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_loaf&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://loaf.cantbedone.org&#34;&gt;LOAF&lt;/a&gt;&lt;/strong&gt;. Our own system for incorporating social networks onto email using Bloom filters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.eecs.harvard.edu/~michaelm/NEWWORK/postscripts/cbf2.pdf&#34;&gt;Compressed Bloom Filters&lt;/a&gt;&lt;/strong&gt;. If you are passing filters around a network, you will want to optimize them for minimum size; this paper gives a good overview of compressed Bloom filters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_bloom16&#34;&gt;&lt;/span&gt;&lt;a href=&#34;https://metacpan.org/pod/Bloom16&#34;&gt;Bloom16&lt;/a&gt;.&lt;/strong&gt; A CPAN module implementing a counting Bloom filter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_bloom&#34;&gt;&lt;/span&gt;&lt;a href=&#34;https://metacpan.org/pod/Text::Bloom&#34;&gt;Text::Bloom&lt;/a&gt;&lt;/strong&gt;. CPAN module for using Bloom filters with text collections.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2eresearch_2eatt_2ecom_2f_7esmb_2&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.research.att.com/~smb/papers/bloom-encrypt.pdf&#34;&gt;Privacy-Enhanced Searches Using Encryted Bloom Filters&lt;/a&gt;&lt;/strong&gt;. This paper discusses how to use encryption and Bloom filters to set up a query system that prevents the search engine from knowing the query you are running.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.cs.wisc.edu/~cao/papers/summary-cache/node9.html&#34;&gt;Bloom Filters as Summaries&lt;/a&gt;&lt;/strong&gt;. Some performance data on actually using Bloom filters as cache summaries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.research.att.com/~smb/papers/draft-bellovin-dnsext-bloomfilt-00.txt&#34;&gt;Using Bloom Filters for Authenticated Yes/No Answers in the DNS&lt;/a&gt;&lt;/strong&gt;. Internet draft for using Bloom filters to implement Secure DNS&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Building a Vector Space Search Engine in Perl</title>
      <link>http://localhost:1313/pub/2003/02/19/engine.html/</link>
      <pubDate>Wed, 19 Feb 2003 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2003/02/19/engine.html/</guid>
      <description>

&lt;p&gt;&lt;span id=&#34;__index__&#34;&gt;&lt;/span&gt;
-   &lt;a href=&#34;#building%20a%20vector%20space%20search%20engine%20in%20perl&#34;&gt;Building a Vector Space Search Engine in Perl&lt;/a&gt;
-   &lt;a href=&#34;#a%20few%20words%20about%20vectors&#34;&gt;A Few Words About Vectors&lt;/a&gt;
-   &lt;a href=&#34;#getting%20down%20to%20business&#34;&gt;Getting Down To Business&lt;/a&gt;
-   &lt;a href=&#34;#building%20the%20search%20engine&#34;&gt;Building the Search Engine&lt;/a&gt;
-   &lt;a href=&#34;#making%20it%20better&#34;&gt;Making it Better&lt;/a&gt;
-   &lt;a href=&#34;#further%20reading&#34;&gt;Further Reading&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&#34;building a vector space search engine in perl&#34;&gt;&lt;/span&gt;
&lt;em&gt;Why waste time reinventing the wheel, when you could be reinventing the engine?&lt;/em&gt; &lt;em&gt;-Damian Conway&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As a Perl programmer, sooner or later you&amp;rsquo;ll get an opportunity to build a search engine. Like many programming tasks - parsing a date, validating an e-mail address, writing to a temporary file - this turns out to be easy to do, but hard to get right. Most people try end up with some kind of reverse index, a data structure that associates words with lists of documents. Onto this, they graft a scheme for ranking the results by relevance.&lt;/p&gt;

&lt;p&gt;Nearly every search engine in use today - from Google on down - works on the basis of a &lt;strong&gt;reverse keyword index&lt;/strong&gt;. You can write such a keyword engine in Perl, but as your project grows you will inevitably find yourself gravitating to some kind of relational database system. Since databases are customized for fast lookup and indexing, it&amp;rsquo;s no surprise that most keyword search engines make heavy use of them. But writing code for them isn&amp;rsquo;t much fun.&lt;/p&gt;

&lt;p&gt;More to the point, companies like Google and Atomz already offer excellent, free search services for small Web sites. You can get an instant search engine with a customizable interface, and spend no time struggling with Boolean searches, text highlighting, or ranking algorithms. Why bother duplicating all that effort?&lt;/p&gt;

&lt;p&gt;As Perl programmers, we know that laziness is a virtue. But we also know that there is more than one way to do things. Despite the ubiquity of reverse-index search, there are many other ways to build a search engine. Most of them originate in the field of information retrieval, where researchers are having all kinds of fun. Unfortunately, finding documentation about these alternatives isn&amp;rsquo;t easy. Most of the material available online is either too technical or too impractical to be of use on real-world data sets. So programmers are left with the false impression that vanilla search is all there is.&lt;/p&gt;

&lt;p&gt;In this article, I want to show you how to build and run a search engine using a &lt;strong&gt;vector-space model&lt;/strong&gt;, an alternative to reverse index lookup that does not require a database, or indeed any file storage at all. Vector-space search engines eliminate many of the disadvantages of keyword search without introducing too many disadvantages of their own. Best of all, you can get one up and running in just a few dozen lines of Perl.&lt;/p&gt;

&lt;h1 id=&#34;span-id-a-few-words-about-vectors-a-few-words-about-vectors-span&#34;&gt;&lt;span id=&#34;a few words about vectors&#34;&gt;A Few Words About Vectors&lt;/span&gt;&lt;/h1&gt;

&lt;p&gt;Vector-space search engines use the notion of a &lt;strong&gt;term space&lt;/strong&gt;, where each document is represented as a vector in a high-dimensional space. There are as many dimensions as there are unique words in the entire collection. Because a document&amp;rsquo;s position in the term space is determined by the words it contains, documents with many words in common end up close together, while documents with few shared words end up far apart.&lt;/p&gt;

&lt;p&gt;To search our collection, we project a query into this term space and calculate the distance from the query vector to all the document vectors in turn. Those documents that are within a certain threshold distance get added to our result set. If all this sounds like gobbledygook to you, then don&amp;rsquo;t worry - it will become clearer when we write the code.&lt;/p&gt;

&lt;p&gt;The vector-space data model gives us a search engine with several useful features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Searches take place in RAM, there is no disk or database access&lt;/li&gt;
&lt;li&gt;Queries can be arbitrarily long&lt;/li&gt;
&lt;li&gt;Users don&amp;rsquo;t have to bother with Boolean logic or regular expressions&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s trivially easy to do &amp;lsquo;find similar&amp;rsquo; searches on returned documents&lt;/li&gt;
&lt;li&gt;You can set up a &amp;lsquo;saved results&amp;rsquo; basket, and do similarity searches on the documents in it&lt;/li&gt;
&lt;li&gt;You get to talk about &amp;lsquo;vector spaces&amp;rsquo; and impress your friends&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;span-id-getting-down-to-business-getting-down-to-business-span&#34;&gt;&lt;span id=&#34;getting down to business&#34;&gt;Getting Down to Business&lt;/span&gt;&lt;/h1&gt;

&lt;p&gt;The easiest way to understand the vector model is with a concrete example.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you have a collection of 10 documents, and together they contain 50 unique words. You can represent each document as a vector by counting how many times a word appears in the document, and moving that distance along the appropriate axis. So if Document A contains the sentence &amp;ldquo;cats like chicken&amp;rdquo;, then you find the axis for &lt;code&gt;cat&lt;/code&gt;, move along one unit, and then do the same for &lt;code&gt;like&lt;/code&gt; and &lt;code&gt;chicken&lt;/code&gt;. Since the other 47 words don&amp;rsquo;t appear in your document, you don&amp;rsquo;t move along the corresponding axes at all.&lt;/p&gt;

&lt;p&gt;Plot this point and draw a line to it from the origin, and you have your &lt;strong&gt;document vector&lt;/strong&gt;. Like any vector, it has a magnitude (determined by how many times each word occurs), and a direction (determined by which words appeared, and their relative abundance).&lt;/p&gt;

&lt;p&gt;There are two things to notice here:&lt;/p&gt;

&lt;p&gt;The first is that we throw away all information word order, and there is no guarantee that the vector will be unique. If we had started with the sentence &amp;ldquo;chickens like cats&amp;rdquo; (ignoring plurals for the moment), then we would have ended up with an identical document vector, even though the documents are not the same.&lt;/p&gt;

&lt;p&gt;This may seem to be a big limitation, but it turns out that word order in natural language contains little information about content - you can infer most of what a document is about by studying the word list. Bad news for English majors, good news for us.&lt;/p&gt;

&lt;p&gt;The second thing to notice is that with three non-zero values out of a possible 50, our document vector is sparse. This will hold true for any natural language collection, where a given document will contain only a tiny proportion of all the possible words in the language. This makes it possible to build in-RAM search engines even for large collections, although the details are outside the scope of this article. The point is, you can scale this model up quite far before having to resort to disk access.&lt;/p&gt;

&lt;p&gt;To run a vector-space search engine, we need to do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assemble a document collection&lt;/li&gt;
&lt;li&gt;Create a term space and map the documents into it&lt;/li&gt;
&lt;li&gt;Map an incoming query into the same term space&lt;/li&gt;
&lt;li&gt;Compare the query vector to the stored document vectors&lt;/li&gt;
&lt;li&gt;Return a list of nearest documents, ranked by distance&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now let&amp;rsquo;s see how to implement these steps in Perl.&lt;/p&gt;

&lt;h1 id=&#34;span-id-building-the-search-engine-building-the-search-engine-span&#34;&gt;&lt;span id=&#34;building the search engine&#34;&gt;Building the Search Engine&lt;/span&gt;&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ll make things easy by starting with a tiny collection of four documents, each just a few words long:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        &amp;quot;The cat in the hat&amp;quot;

        &amp;quot;A cat is a fine pet.&amp;quot;

        &amp;quot;Dogs and cats make good pets.&amp;quot;

        &amp;quot;I haven&#39;t got a hat.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our first step is to find all the unique words in the document set. The easiest way to do this is to convert each document into a word list, and then combine the lists together into one. Here&amp;rsquo;s one (awful) way do it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        sub get_words { 
                my ( $text ) = @_;
                return map { lc $_ =&amp;gt; 1 }
                       map { /([a-z\-&#39;]+)/i } 
                       split /\s+/s, $text;
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The subroutine splits a text string on whitespace, takes out all punctuation except hyphens and apostrophes, and converts everything to lower case before returning a hash of words.&lt;/p&gt;

&lt;p&gt;The curious &lt;code&gt;do&lt;/code&gt; statement is just a compact way of creating a lookup hash. &lt;code&gt;%doc_words&lt;/code&gt; will end up containing our word list as its keys, and its values will be the number of times each word appeared.&lt;/p&gt;

&lt;p&gt;If we run this &amp;lsquo;parser&amp;rsquo; on all four documents in turn, then we get a master word list:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        a
        and
        cat
        cats
        dogs
        fine
        good
        got
        hat
        haven&#39;t
        i
        in
        is
        make
        pet
        pets
        the
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that many of the words in this list are junk words - pronouns, articles, and other grammatical flotsam that&amp;rsquo;s not useful in a search context. A common procedure in search engine design is to strip out words like these using a &lt;strong&gt;stop list&lt;/strong&gt;. Here&amp;rsquo;s the same subroutine with a rudimentary stop list added in, filtering out the most common words:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        our %stop_hash;
        our @stop_words = qw/i in a to the it have haven&#39;t was but is be from/;
        foreach @stop_words {$stop_hash{$_}++ };


        sub get_words {


                # Now with stop list action!


                my ( $text ) = @_;
                return map { $_ =&amp;gt; 1 }
                       grep { !( exists $stop_hash{$_} ) }
                       map lc,
                       map { /([a-z\-&#39;]+)/i } 
                       split /\s+/s, $text;
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A true stop list would be longer, and tailored to fit our document collection. You can find a real stop list in the &lt;code&gt;DATA&lt;/code&gt; section of &lt;a href=&#34;http://localhost:1313/media/_pub_2003_02_19_engine/VectorSpace.pm&#34;&gt;Listing 1&lt;/a&gt;, along with a complete implementation of the search engine described here. Note that because of Perl&amp;rsquo;s fast hash lookup algorithm, we can have a copious stop list without paying a big price in program speed. Because word frequencies in natural language obey a power-law distribution, getting rid of the most common words removes a disproportionate amount of bulk from our vectors.&lt;/p&gt;

&lt;p&gt;Here is what our word list looks after we munge it with the stop list:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        cat
        cats
        dogs
        fine
        good
        got
        hat
        make
        pet
        pets
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve narrowed the list down considerably, which is good. But notice that our list contains a couple of variants (&amp;ldquo;cat&amp;rdquo; and &amp;ldquo;cats&amp;rdquo;, &amp;ldquo;pet&amp;rdquo; and &amp;ldquo;pets&amp;rdquo;), that differ only in number. Also note that someone who searches on &amp;lsquo;dog&amp;rsquo; in our collection won&amp;rsquo;t get any matches, even though &amp;lsquo;dogs&amp;rsquo; in the plural form is a valid hit. That&amp;rsquo;s bad.&lt;/p&gt;

&lt;p&gt;This is a common problem in search engine design, so of course there is a module on the CPAN to solve it. The bit of code we need is called a &lt;strong&gt;stemmer&lt;/strong&gt;, a set of heuristics for removing suffixes from English words, leaving behind a common root. The stemmer we can get from CPAN uses the &lt;strong&gt;Porter stemming algorithm&lt;/strong&gt;, which is an imperfect but excellent way of finding word stems in English.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        use Lingua::Stem;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll wrap the stemmer in our own subroutine, to hide the clunky Lingua::Stem syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        sub stem {
                my ( $word) = @_;
                my $stemref = Lingua::Stem::stem( $word );
                return $stemref-&amp;gt;[0];
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here&amp;rsquo;s how to fold it in to the &lt;code&gt;get_words&lt;/code&gt; subroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        return map { stem($_) =&amp;gt; 1 }
               grep { !( exists $stop_hash{$_} ) }
               map lc,
               map { /([a-z\-&#39;]+)/i } 
               split /\s+/s, $text;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that we apply our stop list before we stem the words. Otherwise, a valid word like &lt;code&gt;beings&lt;/code&gt; (which stems to &lt;code&gt;be&lt;/code&gt;) would be caught by the overzealous stop list. It&amp;rsquo;s easy to make little slips like this in search algorithm design, so be extra careful.&lt;/p&gt;

&lt;p&gt;With the stemmer added in, our word list now looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        cat
        dog
        fine
        good
        got
        hat
        make
        pet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Much better! We have halved the size of our original list, while preserving all of the important content.&lt;/p&gt;

&lt;p&gt;Now that we have a complete list of &lt;strong&gt;content words&lt;/strong&gt;, we&amp;rsquo;re ready for the second step - mapping our documents into the term space. Because our collection has a vocabulary of eight content words, each of our documents will map onto an eight-dimensional vector.&lt;/p&gt;

&lt;p&gt;Here is one example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        # A cat is a fine pet



        $vec = [ 1, 0, 1, 0, 0, 0, 1 ];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The sentence &amp;ldquo;A cat is a fine pet&amp;rdquo; contains three content words. Looking at our sorted list of words, we find &lt;code&gt;cat&lt;/code&gt;, &lt;code&gt;fine&lt;/code&gt;, and &lt;code&gt;pet&lt;/code&gt; at positions one, three, and eight respectively, so we create an anonymous array and put ones at those positions, with zeroes everywhere else.&lt;/p&gt;

&lt;p&gt;If we wanted to go in the opposite direction, then we could take the vector and look up the non-zero values at the appropriate position in a sorted word list, getting back the content words in the document (but no information about word order).&lt;/p&gt;

&lt;p&gt;The problem with using Perl arrays here is that they won&amp;rsquo;t scale. Perl arrays eat lots of memory, and there are no native functions for comparing arrays to one another. We would have to loop through our arrays, which is slow.&lt;/p&gt;

&lt;p&gt;A better data way to do it is to use the PDL module, a set of compiled C extensions to Perl made especially for use with matrix algebra. You can find it on the CPAN. PDL stands for &amp;ldquo;Perl Data Language&amp;rdquo;, and it is a powerful language indeed, optimized for doing math operations with enormous multidimensional matrices. All we&amp;rsquo;ll be using is a tiny slice of its functionality, the equivalent of driving our Ferrari to the mailbox.&lt;/p&gt;

&lt;p&gt;It turns out that a PDL vector (or &amp;ldquo;piddle&amp;rdquo;) looks similar to our anonymous array:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        use PDL;
        my $vec = piddle [ 1, 0, 1, 0, 0, 0, 1 ];


        &amp;gt; print $vec

        [1 0 0 1 0 0 0 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We give the piddle constructor the same anonymous array as an argument, and it converts it to a smaller data structure, requiring less storage.&lt;/p&gt;

&lt;p&gt;Since we already know that most of our values in each document vector will be zero (remember how sparse natural language is), passing full-length arrays to the piddle constructor might get a little cumbersome. So instead we&amp;rsquo;ll use a shortcut to create a zero-filled piddle, and then set the non-zero values explicitly. For this we have the &lt;code&gt;zeroes&lt;/code&gt; function, which takes the size of the vector as its argument:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        my $num_words = 8;
        my $vec = zeroes $num_words;



        &amp;gt; print $vec

        [0 0 0 0 0 0 0 0 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To set one of the zero values to something else, we&amp;rsquo;ll have to use the obscure PDL syntax:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        my $value = 3;
        my $offset = 4;



        index( $vec, $offset ) .= $value;

        &amp;gt; print $vec;

        [0 0 0 3 0 0 0 0 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we&amp;rsquo;ve said &amp;ldquo;take this vector, and set the value at position 4 to 3&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;This turns out to be all we need to create a document vector. Now we just have to loop through each document&amp;rsquo;s word list, and set the appropriate values in the corresponding vector. Here&amp;rsquo;s a subroutine that does the whole thing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        # $word_count is the total number of words in collection
        # %index is a lookup hash of word to its position in the master list



        sub make_vector {
                my ( $doc ) = @_;
                my %words = get_words( $doc );  
                my $vector = zeroes $word_count;


                foreach my $w ( keys %words ) {
                        my $value = $words{$w};
                        my $offset = $index{$w};
                        index( $vector, $offset ) .= $value;
                }
                return $vector;
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we can generate a vector for each document in our collection, as well as turn an incoming query into a query vector (by feeding the query into the &lt;code&gt;make_vector&lt;/code&gt; subroutine), all we&amp;rsquo;re missing is a way to calculate the distance between vectors.&lt;/p&gt;

&lt;p&gt;There are many ways to do this. One of the simplest (and most intuitive) is the &lt;strong&gt;cosine measure&lt;/strong&gt;. Our intuition is that document vectors with many words in common will point in roughly the same direction, so the angle between two document vectors is a good measure of their similarity. Taking the cosine of that angle gives us a value from zero to one, which is handy. Documents with no words in common will have a cosine of zero; documents that are identical will have a cosine of one. Partial matches will have an intermediate value - the closer that value is to one, the more relevant the document.&lt;/p&gt;

&lt;p&gt;The formula for calculating the cosine is this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        cos  = ( V1 * V2 ) / ||V1|| x ||V2||
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where V2 and V2 are our vectors, the vertical bars indicate the 2-norm, and the &lt;code&gt;*&lt;/code&gt; indicates the inner product. You can take the math on faith, or look it up in any book on linear algebra.&lt;/p&gt;

&lt;p&gt;With PDL, we can express that relation easily:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        sub cosine {
                my ($vec1, $vec2 ) = @_;
                my $n1 = norm $vec1;
                my $n2 = norm $vec2;
                my $cos = inner( $n1, $n2 );    # inner product
                return $cos-&amp;gt;sclr();  # converts PDL object to Perl scalar
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can normalize the vectors to unit length using the &lt;code&gt;norm&lt;/code&gt; function, because we&amp;rsquo;re not interested in their absolute magnitude, only the angle between them.&lt;/p&gt;

&lt;p&gt;Now that we have a way of computing distance between vectors, we&amp;rsquo;re almost ready to run our search engine. The last bit of the puzzle is a subroutine to take a query vector and compare it against all of the document vectors in turn, returning a ranked list of matches:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        sub get_cosines {
                my ( $query_vec ) = @_;
                my %cosines;
                while ( my ( $id, $vec ) = each  %document_vectors ) {
                        my $cosine = cosine( $vec, $query_vec );
                        next unless $cosine &amp;gt; $threshold;
                        $cosines{$id} = $cosine;
                }
                return %cosines;
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives us back a hash with document IDs as its keys, and cosines as its values. We&amp;rsquo;ll call this subroutine from a &lt;code&gt;search&lt;/code&gt; subroutine that will be our module&amp;rsquo;s interface with the outside world:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        sub search {
                my ( $query ) = @_;
                my $query_vec = make_vector( $query );
                my %results = get_cosines( $query_vec );
                return %results;
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that remains is to sort the results by the cosine (in descending order), format them, and display them to the user.&lt;/p&gt;

&lt;p&gt;You can find an object-oriented implementation of this code in &lt;a href=&#34;http://localhost:1313/media/_pub_2003_02_19_engine/VectorSpace.pm&#34;&gt;Listing 1&lt;/a&gt;, complete with built-in stop list and some small changes to make the search go faster (for the curious, we normalize the document vectors before storing them, to save having to do it every time we run the &lt;code&gt;cosine&lt;/code&gt; subroutine).&lt;/p&gt;

&lt;p&gt;Once we&amp;rsquo;ve actually written the code, using it is straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    use Search::VectorSpace;



    my @docs = get_documents_from_somewhere();


    my $engine = Search::VectorSpace-&amp;gt;new( docs =&amp;gt; \@docs );


    $engine-&amp;gt;build_index();
    $engine-&amp;gt;set_threshold( 0.8 );


    while ( my $query = &amp;lt;&amp;gt; ) {
        my %results = $engine-&amp;gt;search( $query );
        foreach my $result ( sort { $results{$b} &amp;lt;=&amp;gt; $results{$a} }
                             keys %results ) {
                print &amp;quot;Relevance: &amp;quot;, $results{$result}, &amp;quot;\n&amp;quot;;
                print $result, &amp;quot;\n\n&amp;quot;;
        }


        print &amp;quot;Next query?\n&amp;quot;;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And there we have it, an instant search engine, all in Perl.&lt;/p&gt;

&lt;h1 id=&#34;span-id-making-it-better-making-it-better-span&#34;&gt;&lt;span id=&#34;making it better&#34;&gt;Making It Better&lt;/span&gt;&lt;/h1&gt;

&lt;p&gt;There are all kinds of ways to improve on this basic model. Here are a few ideas to consider:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_Better_parsing&#34;&gt;Better parsing&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;
Our &lt;code&gt;get_words&lt;/code&gt; subroutine is rudimentary - the code equivalent of a sharpened stick. For one thing, it will completely fail on text containing hyperlinks, acronyms or XML. It also won&amp;rsquo;t recognize proper names or terms that contain more than one word ( like &amp;ldquo;Commonwealth of Independent States&amp;rdquo;). You can make the parser smarter by stripping out HTML and other markup with a module like &lt;code&gt;HTML::TokeParser&lt;/code&gt;, and building in a part-of-speech tagger to find proper names and noun phrases (look for our own Lingua::Tagger::En, coming soon on the CPAN).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_Non%2DEnglish_collections&#34;&gt;Non-English Collections&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;
Perl has great Unicode support, and the vector model doesn&amp;rsquo;t care about language, so why limit ourselves to English? As long as you can write a parser, you can adapt the search to work with any language.&lt;/p&gt;

&lt;p&gt;Most likely you will need a special stemming algorithm. This can be easy as pie for some languages (Chinese, Italian), and really hard for others (Arabic, Russian, Hungarian). It depends entirely on the morphology of the language. You can find published stemming algorithms online for several Western European languages, including German, Spanish and French.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_Similarity_search&#34;&gt;Similarity Search&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;
It&amp;rsquo;s easy to add a &amp;ldquo;find similar&amp;rdquo; feature to your search engine. Just use an existing document vector as your query, and everything else falls into place. If you want to do a similarity search on multiple documents, then add the vectors together.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_Term_weighting&#34;&gt;Term Weighting&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;Term weighting&lt;/strong&gt; is a fancy way of saying &amp;ldquo;some words are more important than others&amp;rdquo;. Done properly, it can greatly improve search results.&lt;/p&gt;

&lt;p&gt;You calculate weights when building document vectors. &lt;strong&gt;Local weighting&lt;/strong&gt; assigns values to words based on how many times they appear in a single document, while &lt;strong&gt;global weighting&lt;/strong&gt; assigns values based on word frequency across the entire collection. The intuition is that rare words are more interesting than common words (global weighting), and that words that appear once in a document are not as relevant as words that occur multiple times (local weighting).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_Incorporating_metadata&#34;&gt;Incorporating Metadata&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;
If your documents have metadata descriptors (dates, categories, author names), then you can build those in to the vector model. Just add a slot for each category, like you did for your keywords, and apply whatever kind of weighting you desire.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_Exact_phrase_matching&#34;&gt;Exact Phrase Matching&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;
You can add arbitrary constraints to your result set by adding a chain of filters to your result set. An easy way to do exact phrase matching is to loop through your result set with a regular expression. This kind of post-processing is also a convenient way to sort your results by something other than relevance.&lt;/p&gt;

&lt;h1 id=&#34;span-id-further-reading-further-reading-span&#34;&gt;&lt;span id=&#34;further reading&#34;&gt;Further Reading&lt;/span&gt;&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s a vast body of material available on search engine design, but little of it is targeted at the hobbyist or beginner. The following are good places to start:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_http%3A%2F%2Fhotwired%2Elycos%2Ecom%2Fwebmonkey%2F&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://hotwired.lycos.com/webmonkey/code/97/16/index2a.html?collection=perl&#34;&gt;http://hotwired.lycos.com/webmonkey/code/97/16/index2a.html?collection=perl&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
This Webmonkey article dates back to 1997, but it&amp;rsquo;s still the best tutorial on writing a reverse index search engine in Perl.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_http%3A%2F%2Fmoskalyuk%2Ecom%2Fsoftware%2Fperl%2Fs&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://moskalyuk.com/software/perl/search/kiss.htm&#34;&gt;http://moskalyuk.com/software/perl/search/kiss.htm&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
An example of a simple keyword search engine - no database, just a deep faith in Perl&amp;rsquo;s ability to parse files quickly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_http%3A%2F%2Fwww%2Emovabletype%2Eorg%2Fdownload%2E&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.movabletype.org/download.shtml&#34;&gt;http://www.movabletype.org/download.shtml&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
Movable Type is a popular weblog application, written in Perl. The search engine lives in MT::App::Search, and supports several advanced features. It&amp;rsquo;s not pretty, but it&amp;rsquo;s real production code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_http%3A%2F%2Fjakarta%2Eapache%2Eorg%2Flucene%2Fdoc&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://jakarta.apache.org/lucene/docs/index.html&#34;&gt;http://jakarta.apache.org/lucene/docs/index.html&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
Lucene is a Java-based keyword search engine, part of the Apache project. It&amp;rsquo;s a well-designed, open-source search engine, intended for larger projects. The documentation discusses some of the challenges of implementing a large search engine; it&amp;rsquo;s worth reading even if you don&amp;rsquo;t know Java.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_http%3A%2F%2Fmitpress%2Emit%2Eedu%2Fcatalog%2Fitem&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;amp;tid=3391&#34;&gt;http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;amp;tid=3391&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Foundations of Statistical Natural Language Processing&lt;/em&gt;, MIT Press (hardcover textbook). Don&amp;rsquo;t be put off by the title - this book is a fantastic introduction to all kinds of issues in text search, and includes a thorough discussion of vector space models.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span id=&#34;item_http%3A%2F%2Fwww%2Enitle%2Eorg%2Flsi%2Fintro%2F&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.nitle.org/lsi/intro/&#34;&gt;http://www.nitle.org/lsi/intro/&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
An introduction to &lt;strong&gt;latent semantic indexing&lt;/strong&gt;, the vector model on steroids. The document is aimed at nontechnical readers, but gives some more background information on using vector techniques to search and visualize data collections.&lt;/p&gt;

&lt;p&gt;The adventurous can also download some Perl code for latent semantic indexing at &lt;a href=&#34;http://www.nitle.org/lsi.php.&#34;&gt;http://www.nitle.org/lsi.php.&lt;/a&gt; Both the code and the article come from my own work for the National Institute for Technology and Liberal Education.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

