<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Science on Perl.com - programming news, code and culture</title>
    <link>http://localhost:1313/categories/science/</link>
    <description>Recent content in Science on Perl.com - programming news, code and culture</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Jan 2019 15:41:39 +0000</lastBuildDate>
    <atom:link href="/categories/science/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Find Relationships Quickly in Data With Chart::Plot</title>
      <link>http://localhost:1313/article/find-relationships-quickly-in-data-with-chart-plot/</link>
      <pubDate>Mon, 21 Jan 2019 15:41:39 +0000</pubDate>
      
      <guid>http://localhost:1313/article/find-relationships-quickly-in-data-with-chart-plot/</guid>
      <description>

&lt;p&gt;Last week I was analyzing server log data at work, and my boss asked me to plot it on a scatter chart. &amp;ldquo;No problem!&amp;rdquo; I thought, firing up Google Sheets. But Sheets slowed to a crawl when I uploaded 250,000 records to plot. Using CPAN I found something better: &lt;a href=&#34;https://metacpan.org/pod/Chart::Plot&#34;&gt;Chart::Plot&lt;/a&gt;. It has a simple interface and is really fast. Here&amp;rsquo;s how you use it.&lt;/p&gt;

&lt;h2 id=&#34;a-scatter-plot-script&#34;&gt;A scatter plot script&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-perl&#34; data-lang=&#34;perl&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/env perl&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; Chart::Plot;

open &lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $mlb, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017-so-hr.csv&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; die $!;

&lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; @series;
&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;$mlb&amp;gt;&lt;/span&gt;) {
  chomp;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; ($x, $y) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; split &lt;span style=&#34;color:#e6db74&#34;&gt;/\t/&lt;/span&gt;;
  push @series, $x, $y;
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Chart::Plot&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;800&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;640&lt;/span&gt;);
$img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;setData(&lt;span style=&#34;color:#f92672&#34;&gt;\&lt;/span&gt;@series, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Points Noline Blue&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; die $img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;error;

$img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;setGraphOptions (
  vertGraphOffset &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,
  horGraphOffset  &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;75&lt;/span&gt;,
  vertAxisLabel   &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;HR&amp;#39;&lt;/span&gt;,
  horAxisLabel    &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SO&amp;#39;&lt;/span&gt;,
  title           &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017 MLB Strikeouts and Home Runs&amp;#39;&lt;/span&gt;,
);

open &lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $fh, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;gt;:raw&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017-mlb-so-hr.png&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; die $!;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; $fh $img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;draw();&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To demo Chart::Plot, I&amp;rsquo;ve extracted MLB 2017 pitching data from the &lt;a href=&#34;http://www.seanlahman.com/baseball-archive/statistics/&#34;&gt;Lahman database&lt;/a&gt; for strikeouts and home runs given up to see if there&amp;rsquo;s a relationship between them.&lt;/p&gt;

&lt;p&gt;My script opens the csv (which is tab separated) and parses it, pushing the columns into &lt;code&gt;@series&lt;/code&gt;. It creates a new 800x640 pixel Chart::Plot object and calls the &lt;a href=&#34;https://metacpan.org/pod/Chart::Plot#Acquire-a-dataset:-setData()&#34;&gt;setData&lt;/a&gt; method, passing it an arrayref to &lt;code&gt;@series&lt;/code&gt;, and a style string describing the shape and color to plot the series with. Here is the output file:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/find-relationships-quickly-in-data-with-chart-plot/2017-mlb-so-hr.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A cursory look at this data would suggest there &lt;em&gt;is&lt;/em&gt; a correlation between strikeouts and home runs, maybe hard-throwing pitchers throw more strikes, but the additional velocity gives up more home runs?&lt;/p&gt;

&lt;h2 id=&#34;multiple-series&#34;&gt;Multiple series&lt;/h2&gt;

&lt;p&gt;To display multiple series on the chart, simply call &lt;code&gt;setData&lt;/code&gt; for each additional series to add. I&amp;rsquo;ve updated the MLB data to include which league the pitcher was throwing in:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-perl&#34; data-lang=&#34;perl&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/env perl&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;use&lt;/span&gt; Chart::Plot;

open &lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $mlb, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017-so-hr-lg.csv&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; die $!;

&lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; (@nl, @al);
&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;$mlb&amp;gt;&lt;/span&gt;) {
  chomp;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; ($x, $y, $league) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; split &lt;span style=&#34;color:#e6db74&#34;&gt;/\t/&lt;/span&gt;;

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ($league &lt;span style=&#34;color:#f92672&#34;&gt;eq&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;NL&amp;#39;&lt;/span&gt;) {
    push @nl, $x, $y;
  }
  &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
    push @al, $x, $y;
  }
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Chart::Plot&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;800&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;640&lt;/span&gt;);
$img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;setData(&lt;span style=&#34;color:#f92672&#34;&gt;\&lt;/span&gt;@nl, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Points Noline Blue&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; die $img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;error;
$img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;setData(&lt;span style=&#34;color:#f92672&#34;&gt;\&lt;/span&gt;@al, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Points Noline Red&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; die $img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;error;

$img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;setGraphOptions (
  vertGraphOffset &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;,
  horGraphOffset  &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;75&lt;/span&gt;,
  vertAxisLabel   &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;HR&amp;#39;&lt;/span&gt;,
  horAxisLabel    &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;SO&amp;#39;&lt;/span&gt;,
  title           &lt;span style=&#34;color:#f92672&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017 MLB Strikeouts and Home Runs&amp;#39;&lt;/span&gt;,
);

open &lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $fh, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;gt;:raw&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;2017-mlb-so-hr-lg.png&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; die $!;
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; $fh $img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;draw();&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I separate the input data into two series, one for the National League and one for the American League. I then call &lt;code&gt;setData&lt;/code&gt; for each series, using blue and red styles. Here is the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/find-relationships-quickly-in-data-with-chart-plot/2017-mlb-so-hr-lg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;non-transparent-charts&#34;&gt;Non-transparent charts&lt;/h2&gt;

&lt;p&gt;By default, Chart::Plot produces charts with a transparent background. If you wanted to add a white background you could use &lt;code&gt;convert&lt;/code&gt; at the command line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ convert -flatten 2017-mlb-so-hr.png 2017-mlb-so-hr-whitebg.png
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But Chart::Plot gives you access to the underlying GD graphics object, so why not manipulate it directly in the script?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-perl&#34; data-lang=&#34;perl&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $gd &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; $img&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;getGDobject();
&lt;span style=&#34;color:#66d9ef&#34;&gt;my&lt;/span&gt; $white &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; $gd&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;colorAllocate(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;);
$gd&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;filledRectangle(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;798&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;638&lt;/span&gt;,$white);&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This snippet creates a new white color by calling &lt;code&gt;colorAllocate&lt;/code&gt; with the RGB values for white, which creates the color and returns its &lt;a href=&#34;https://en.wikipedia.org/wiki/Indexed_color&#34;&gt;index&lt;/a&gt;. Then it draws a white rectangle starting at the top-left pixel (plus 1 to avoid overwriting the border) and ending at the bottom-right pixel (minus 1).&lt;/p&gt;

&lt;h2 id=&#34;strikeouts-and-home-runs-revisited&#34;&gt;Strikeouts and home runs revisited&lt;/h2&gt;

&lt;p&gt;If I divide each pitcher&amp;rsquo;s strikeouts and home runs by the number of innings they pitched in 2017, the data tells a different story:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/find-relationships-quickly-in-data-with-chart-plot/2017-mlb-so-hr-ip.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Strikeouts and home runs given up increase as pitchers throw for more innings, but the relationship between them has all but disappeared&amp;hellip; there goes my sabermetrics career.&lt;/p&gt;

&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap up&lt;/h2&gt;

&lt;p&gt;Chart::Plot is great for generating scatter or line charts when you have a lot of data. Its simple interface and sane defaults make charting easy.&lt;/p&gt;

&lt;p&gt;However, as its style strings only contain four colors, you can only plot 4 different data series on a single chart. The chart text styling is hardcoded to a rather spartan style. And whilst it does give you access to the underlying GD object, it can be quite cumbersome to draw on the image pixel-by-pixel. Pie and bar charts are not supported, so you&amp;rsquo;ll need to use a different module for those. It isn&amp;rsquo;t a general-purpose charting library, it&amp;rsquo;s a specialized instrument. Everyone&amp;rsquo;s toolbox should have room for a few of those.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Munging for Non-Programming Biologists</title>
      <link>http://localhost:1313/pub/2005/10/20/scriptome.html/</link>
      <pubDate>Thu, 20 Oct 2005 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2005/10/20/scriptome.html/</guid>
      <description>

&lt;p&gt;Have you ever renamed 768 files? Merged the content from 96 files into a spreadsheet? Filtered 100 lines out of a 20,000-line file?&lt;/p&gt;

&lt;p&gt;Have you ever done these things by hand?&lt;/p&gt;

&lt;p&gt;Disciples of laziness&amp;ndash;one of the three Perl programmer&amp;rsquo;s virtues&amp;ndash;know that you should never repeat anything five times, let alone 768. It dismayed me to learn that biologists do this kind of thing all the time.&lt;/p&gt;

&lt;h3 id=&#34;on-the-origin-of-scripts-the-problem&#34;&gt;On the Origin of Scripts: The Problem&lt;/h3&gt;

&lt;p&gt;Experimental biologists increasingly face large sets of large files in often-incompatible formats, which they need to filter, reformat, merge, and otherwise &lt;a href=&#34;http://www.catb.org/~esr/jargon/html/M/munge.html&#34;&gt;munge&lt;/a&gt; (definition 3). Biologists who can&amp;rsquo;t write Perl (most of them) often end up editing large files by hand. When they have the same problem a week later, &lt;em&gt;they do the same thing again&lt;/em&gt;&amp;ndash;or they just give up.&lt;/p&gt;

&lt;p&gt;My job description includes helping biologists to use computers. I could just write tailored, one-off scripts for them, right? As an answer, let me tell you about Neeraj. Neeraj is a typical NPB (non-programming biologist) who works down the hall. He came into my office, saying, &amp;ldquo;I have 12,000 sequences that I need to make primers for.&amp;rdquo; I said, &amp;ldquo;Make what?&amp;rdquo; (I haven&amp;rsquo;t been doing biology for very long.) Luckily, we figured out pretty quickly that all he wants to do is get characters 201-400 from each DNA sequence in a file. Those of you who have been Perling for a while can do this with your eyes closed (if you can touch type):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;perl -ne &#39;print substr($_, 200, 200), &amp;quot;\n&amp;quot;&#39; sequences.in &amp;gt;
    primers.out
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Voilá! I gave Neeraj his output file and he went away, happy, to finish building his clone army to take over the world. (Or was he going to genetically modify rice to solve world hunger? I keep forgetting.)&lt;/p&gt;

&lt;p&gt;Unfortunately, that wasn&amp;rsquo;t the end. The next day, Neeraj came back, because he also wanted primers from the back end of the sequences (&lt;code&gt;substr($_, -400, 200)&lt;/code&gt;). Because he&amp;rsquo;s doing cutting-edge research, he may have totally different requirements next month, when he finishes his experiments. With just a few people in our group supporting hundreds or even thousands of biologists, writing tailored scripts, even quick one-liners, doesn&amp;rsquo;t scale. Other common solutions, such as teaching biologists Perl or creating graphical workflow managers, didn&amp;rsquo;t seem to fully address the data manipulation problem especially for occasional users, who won&amp;rsquo;t be munging every day.&lt;/p&gt;

&lt;p&gt;We need some tool that allows Neeraj, or any NPB, to munge his own data, rather than relying on (and explaining biology to) a programmer. Keeping the biologist in the loop this way gives him the best chance of applying the relevant data and algorithms to answer the right questions. The tool must be easy for a non-programmer to learn and to remember after a month harvesting fish eyes in Africa. It should also be TMTOWTDI-compliant, allowing him to play with data until he can sculpt it in the most meaningful way. While we&amp;rsquo;re at it, the tool will need to evolve rapidly as biologists ask new questions and create new kinds of data at an ever-increasing rate.&lt;/p&gt;

&lt;p&gt;When I told Neeraj&amp;rsquo;s story to others in our group, they said that they have struggled with this problem for years. During one of our brainstorming sessions, my not-so-pointy-haired boss, Eitan Rubin, said, &amp;ldquo;Wouldn&amp;rsquo;t it be nice if we could just give them a book of magical data-munging scripts that Just Work?&amp;rdquo; &amp;ldquo;Hm&amp;ndash;a sort of Script Tome?&amp;rdquo; And thus the Scriptome was born. (The joke here is that every self-respecting area of study in biology these days needs to have &amp;ldquo;ome&amp;rdquo; in its name: the genome, proteome, metabolome. There&amp;rsquo;s even a journal called &lt;em&gt;OMICS&lt;/em&gt; now.)&lt;/p&gt;

&lt;h3 id=&#34;harnessing-the-power-of-the-atom&#34;&gt;Harnessing the Power of the Atom&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://sysbio.harvard.edu/csb/resources/computational/scriptome/&#34;&gt;The Scriptome&lt;/a&gt; is a cookbook for munging biological data. The cookbook model nicely fits the UNIX paradigm of small tools that do simple operations. Instead of UNIX pipes, though, we encourage the use of intermediate files to avoid errors.&lt;/p&gt;

&lt;p&gt;We use a couple of tricks in order to make this cookbook accessible to NPBs. We use the familiar web browser as our GUI and harness the power of hyperlinking to develop a highly granular, hierarchical table of contents for the tools. This means we can include dozens to hundreds of tools, without requiring users to remember command names. Another trick is syntax highlighting. We gray out most of the Perl, to signify that reading it is optional. Parameters&amp;ndash;such as filenames, or maximum values to filter a certain column by&amp;ndash;we highlight in attention-getting red. Finally, we make a conscious effort to avoid computer science or invented terminology. Instead, we use language biologists find familiar. For example, tools are &amp;ldquo;atoms,&amp;rdquo; rather than &amp;ldquo;snippets.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Each Scriptome tool consists of a Perl one-liner in a colored box, along with a couple of sentences of documentation (any more than that and no one will read it), and sample inputs and outputs. In order to use a tool, you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pick a tool type, perhaps &amp;ldquo;Choose&amp;rdquo; to choose certain lines or columns from a file.&lt;/li&gt;
&lt;li&gt;Browse a hierarchical table of contents.&lt;/li&gt;
&lt;li&gt;Cut and paste the code from the colored box onto a Unix, Mac OS X, or Windows command line. (Friendlier interfaces are in alpha testing&amp;ndash;a later section explains more.)&lt;/li&gt;
&lt;li&gt;Change red text as desired, using arrow keys or a text editor.&lt;/li&gt;
&lt;li&gt;Hit &lt;code&gt;Enter&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;That&amp;rsquo;s it!&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&#34;http://localhost:1313/media/_pub_2005_10_20_scriptome/perl_com_pic.gif&#34;&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2005_10_20_scriptome/perl_com_pic_sm.gif&#34; alt=&#34;Figure 1&#34; width=&#34;300&#34; height=&#34;114&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
Figure 1. A Scriptome tool for finding unique lines in a file--click image for full-size screen shot.&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The tool in Figure 1 reads the input line by line, using Perl&amp;rsquo;s &lt;code&gt;-n&lt;/code&gt; option, and prints each line only when it sees the value in a given, user-editable column for the first time. The substitution removes a newline, even if it&amp;rsquo;s a Windows file being read on a UNIX machine. Then the line is split on tabs. A hash keeps track of unique values in the given column, deciding which lines to print. Finally, the script prints to the screen a very quick diagnostic, specifically how many lines it chose out of how many total lines it read. (Choosing all lines or zero lines may mean you&amp;rsquo;re not filtering correctly.)&lt;/p&gt;

&lt;p&gt;By cutting and pasting tools like this, a biologist can perform basic data munging operations, without any programming knowledge or program installation (except for ActivePerl on Windows). Unfortunately, that&amp;rsquo;s still not really enough to solve real-world problems.&lt;/p&gt;

&lt;h3 id=&#34;splicing-the-scriptome&#34;&gt;Splicing the Scriptome&lt;/h3&gt;

&lt;p&gt;As it happens, the story I told you about Neeraj earlier wasn&amp;rsquo;t entirely accurate. He actually wanted to print both the beginning and ending substrings from his sequences. Also, his input was in the common FASTA format, where each sequence has an ID line like &lt;code&gt;&amp;gt;A2352334&lt;/code&gt; followed by a variable number of lines with DNA letters. We don&amp;rsquo;t have one tool that parses FASTA and takes out two different substrings; writing every possible combination of tools would take even longer than writing Perl 6 (ahem). Instead, again following UNIX, we leave it up to the biologist to combine the tools into problem-specific solutions. In this case, that solution would involve using the FASTA-to-table converter, followed by a tool to pull out the sequence column, and then two copies of the substring tool.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re asking biologists to break a problem down into pieces&amp;ndash;each of which is solvable using some set of tools&amp;ndash;and then to string those tools together in the right order with the right parameters. That sounds an awful lot like programming, doesn&amp;rsquo;t it? Although you may not think about it anymore, some of the fundamental concepts of programming are new and difficult. Luckily, it turns out that biologists learned more in grad school than how to extract things out of (reluctant) other things. In fact, they already know how to break down problems, loop, branch, and debug; instead of programming, though, they call it developing protocols. They also already have &lt;a href=&#34;http://www.molecularcloning.com/public/tour/index.html&#34;&gt;cookbooks for experimental molecular biology&lt;/a&gt;. Such a protocol might include lines like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Add 1 ml of such-and-such enzyme to the DNA.&lt;/li&gt;
&lt;li&gt;Incubate test tube at 90 degrees C for an hour.&lt;/li&gt;
&lt;li&gt;If the mixture turns clear, goto step 5.&lt;/li&gt;
&lt;li&gt;Repeat steps 2-3 three times.&lt;/li&gt;
&lt;li&gt;Pour liquid into a sterile bottle &lt;em&gt;very carefully&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We borrowed the term &amp;ldquo;protocol&amp;rdquo; to describe an ordered set of parameterized Scriptome tools that solves a larger problem. (The right word for this is a script, but don&amp;rsquo;t tell our users&amp;ndash;they might realize they&amp;rsquo;re learning how to program.) We feature some pre-written protocols on the website. Note that because each tool is a command-line command, a set of them together is really just an executable shell script.&lt;/p&gt;

&lt;p&gt;The Scriptome may be even more than a high-level, mostly syntax-free, non-toy language for NPBs. Because it exposes the Perl directly on the website&amp;ndash;giving new meaning to the term &amp;ldquo;open source&amp;rdquo;&amp;ndash;some curious biologists may even start reading the short, simple, relevant examples of Perl code. (Unfortunately, putting the command into one line makes it harder to read. One of our TODOs is an Explain button next to each tool, which would show you a commented, multi-line version of each script.) From there, it&amp;rsquo;s a short hop to tweaking the tools, and before you know it, we&amp;rsquo;ll have more annoying newbie posts on &lt;em&gt;comp.lang.perl.misc&lt;/em&gt;!&lt;/p&gt;

&lt;h3 id=&#34;intelligent-design-the-geeky-details&#34;&gt;Intelligent Design: The Geeky Details&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;ve read this far, you may have realized by now that the Scriptome is not a programming project at heart. Design, interface, documentation, and examples are as important as the programming itself, which is pretty easy. This being an article on Perl.com, though, I want to discuss the use of Perl throughout the project.&lt;/p&gt;

&lt;h4 id=&#34;why-perl&#34;&gt;Why Perl?&lt;/h4&gt;

&lt;p&gt;Several people asked me why we didn&amp;rsquo;t write the Scriptome in Python, or R, or just use UNIX &lt;code&gt;sh&lt;/code&gt; for everything. Well, other than the obvious (&amp;ldquo;It&amp;rsquo;s the One True Language!&amp;rdquo;), Perl data munges by design, it&amp;rsquo;s great for fast tool development, it&amp;rsquo;s portable to many platforms, and it&amp;rsquo;s already installed on every Unix and Mac OS X box. Moreover, the &lt;a href=&#34;http://bioperl.org/&#34;&gt;Bioperl&lt;/a&gt; modules offer me a huge number of tools to steal, um, reuse. Finally, Perl is the preferred language of the entire Scriptome development team (me).&lt;/p&gt;

&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;div class=&#34;secondary&#34;&gt;
&lt;h4 id=&#34;what-kind-of-perl&#34; align=&#34;center&#34;&gt;What kind of Perl?&lt;/h4&gt;
&lt;p&gt;Perl allows you to write pretty impressive tools in only a couple of hundred characters, with Perl Golf tricks such as the &lt;code&gt;-n&lt;/code&gt; option, autovivification, and the implicit &lt;code&gt;$_&lt;/code&gt; variable. On the other hand, we want the code to be readable, especially if we want newbies to learn from it, so we can&#39;t use too many Golf shortcuts. (For example, here&#39;s the winning solution in the Perl Golf contest for a script to find the last non-zero digit of N factorial by Juho Snellman:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; #!perl -l $_*=$`%9e9,??for+1=~?0*$?..pop;print$`%10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some might consider this difficult for newbies to read.)&lt;/p&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&#34;the-scriptome-build&#34;&gt;The Scriptome Build&lt;/h4&gt;

&lt;p&gt;Even though we&amp;rsquo;re trying to keep the tools pretty generic and simple, we know we&amp;rsquo;ll need several dozen at least, to be at all useful. In addition, data formats and biologists&amp;rsquo; interests will change over time. We knew we had to make the process of creating a new tool fast and automatic.&lt;/p&gt;

&lt;p&gt;I write the tool pages in POD, which lets me use Vim rather than a fancy web-page editor. My Makefile runs &lt;code&gt;pod2html&lt;/code&gt; to create a nice, if simple, web page that includes the table of contents for free. A Perl filter then adds a navigation bar and some simple interface-enhancing JavaScript, and makes the parameters red. I may give in and switch to a templating system, database back end, or XML eventually, and automated testing would be great. For now, keeping it simple means I can create, test, document, and publish a new tool in under an hour. (Okay, I didn&amp;rsquo;t include debugging in that time.)&lt;/p&gt;

&lt;h4 id=&#34;perl-culture&#34;&gt;Perl Culture&lt;/h4&gt;

&lt;p&gt;There&amp;rsquo;s lots of Perl code in the project, but I&amp;rsquo;m trying to incorporate some Perl &lt;em&gt;attitude&lt;/em&gt; as well. The &amp;ldquo;Aha!&amp;rdquo; moment of the Scriptome came when we realized we could just post a bunch of hacked one-liners on the Web to help biologists &lt;em&gt;now&lt;/em&gt;, rather than spend six or 12 months crafting the perfect solution. While many computational biologists focus on writing O(N) programs for sophisticated sequence analysis or gene expression studies, we&amp;rsquo;re not ashamed to write glue instead; we solve the unglamorous problem of taking the output from their fancy programs and throwing it into tabular format, so that a biologist can study the results in Excel. After all, if data munging is even one step in Neeraj&amp;rsquo;s pipeline, then he still can&amp;rsquo;t get his paper published without these tools. Finally, we&amp;rsquo;re listening aggressively to our users, because only they can tell us which easy things to make easy and which hard things to make possible.&lt;/p&gt;

&lt;h3 id=&#34;filling-the-niche-the-scriptome-and-other-solutions&#34;&gt;Filling the Niche: The Scriptome and Other Solutions&lt;/h3&gt;

&lt;p&gt;One of my greatest concerns in talking to people about biologists&amp;rsquo; data munging is that people don&amp;rsquo;t even realize that there&amp;rsquo;s a problem, or they think it&amp;rsquo;s already been solved. Biologists&amp;ndash;who happily pipette things over and over and over again&amp;ndash;don&amp;rsquo;t realize that computers could save them lots of time. Too many programmers figure that anyone who needs to can just read &lt;em&gt;Learning Perl&lt;/em&gt;. I&amp;rsquo;m all for that, of course, but experimental biologists need to spend much more of their time getting data (dissecting bee brains, say) than analyzing it, so they can&amp;rsquo;t afford the time it takes to become programmers. They shouldn&amp;rsquo;t have to. Does the average biologist need multiple inheritance, &lt;code&gt;getprotobyname()&lt;/code&gt;, and negative look-behind regexes? There&amp;rsquo;s a large body of problems out there that are too diverse for simple, inflexible tools to handle, but are too simple to need full-fledged programming.&lt;/p&gt;

&lt;p&gt;How about teaching a three-hour course with just enough Perl to munge simple data? At minimum, it should teach variables, arrays, hashes, regular expressions, and control structures&amp;ndash;and then there&amp;rsquo;s syntax. &amp;ldquo;Wait, what&amp;rsquo;s the difference between &lt;code&gt;@{$a[$a]}&lt;/code&gt; and &lt;code&gt;@a{$a[$a]}&lt;/code&gt; again?&amp;rdquo; &amp;ldquo;Oh, my, look at the time.&amp;rdquo; As Damian Conway writes in &amp;ldquo;&lt;a href=&#34;http://www.csse.monash.edu.au/~damian/papers/PDF/SevenDeadlySins.pdf&#34;&gt;Seven Deadly Sins of Introductory Programming Language Design&lt;/a&gt;&amp;rdquo; (PDF link), syntax oddities often distract newbies from learning basic programming concepts. How much can you teach in three hours, and how much will they remember after a month without practicing?&lt;/p&gt;

&lt;p&gt;Another route would be building a graphical program that can do everything a biologist would want, where pipelines are developed by dragging and dropping icons and connectors. Unfortunately, a comprehensive graphical environment requires a major programming effort to build, and to keep current. Not only that, but the interface for such a full-featured, graphical program will necessarily be complex, raising the learning barrier.&lt;/p&gt;

&lt;p&gt;In building the Scriptome, we purposely narrowed our scope, to maximize learnability and memorability for occasional users. While teaching programming and graphical tools are effective solutions for some, I believe the Scriptome fills an empty niche in the data munging ecosphere (the greposphere?).&lt;/p&gt;

&lt;h3 id=&#34;creation-is-not-easy&#34;&gt;Creation Is Not Easy&lt;/h3&gt;

&lt;p&gt;How much progress have we made in addressing the problem space between tool use and programming? Our early reviews have been mostly positive, or at least constructive. Suzy, our first power user, started out skeptical, saying she&amp;rsquo;d probably have to learn Perl because any tools we gave her wouldn&amp;rsquo;t be flexible enough. I encouraged her to use the Scriptome in parallel with learning Perl. She ended up a self-described &amp;ldquo;Scriptome monster,&amp;rdquo; tweaking tool code and creating a 16-step protocol that did real bioinformatics. Still, one good review won&amp;rsquo;t get you any Webby awards. Our first priority at this point is to build a user base and to get feedback on the learnability, memorability, and effectiveness of the website, with its 50 or so tools.&lt;/p&gt;

&lt;p&gt;It will take more than just feedback to implement the myriad ideas we have for improving the Scriptome, which is why I&amp;rsquo;m here to make a bald-faced plea for your help. The project needs lots of new tools, new protocols, and possibly new interfaces. You, the Perl.com reader, can certainly write code for new tools; the real question is whether you (unlike certain, unnamed CPAN contributors) can also write good documentation and examples, or find bugs in early versions of tools. We would also love to get relevant protocol ideas. Check out the &lt;a href=&#34;https://bioinformatics.org/project/?group_id=505&#34;&gt;Scriptome project page&lt;/a&gt; and send something to me or the &lt;em&gt;scriptome-users&lt;/em&gt; mailing list.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a little challenge. I really did have a client who renamed 768 files by hand before I could Perl it for him. Can you write a generic renaming atom that a NPB could use? (Hint: &amp;ldquo;Tell the user to learn regular expressions&amp;rdquo; is not a valid solution.) The winner will receive a commemorative plaque (&lt;code&gt;&amp;lt;bgcolor=&amp;quot;gold&amp;quot;&amp;gt;&lt;/code&gt;) on the Scriptome website.&lt;/p&gt;

&lt;p&gt;Speaking of new interfaces, one common concern we hear from programmers is that NPBs won&amp;rsquo;t be able or willing to handle the command-line paradigm shift and the few commands needed (&lt;code&gt;cd&lt;/code&gt;, &lt;code&gt;more&lt;/code&gt;, &lt;code&gt;dir/ls&lt;/code&gt;) to use the Scriptome. In case our users do tell us it&amp;rsquo;s a problem, we&amp;rsquo;re exploring a few different ways to wrap the Scriptome tools, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A Firefox plugin that gives you a command line in a toolbar and displays your output file in the browser. (Currently being developed by Rob Miller and his group at MIT.)&lt;/li&gt;
&lt;li&gt;An Excel VBA that lets you put command lines into a column, and creates a shell script out of it.&lt;/li&gt;
&lt;li&gt;Wrapping the command-line tools in &lt;a href=&#34;http://www.pasteur.fr/recherche/unites/sis/Pise/&#34;&gt;Pise&lt;/a&gt; (web forms around shell commands) or &lt;a href=&#34;http://www.genepattern.org/&#34;&gt;GenePattern&lt;/a&gt; (a more general GUI bio tool).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;ll probably try several of these avenues, because they allow us to keep using the command-line interface if desired.&lt;/p&gt;

&lt;p&gt;As for the future, well, who says that only biologists are interested in munging tabular data? Certainly, chemists and astronomers could get into this. I set my sights even higher. How about a Scriptome for a business manager wanting to munge reports? An Apache Scriptome to munge your website&amp;rsquo;s access logs? An iTunes Scriptome to manage your music? Let&amp;rsquo;s give users the power to do what they want with their data.&lt;/p&gt;

&lt;p&gt;Sorry, &lt;em&gt;GUI Neanderthalis&lt;/em&gt;, but you can&amp;rsquo;t adapt to today&amp;rsquo;s data munging needs. Make room for &lt;em&gt;Homo Scriptiens&lt;/em&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parsing iCal Data</title>
      <link>http://localhost:1313/pub/2005/08/18/ical_dot.html/</link>
      <pubDate>Thu, 18 Aug 2005 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2005/08/18/ical_dot.html/</guid>
      <description>

&lt;p&gt;One of the attributes of a killer application is that it does something cool: it allows you to view or organize information in new and interesting ways, saves you time, or helps you win that auction bid. Yet one of the fundamental aggravations of applications in general is that they don&amp;rsquo;t always work well together; typically, you cannot send your data to mixed-and-matched applications unless they were explicitly designed to allow this. One of the great strengths of a language such as Perl is its ability to overcome these differences and act as &amp;ldquo;glue.&amp;rdquo; As long as you can figure out what the incoming data looks like, and how the outgoing data should look, it is very simple to share data between previously incompatible applications. By simply building a parser between the applications and creating input files for the target application from the former&amp;rsquo;s data, you extend the usefulness of your tools. In a sense, you can create killer applications out of various mundane tools on your system.&lt;/p&gt;

&lt;p&gt;A somewhat trivial example of this sort of creation is an application that converts iCalendar data into a directed graph, readable by an application such as GraphViz. This example seems so trivial that you might ask yourself why you would wish to do such a thing. The answer is perhaps equally trivial: aside from the challenge factor, the ability to convert data could provide an alternative (or complement) to Gantt charts in project documentation, map relationships between events, etc. Moreover, by providing a simple way to allow disparate applications to interoperate, you can cumulatively build suites of applications, hopefully allowing for unforeseen advantages in the future.&lt;/p&gt;

&lt;p&gt;Returning to the example, say you would like to take an iCal calendar (Figure 1) and turn it into an interesting visualization (Figure 2). How would you do this? Such an ability to convert formats is one step in constructing that killer application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2005_08_18_ical_dot/ical_screen.gif&#34; alt=&#34;an iCal calendar&#34; width=&#34;500&#34; height=&#34;366&#34; /&gt;
&lt;em&gt;Figure 1. An iCal calendar&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2005_08_18_ical_dot/ical_dot.gif&#34; alt=&#34;an alternate visualization&#34; width=&#34;500&#34; height=&#34;254&#34; /&gt;
&lt;em&gt;Figure 2. An alternate visualization of the calendar data&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;reading-the-icalendar-format&#34;&gt;Reading the iCalendar Format&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.ietf.org/rfc/rfc2446.txt&#34;&gt;RFC 2446&lt;/a&gt; defines the iCalendar format, which Apple&amp;rsquo;s iCal application uses. Each iCalendar file represents an individual calendar and contains at least one block of event data in key:value tuples, starting with a &lt;code&gt;BEGIN:VEVENT&lt;/code&gt; tuple and ending with &lt;code&gt;END:VEVENT&lt;/code&gt;. Here is an example (with indentation added for readability) of a small iCalendar file containing two events:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BEGIN:VCALENDAR
        CALSCALE:GREGORIAN
        PRODID:-//Apple Computer\, Inc//iCal 2.0//EN
        VERSION:2.0
        BEGIN:VEVENT
                LOCATION:San Francisco
                DTSTAMP:20050618T151130Z
                UID:BDF17182-CA21-4752-8D4F-40A4FE47C90D
                SEQUENCE:8
                URL;VALUE=URI:http://developer.apple.com/wwdc/
                DTSTART;VALUE=DATE:20050606
                SUMMARY:Apple WWDC
                DTEND;VALUE=DATE:20050612
                DESCRIPTION:Lots of sessions.
        END:VEVENT
        BEGIN:VEVENT
                DURATION:PT1H
                LOCATION:Home
                DTSTAMP:20050618T151543Z
                UID:5F88A0EC-AD21-428E-AAAD-005F1B1AB72E
                SEQUENCE:6
                DTSTART;TZID=America/Chicago:20050615T180000
                SUMMARY:Set up File Server
                DESCRIPTION:Music server for the kids.
        END:VEVENT
END:VCALENDAR
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are several possible approaches to parsing the above data in Perl, but perhaps the easiest one is to create a hash of events, modeled after the iCalendar structure. With this approach, a single calendar becomes a hash of hashes with a key:value pair for each event, where the key is the event ID and the value is a hash containing the event data. While it would be just as easy to &lt;em&gt;store&lt;/em&gt; the data as an array of hashes, the ability to pull an event by its ID allows greater flexibility and power to &lt;em&gt;manipulate&lt;/em&gt; the data. The data for a single event might look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Calendar-&amp;gt;EventUID = { &#39;UID&#39;         =&amp;gt; EventUID,
                       &#39;LOCATION&#39;    =&amp;gt; EventLocation,
                       &#39;START&#39;       =&amp;gt; EventStart,
                       &#39;END&#39;         =&amp;gt; EventEnd,
                       &#39;DURATION&#39;    =&amp;gt; EventDuration,
                       &#39;DTSTAMP&#39;     =&amp;gt; EventDatestamp,
                       &#39;SEQUENCE&#39;    =&amp;gt; EventSequence,
                       &#39;SUMMARY&#39;     =&amp;gt; EventSummary,
                       &#39;DESCRIPTION&#39; =&amp;gt; EventDescription,
                       &#39;URL&#39;         =&amp;gt; EventURL };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that these keys represent only a subset of all possibilities as defined in RFC 2246. Each event may not contain all of the above keys. For example, the first event in my example does not contain &lt;code&gt;DURATION&lt;/code&gt;. Further, certain keys (such as &lt;code&gt;SEQUENCE&lt;/code&gt;) may be irrelevant for your purposes.&lt;/p&gt;

&lt;p&gt;With the data structure designed, what&amp;rsquo;s the right way to convert iCalendar data into such a structure? Realizing the mantra of Perl, that there is more than one way to do things, perhaps the easiest approach is to match key names, starting a new event block when the parser sees &lt;code&gt;BEGIN:VEVENT&lt;/code&gt; and ending it when &lt;code&gt;END:VEVENT&lt;/code&gt; appears. Given the large number of possible keys, it may be easiest to use switch-like behavior. Here is an example of how to do this, splitting a key:value on the colon character (as the semicolon precedes any modifiers to the data):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SWITCH: {
        if ( $_ =~ /BEGIN:VEVENT/ ) {
                ##-----------------------------------------
                ## We have a new event, so start fresh.
                ##-----------------------------------------
                $eventHash = {};
                last SWITCH; }


        if ( $_ =~ /END:VEVENT/ ) {
                ##-----------------------------------------
                ## We hit the event end, so store it.
                ##-----------------------------------------
                $calHash-&amp;gt;{$eventHash-&amp;gt;{&#39;UID&#39;}} = 
                {
                     &#39;UID&#39;         =&amp;gt; $eventHash-&amp;gt;{&#39;UID&#39;},
                     &#39;LOCATION&#39;    =&amp;gt; $eventHash-&amp;gt;{&#39;LOCATION&#39;},
                      #...The rest of our keys...
                     &#39;URL&#39;         =&amp;gt; $eventHash-&amp;gt;{&#39;URL&#39;} 
                };
                last SWITCH; }


          ## we will split the key:value pair into an array 
             and grab the value (1st element)
        if ( $_ =~ /^UID/ ) {
                $eventHash-&amp;gt;{&#39;UID&#39;} = ( split ( /:/, $_ ) )[1];
                last SWITCH; }


        if ( $_ =~ /^LOCATION/ ) {
                $eventHash-&amp;gt;{&#39;LOCATION&#39;} = ( split ( /:/, $_ ) )[1];
                last SWITCH; }

...The rest of our key matches...

        if ( $_ =~ /^URL/ ) {
                $eventHash-&amp;gt;{&#39;DESCRIPTION&#39;} = ( split ( /:/, $_ ) )[1];
                last SWITCH; }

} # end switch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While this example does a good job of showing how to fill the data structure, it does a poor job of leveraging the power of Perl. More extensive use of regular expressions, the use of one of the Parse modules in CPAN, or even a bit of recursive programming could make this code more elegant and perhaps even a bit faster. However, these tactics may also make the code a bit harder to read&amp;ndash;which is not always bad, unless you are attempting to explain concepts in an article. For further ideas, Toedor Zlatanov has written an article on &lt;a href=&#34;http://www-106.ibm.com/developerworks/linux/library/l-perl-parsing/&#34;&gt;using Perl parsing modules&lt;/a&gt; as well as a real mind-bender on &lt;a href=&#34;http://www-128.ibm.com/developerworks/linux/library/l-road4.html&#34;&gt;using a functional programming approach in Perl&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;the-dot-specification&#34;&gt;The Dot Specification&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.graphviz.org/Documentation/dotguide.pdf&#34;&gt;Dot&lt;/a&gt; (PDF) is a diagramming, or directed, graph language created by Emden Gansner, Eleftherios Koutsofios, and Stephen North at Bell Labs. There are several implementations of Dot, including &lt;a href=&#34;http://www.graphviz.org&#34;&gt;GraphViz&lt;/a&gt;, &lt;a href=&#34;http://www.graphviz.org/webdot/&#34;&gt;WebDot&lt;/a&gt;, and &lt;a href=&#34;http://www.research.att.com/~john/Grappa/&#34;&gt;Grappa&lt;/a&gt;. Interestingly, &lt;a href=&#34;http://www.omnigroup.com/applications/omnigraffle/&#34;&gt;OmniGraffle&lt;/a&gt;, a powerful diagramming tool for Macintosh computers, can read simple Dot files.&lt;/p&gt;

&lt;h3 id=&#34;creating-dot-files&#34;&gt;Creating Dot Files&lt;/h3&gt;

&lt;p&gt;The basic syntax of Dot is that there are objects or things that you describe by adding data within digraph &lt;code&gt;{}&lt;/code&gt; braces. You denote relationships between objects with the &lt;code&gt;-&amp;gt;&lt;/code&gt; combination of characters. With this code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;digraph my_first_graph {
  object1 -&amp;gt; object2;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;your Dot-driven application (such as GraphViz) will display an image something like Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2005_08_18_ical_dot/firstgraph.gif&#34; alt=&#34;a simple graph&#34; width=&#34;103&#34; height=&#34;159&#34; /&gt;
&lt;em&gt;Figure 3. A simple graph&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The specification describes additional complexity in terms of sub-objects/structures, alternate shapes (the default is an oval), ranking, and more. One additional item worth noting is that Dot recognizes comments in C- and Java-style formats (&lt;code&gt;//&lt;/code&gt; and &lt;code&gt;/*&lt;/code&gt;). To help troubleshoot problems (and for good coding practice), I suggest that your parser insert comments into the Dot input file.&lt;/p&gt;

&lt;p&gt;Consider how you might create a Dot file from the data parsed earlier. If you pass to the function that handles the writing of the Dot file the reference to the filehandle of your Dot input file (the output of your conversion) along with the reference to your parsed data structure, then you might generate your Dot file along these lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;##------------------------------
  ## Name our Dot graph 
  ##------------------------------
  if ( $raw-&amp;gt;{&#39;CALNAME&#39;} ) {
      print { $$file } &#39;digraph &amp;quot;&#39;. $raw-&amp;gt;{&#39;CALNAME&#39;} .&amp;quot;\&amp;quot; {\n\n&amp;quot;;
   } elsif ( $$raw{&#39;CALID&#39;} ) {
      print { $$file } &#39;digraph &amp;quot;&#39;. $raw-&amp;gt;{&#39;CALID&#39;} .&amp;quot;\&amp;quot; {\n\n&amp;quot;;
   } else {
      print { $$file } &amp;quot;digraph unnamed {\n\n&amp;quot;;
   }


   ##-----------------------------------------
   ## Some optional rendering info
   ##-----------------------------------------
   print { $$file } &#39;   size     = &amp;quot;10,7&amp;quot;;&#39;. &amp;quot;\n&amp;quot;.
                    &#39;   compound = true;&#39;  . &amp;quot;\n&amp;quot;.
                    &#39;   ratio    = fill;&#39;  . &amp;quot;\n&amp;quot;.
                    &#39;   rankdir  = LR;&#39;    . &amp;quot;\n\n&amp;quot;;


   ##-----------------------------------------
   ## Generate our Dot data
   ##   we will wrap most data in double-quotes 
   ##   since most Dot interpreters don&#39;t like spaces, 
   ##   something allowed in iCal data
   ##-----------------------------------------
   foreach $key ( keys %$raw ) {
      if ( ref( $raw-&amp;gt;{$key} ) eq &#39;HASH&#39; ) {
         my $block = $raw-&amp;gt;{$key};

           ##------------------------------
           ## graphViz doesn&#39;t like - in names
           ##------------------------------
         $block-&amp;gt;{&#39;UID&#39;} =~ s/-/_/g;

           ##------------------------------
           ## produce list of all unique tasks
           ##------------------------------
         push( @{ $tasks-&amp;gt;{$block-&amp;gt;{&#39;SUMMARY&#39;}} }, &#39;&amp;quot;&#39;. $block-&amp;gt;{&#39;UID&#39;} .&#39;&amp;quot;&#39; );

           ##------------------------------
           ## build record
           ##------------------------------
         my $eventBlock = &#39;&amp;quot;&#39;. $block-&amp;gt;{&#39;UID&#39;} .
                          &#39;&amp;quot; [ shape = record, label = &amp;quot;&#39;. $block-&amp;gt;{&#39;SUMMARY&#39;} .
                           &#39; | &amp;lt;START&amp;gt; Start | &amp;lt;END&amp;gt; End &#39;;

         if ( $block-&amp;gt;{&#39;DESCRIPTION&#39;} ) {
            $eventBlock .= &#39; | &#39;. $block-&amp;gt;{&#39;DESCRIPTION&#39;};
         }
         $eventBlock .= &#39;&amp;quot;];&#39;;

         print { $$file } &#39;   &#39;. $eventBlock .&amp;quot;\n\n&amp;quot;;


            ##------------------------------
            ## build relations based upon time
            ##------------------------------
         push( @timeLine,    &#39;&amp;quot;&#39;. $block-&amp;gt;{&#39;START&#39;} .&#39;&amp;quot;&#39; );
         print { $$file } &#39;   &amp;quot;&#39;. $block-&amp;gt;{&#39;UID&#39;} .&#39;&amp;quot;:START  
            -&amp;gt; &amp;quot;&#39;. $block-&amp;gt;{&#39;START&#39;} .&amp;quot;\&amp;quot;\;\n\n&amp;quot;;

         if ( $$block{&#39;END&#39;} ) {
            push( @timeLine,    &#39;&amp;quot;&#39;. $block-&amp;gt;{&#39;END&#39;} .&#39;&amp;quot;&#39; );
            print { $$file } &#39;   &amp;quot;&#39;. $block-&amp;gt;{&#39;UID&#39;} .&#39;&amp;quot;:END    
               -&amp;gt; &amp;quot;&#39;. $block-&amp;gt;{&#39;END&#39;} .&amp;quot;\&amp;quot;\;\n\n&amp;quot;;
         }

         print { $$file } &amp;quot;\n\n&amp;quot;;

}

      ##------------------------------
      ## tie non-unique tasks
      ##------------------------------
    print { $$file } &#39;   // Create tasks relationships&#39;. &amp;quot;\n\n&amp;quot;;
    foreach ( keys %$tasks ) {
       if ( @{ $tasks-&amp;gt;{$_} } &amp;gt; 1 ) {
          print { $$file } &#39;   &#39;. join( &#39; -&amp;gt; &#39;, @{ $tasks-&amp;gt;{$_} } ) .&amp;quot;\;\n\n&amp;quot;;
       }
    }
    print { $$file } &amp;quot;\n\n&amp;quot;;


      ##------------------------------
      ## Render our timeline
      ##------------------------------
    print { $$file } &#39;   // Create timeline relationships&#39;. &amp;quot;\n\n&amp;quot;;
    print { $$file } &#39;   &#39;. join( &#39; -&amp;gt; &#39;, sort( @timeLine )) .&amp;quot;\;\n\n&amp;quot;;


      ##------------------------------
      ## Close off dot file
      ##------------------------------
    print { $$file } &amp;quot;}\n&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code will produce the following Dot file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;digraph unnamed {
   size     = &amp;quot;10,7&amp;quot;;
   compound = true;
   ratio    = fill;
   rankdir  = LR;

   &amp;quot;5F88A0EC_AD21_428E_AAAD_005F1B1AB72E&amp;quot; [ shape = record, 
      label = &amp;quot;Set up File Server | &amp;lt;START&amp;gt; Start | 
      &amp;lt;END&amp;gt; End  | Music server for the kids.&amp;quot;];

   &amp;quot;5F88A0EC_AD21_428E_AAAD_005F1B1AB72E&amp;quot;:START  -&amp;gt; &amp;quot;20050615T180000&amp;quot;;

   &amp;quot;BDF17182_CA21_4752_8D4F_40A4FE47C90D&amp;quot; [ shape = record, label = &amp;quot;WWDC | 
      &amp;lt;START&amp;gt; Start | &amp;lt;END&amp;gt; End  | Lots of sessions.&amp;quot;];

   &amp;quot;BDF17182_CA21_4752_8D4F_40A4FE47C90D&amp;quot;:START  -&amp;gt; &amp;quot;20050606&amp;quot;;

   &amp;quot;BDF17182_CA21_4752_8D4F_40A4FE47C90D&amp;quot;:END    -&amp;gt; &amp;quot;20050612&amp;quot;;

   // Create tasks relationships

   // Create timeline relationships

   &amp;quot;20050606&amp;quot; -&amp;gt; &amp;quot;20050612&amp;quot; -&amp;gt; &amp;quot;20050615T180000&amp;quot;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that this code uses the record shape, holding individual segments within the larger object. This is slightly more complicated than the default oval that Dot uses.&lt;/p&gt;

&lt;h3 id=&#34;where-to-go-from-here&#34;&gt;Where to Go from Here&lt;/h3&gt;

&lt;p&gt;If you are using Apple&amp;rsquo;s iCal application, note that the location and naming scheme of iCalendar files changed between the 1.x and 2.x releases. Previously, iCalendar files went in the &lt;em&gt;~/Library/Calendars/&lt;/em&gt; directory and had names of the form &lt;em&gt;&amp;lt;calendar name&amp;gt;.ics&lt;/em&gt;. Thus, a calendar named Work would have a file &lt;em&gt;Work.ics&lt;/em&gt;. However, the 2.x release keeps iCalendar information in the &lt;em&gt;~/Library/Application Support/iCal/Sources/&amp;lt;calendar name&amp;gt;/&lt;/em&gt; directory as &lt;em&gt;sources.ics&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Other applications that implement the iCalendar specification, such as Mozilla&amp;rsquo;s Calendar extension for Mozilla/Firefox/Thunderbird, may follow a different convention. On a Mac, Firefox stores .ics files in the &lt;em&gt;~/Library/Application Support/FireFox/Profiles/&amp;lt;profile&amp;gt;/Calendar/&lt;/em&gt; directory, where &lt;em&gt;&amp;lt;profile&amp;gt;&lt;/em&gt; is the profile specified in the Firefox &lt;em&gt;profile.ini&lt;/em&gt; file. Again, other systems will likely store this information in different locations.&lt;/p&gt;

&lt;p&gt;While on the topic of different implementations, bear in mind that, while the key:value specifications are consistent (as long as the application conforms to RFC 2246), the actual .ics file may look slightly different. For example, Firefox lays out that first event from the previous example as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BEGIN:VEVENT
UID
 :b9794c88-1dd1-11b2-bb51-8a92011a78e8
SUMMARY
 :Apple WWDC
DESCRIPTION
 :Lots of sessions
LOCATION
 :San Francisco
URL
 :http://developer.apple.com/wwdc
STATUS
 :TENTATIVE
CLASS
 :PRIVATE
DTSTART
 ;VALUE=DATE
 :20050606
DTEND
 ;VALUE=DATE
 :20050612
DTSTAMP
 :20050618T191731Z
END:VEVENT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here, the key:value tuples (plus any data modifiers such as &lt;code&gt;VALUE=DATE&lt;/code&gt;) almost always split up across lines. In this case, it would be best to handle this difference when reading in the .ics file, so that the rest of the script can expect data in a generic format. One way to do this is to copy the array representing the .ics file using a finite-state machine. Another method would be to walk the array and join array elements under certain conditions, such as if the first non-white-space character of the current element begins with a colon or semicolon character, or is simply non-alphabetic.&lt;/p&gt;

&lt;p&gt;Hopefully, this article will spur you to create a bridge between two of your favorite applications. Good luck, and please remember to share your contributions with the community.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Flood Control</title>
      <link>http://localhost:1313/pub/2004/11/11/floodcontrol.html/</link>
      <pubDate>Thu, 11 Nov 2004 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2004/11/11/floodcontrol.html/</guid>
      <description>

&lt;p&gt;Accordingly to Merriam-Webster Online, &amp;ldquo;flood&amp;rdquo; means:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1: a rising and overflowing of a body of water especially onto normally dry land;&lt;/p&gt;

&lt;p&gt;2: an overwhelming quantity or volume.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In computer software there are very similar situations when an unpredictable and irregular flow of events can reach higher levels. Such situations usually are not comfortable for users, either slowing down systems or having other undesired effects.&lt;/p&gt;

&lt;p&gt;Floods can occur from accessing web pages, requesting information from various sources (ftp lists, irc services, etc.), receiving SMS notification messages, and email processing. It is obvious that it is not possible to list all flood cases.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Flood control&amp;rdquo; is a method of controlling the processing-rate of a stream of events. It can reject or postpone events until there are available resources (CPU, time, space, etc.) for them. Essentially the flood control restricts the number of events processed in a specific period of time.&lt;/p&gt;

&lt;h3 id=&#34;span-id-closing-the-gates-closing-the-gates-span&#34;&gt;&lt;span id=&#34;closing_the_gates&#34;&gt;Closing the Gates&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;To maintain flood control, you must calculate the flood ratio, which is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_11_11_floodcontrol/flood2-eq1.gif&#34; alt=&#34;flood ratio equation&#34; width=&#34;73&#34; height=&#34;50&#34; /&gt;
&lt;em&gt;Figure 1. Flood ratio equation.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fr flood ratio
ec event count
tp time period for ec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To determine if a flood is occurring, compare the flood ratio to the fixed maximum (threshold) ratio. If the result is less than the threshold, there&amp;rsquo;s no flood. Accept the event. If the result is higher, refuse or postpone the event.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_11_11_floodcontrol/flood2-eq2.gif&#34; alt=&#34;comparing the ratios&#34; width=&#34;68&#34; height=&#34;50&#34; /&gt;
&lt;em&gt;Figure 2. Comparing the ratios.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ec event count
tp time period for ec
fc fixed event count (max)
fp fixed time period for fc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is possible to keep an array of timestamps of all events. Upon receipt of a new event, calculate the time period since the oldest event to use as the current count/time ratio. This approach has two drawbacks. The first is that it uses more and more memory to hold all of the timestamps. Suppose that you want only two events to happen inside a one-minute period, giving two events per minute. Someone can trigger a single event, wait half an hour, and finally flood you with another 58 requests. At this point the ratio will be 1.9/min., well below the 2/min. limit. This is the second drawback.&lt;/p&gt;

&lt;p&gt;A better approach is to keep a sliding window either of events (&lt;code&gt;fc&lt;/code&gt;) or time period (&lt;code&gt;fp&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;This period window requires an array of the last events. This array size is unknown. (The specific time units are not important, but the following examples use minutes.)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;               past                                   now
    Timeline:  1----2----3----4----5----6----7----8----9---&amp;gt; (min)
    Events:    e1      e2 e3         e4     e5 e6     e7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This timeline measures event timestamps. To calculate the flood ratio, you count events newer than the current time window of size &lt;code&gt;fp&lt;/code&gt;. And check against a ratio of four events in three minutes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Time now:      9
Time window:   from 9-3 = 6 to now(9), so window is 6-9
Oldest event:  e5 (not before 6)
Event count:   3 (in 6-9 period)
Flood ratio:   3/3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This ratio of &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; is below the flood threshold of &lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;, so at this moment there is no flood. Perform this check at the last event to check. In this example, this event is &lt;code&gt;e7&lt;/code&gt;. After each check, you can safely remove all events older than the time window to reduce memory consumption.&lt;/p&gt;

&lt;p&gt;The other solution requires a fixed array of events with size &lt;code&gt;fc&lt;/code&gt;. With our 4ev/3min example, then:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;               past                  now
    Timeline:  &amp;lt;--5----6----7----8----9---&amp;gt; (min)
    Events:      e4        e5 e6     e7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The event array (window) is size 4. To check for a flood at &lt;code&gt;e7&lt;/code&gt;, we use this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Window size &amp;quot;fc&amp;quot;: 4
First event time: e4 -&amp;gt; 5
Last  event time: e7 -&amp;gt; 9
Time period &amp;quot;tp&amp;quot;: 9-5 = 4
Flood ratio is:   4/4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ratio of &lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt; is also below the threshold of &lt;sup&gt;4&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;, so it&amp;rsquo;s OK to accept event &lt;code&gt;e7&lt;/code&gt;. When you must check a new event, add it to the end of the event array (window) and remove the oldest one. If the new event would cause a flood, remember to reverse these operations.&lt;/p&gt;

&lt;p&gt;If the flood check fails, you can find a point in the future when this check will be OK. This makes it possible to return some feedback information to the user indicating how much time to wait before the system will accept the next event:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_11_11_floodcontrol/flood2-eq3.gif&#34; alt=&#34;time until next event equation&#34; width=&#34;126&#34; height=&#34;50&#34; /&gt;
&lt;em&gt;Figure 3. Time until next event equation.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ec  event count (requests received, here equal to fc)
fc  fixed event count (max)
fp  fixed time period for fc
now the future time point we search for
ot  oldest event time point in the array (event timestamp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_11_11_floodcontrol/flood2-eq4.gif&#34; alt=&#34;simplified time until next event equation&#34; width=&#34;186&#34; height=&#34;50&#34; /&gt;
&lt;em&gt;Figure 4. Simplified time until next event equation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_11_11_floodcontrol/flood2-eq5.gif&#34; alt=&#34;time to wait equation&#34; width=&#34;192&#34; height=&#34;30&#34; /&gt;
&lt;em&gt;Figure 5. The time-to-wait equation.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;time the actual current time (time of the new event)
wait time period to wait before next allowed event
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;wait&lt;/code&gt; is positive, then this event should be either rejected or postponed. When &lt;code&gt;wait&lt;/code&gt; is 0 or negative, it&amp;rsquo;s OK to process the event immediately.&lt;/p&gt;

&lt;h3 id=&#34;span-id-the-code-the-code-span&#34;&gt;&lt;span id=&#34;the_code&#34;&gt;The Code&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;In the following implementation I&amp;rsquo;ll use a slightly modified version of the sliding window of events. To avoid removing the last event and eventually replacing it after a failed check, I decided to check the current flood ratio with the existing events array and with the time of the new one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;               past                  now
    Timeline:  &amp;lt;--5----6----7----8----9---&amp;gt; (min)
    Events:      e3   e4   e5 e6    (e7)

    Window size fc: 4 (without e7)
    First event time: e4 -&amp;gt; 5
    Last event time: e7 -&amp;gt; 9
    Time period tp: 9-5 = 4
    Flood ratio is:   4/4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This seems a bit strange at first, but it works exactly as needed. The check is performed as if &lt;code&gt;e6&lt;/code&gt; is timed as &lt;code&gt;e7&lt;/code&gt;, which is the worst case (the biggest time period for the fixed event window size). If the check passes, than after removing &lt;code&gt;e3&lt;/code&gt;, the flood ratio will be always below the threshold!&lt;/p&gt;

&lt;p&gt;Following this description I wrote a function to call for each request or event that needs flood control. It receives a fixed, maximum count of requests (the events window size) and a fixed time period. It returns how much time must elapse until the next allowed event, or 0 if it&amp;rsquo;s OK to process the event immediately.&lt;/p&gt;

&lt;p&gt;This function should be generic, so it needs some kind of event names. To achieve this there is a third argument &amp;ndash; the specific event name for each flood check.&lt;/p&gt;

&lt;p&gt;Here is the actual code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# this package hash holds flood arrays for each event name
# hash with flood keys, this is the internal flood check data storage
our %FLOOD;

sub flood_check
{
  my $fc = shift; # max flood events count
  my $fp = shift; # max flood time period for $fc events
  my $en = shift; # event name (key) which identifies flood check data

  $FLOOD{ $en } ||= [];   # make empty flood array for this event name
  my $ar = $FLOOD{ $en }; # get array ref for event&#39;s flood array
  my $ec = @$ar;          # events count in the flood array

  if( $ec &amp;gt;= $fc ) 
    {
    # flood array has enough events to do real flood check
    my $ot = $$ar[0];      # oldest event timestamp in the flood array
    my $tp = time() - $ot; # time period between current and oldest event

    # now calculate time in seconds until next allowed event
    my $wait = int( ( $ot + ( $ec * $fp / $fc ) ) - time() );
    if( $wait &amp;gt; 0 )
      {
      # positive number of seconds means flood in progress
      # event should be rejected or postponed
      return $wait;
      }
    # negative or 0 seconds means that event should be accepted
    # oldest event is removed from the flood array
    shift @$ar;
    }
  # flood array is not full or oldest event is already removed
  # so current event has to be added
  push  @$ar, time();
  # event is ok
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;ve put this on the CPAN as &lt;a href=&#34;https://metacpan.org/pod/Algorithm::FloodControl&#34;&gt;Algorithm::FloodControl&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To test it, I wrote a simple program that accepts text, line by line, from standard input and prints each accepted line or the amount of time before the program will accept the next line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl
use strict;
use Algorithm::FloodControl;

while(&amp;lt;&amp;gt;)
  {
  # time is used to illustrate the results
  my $tm = scalar localtime;

  # exit on `quit&#39; or `exit&#39; strings
  exit if /exit|quit/i;

  # FLOOD CHECK: allow no more than 2 same lines in 10 seconds
  # here I use the actual data for flood event name!
  my $lw = flood_check( 2, 10, $_ );

  if( $lw ) # local wait time
    {
    chomp;
    print &amp;quot;WARNING: next event allowed in $lw seconds (LOCAL CHECK for &#39;$_&#39;)\n&amp;quot;;
    next;
    }
  print &amp;quot;$tm: LOCAL  OK: $_&amp;quot;;

  # FLOOD CHECK: allow no more than 5 lines in 60 seconds
  my $gw = flood_check( 5, 60, &#39;GLOBAL&#39; );

  if( $gw ) # global wait time
    {
    print &amp;quot;WARNING: next event allowed in $gw seconds (GLOBAL CHECK)\n&amp;quot;;
    next;
    }
  print &amp;quot;$tm: GLOBAL OK: $_&amp;quot;;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I named this &lt;code&gt;floodtest.pl&lt;/code&gt;. The of the test were: (&amp;rdquo;&amp;gt;&amp;rdquo; marks my input lines)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cade@aenea:~$ ./floodtest.pl 
&amp;gt; hello
Wed Feb 17 08:25:35 2004: LOCAL  OK: hello
Wed Feb 17 08:25:35 2004: GLOBAL OK: hello
&amp;gt; hello
Wed Feb 17 08:25:38 2004: LOCAL  OK: hello
Wed Feb 17 08:25:38 2004: GLOBAL OK: hello
&amp;gt; hello
WARNING: next event allowed in 5 seconds (LOCAL CHECK for &#39;hello&#39;)
&amp;gt; bye
Wed Feb 17 08:25:43 2004: LOCAL  OK: bye
Wed Feb 17 08:25:43 2004: GLOBAL OK: bye
&amp;gt; hello
Wed Feb 17 08:25:45 2004: LOCAL  OK: hello
Wed Feb 17 08:25:45 2004: GLOBAL OK: hello
&amp;gt; see you
Wed Feb 17 08:25:48 2004: LOCAL  OK: see you
Wed Feb 17 08:25:48 2004: GLOBAL OK: see you
&amp;gt; next time
Wed Feb 17 08:25:52 2004: LOCAL  OK: next time
WARNING: next event allowed in 43 seconds (GLOBAL CHECK)
&amp;gt; one more try?
Wed Feb 17 08:26:09 2004: LOCAL  OK: one more try?
WARNING: next event allowed in 26 seconds (GLOBAL CHECK)
&amp;gt; free again
Wed Feb 17 08:26:31 2004: LOCAL  OK: free again
WARNING: next event allowed in 4 seconds (GLOBAL CHECK)
&amp;gt; free again
Wed Feb 17 08:26:42 2004: LOCAL  OK: free again
Wed Feb 17 08:26:42 2004: GLOBAL OK: free again
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see that I could not enter &amp;ldquo;hello&amp;rdquo; 3 times during the first 10 seconds but still I managed to enter one more &amp;ldquo;hello&amp;rdquo; a bit later (the 10-second flood had ended for the &amp;ldquo;hello&amp;rdquo; line) and 2 other lines before the global flood check triggered (5 lines for 1 minute). After 60 seconds, &lt;em&gt;floodtest.pl&lt;/em&gt; finally accepted my sixth line, &amp;ldquo;free again.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;The next sections show how to use flood control in several applications. These examples are not exhaustive but are very common, so they will work as templates for other cases.&lt;/p&gt;

&lt;h3 id=&#34;span-id-my-scores-please-my-scores-please-span&#34;&gt;&lt;span id=&#34;my_scores_please&#34;&gt;My Scores Please?&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;Imagine an IRC bot (robot) which can report scores from the local game servers. Generally this bot receives requests from someone inside IRC channel (a chat room, for those of you who haven�t used IRC) and reports current scores back to the channel. If this eventually becomes very popular, people will start requesting scores more frequently than it is useful just for fun, so there&amp;rsquo;s a clear need for flood control.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d prefer to allow any user to request scores no more than twice per minute, but at the same time I want to allow 10 requests total every two minutes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub scores_request
{
    my $irc     = shift; # the IRC connection which I communicate over
                         # this is a Net::IRC::Connection object
    my $channel = shift; # the channel where &amp;quot;scores&amp;quot; are requested
    my $user    = shift; # the user who requested scores

    # next line means: do flood check for $user and if it is ok, then
    #                  check for global flood. this is usual Perl idiom.
    my $wait = flood_check( 2, 60, $user ) || flood_check( 10, 120, &#39;*&#39; );
    if( $wait ) # can be 0 or positive number so this check is simple
      {
      # oops flood detected, report this personally to the user
      $irc-&amp;gt;notice( $user, &amp;quot;please wait $wait seconds&amp;quot; );
      }
    else
      {
      # it is ok, there is no flood, print scores back to the channel
      $irc-&amp;gt;privmsg( $channel, get_scores() );
      }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code uses the &lt;a href=&#34;https://metacpan.org/pod/Net::IRC&#34;&gt;Net::IRC&lt;/a&gt; module, so if you want to know the details of the &lt;code&gt;notice()&lt;/code&gt; and &lt;code&gt;privmsg()&lt;/code&gt; functions, check the module documentation.&lt;/p&gt;

&lt;p&gt;This is good example of combining events, but it works correctly only if the second flood ratio (in this case &lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;120&lt;/sub&gt;) is greater than first one (&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;60&lt;/sub&gt;). Otherwise you should extend the &lt;code&gt;flood_check()&lt;/code&gt; function with an array of events to check in one loop, so if any of them fails the internal storage will update. Perhaps &lt;code&gt;Algorithm::FloodControl&lt;/code&gt; will have such a feature in the future.&lt;/p&gt;

&lt;p&gt;Another common case is to limit the execution of resource-consuming web scripts (CGI).&lt;/p&gt;

&lt;h3 id=&#34;span-id-don-t-flood-the-page-don-t-flood-the-page-span&#34;&gt;&lt;span id=&#34;_don_t__flood_the_page_&#34;&gt;(Don&amp;rsquo;t) Flood the Page!&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;If you want to limit CGI-script execution you will hit a problem: you must save and restore the flood-control internal data between script invocations. For this reason the &lt;code&gt;Algorithm::FloodControl&lt;/code&gt; module exports another function called &lt;code&gt;flood_storage&lt;/code&gt;, which can get or set the internal data.&lt;/p&gt;

&lt;p&gt;In this example I&amp;rsquo;ll use two other modules, &lt;a href=&#34;https://metacpan.org/pod/Storable&#34;&gt;Storable&lt;/a&gt; and &lt;a href=&#34;https://metacpan.org/pod/LockFile::Simple&#34;&gt;LockFile::Simple&lt;/a&gt;. I use the first to save and restore the flood-control data to and from disk files and the second to lock this file to avoid corruptions if two or more instances of the script run at the same time:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl
use strict;
use Storable qw( store retrieve );
use LockFile::Simple qw( lock unlock );
use Algorithm::FloodControl;

# this is the file that should keep the flood data though /tmp is not
# the perfect place for it
my $flood_file = &amp;quot;/tmp/flood-cgi.dat&amp;quot;;

# this is required so the web browser will know what is expected
print &amp;quot;Content-type: text/plain\n\n&amp;quot;;

# I wanted to limit the script executions per remote IP so I have to
# read it from the web server environment
my $remote_ip = $ENV{&#39;REMOTE_ADDR&#39;};

# first of all--lock the flood file
lock( $flood_file );

# now read the flood data if flood file exists
my $FLOOD = retrieve( $flood_file ) if -r $flood_file;

# load flood data into the internal storage
flood_storage( $FLOOD ) if $FLOOD;

# do the actual flood check: max 5 times per minute for each IP
# this is the place where more checks can be done
my $wait = flood_check( 5, 60, &amp;quot;TEST_CGI:$remote_ip&amp;quot; );

# save hte internal data back to the disk
store( flood_storage(), $flood_file );

# and finally unlock the file
unlock( $flood_file );

if( $wait )
  {
  # report flood situation
  print &amp;quot;You have to wait $wait seconds before requesting this page again.\n&amp;quot;;
  exit;
  }

# there is no flood, continue with the real work here
print &amp;quot;Hello, this is main script here, local time is:\n&amp;quot;;
print scalar localtime;
print &amp;quot;\n...\n&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are various issues to consider, such as the save/restore method, time required, and locking, but in any case the scheme will be similar.&lt;/p&gt;

&lt;h3 id=&#34;span-id-beep-beep-beep-beep-beep-beep-span&#34;&gt;&lt;span id=&#34;beep__beep__beep___&#34;&gt;Beep, Beep, Beep &amp;hellip;&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;In this last example I&amp;rsquo;ll describe a small program, a variation of which I use for (email) SMS notifications. I wanted to avoid scanning large mail directories so I made my email filter copy incoming messages into a separate folder. The program scans this copy folder every 10 minutes for new messages. If there are any, it sends a notification for each one to my mobile phone and removes the copy of the message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl
use strict;
use Algorithm::FloodControl;

our $MAIL_ROOT = &#39;/home/cade/mail&#39;;
our @SCAN = ( 
              { # this is my personal mail, I&#39;d like to be notified often
                FOLDER  =&amp;gt; &#39;Personal2&#39;, # directory (mail folder) to scan
                FC      =&amp;gt; 20,          # fixed event count
                FP      =&amp;gt; 60*60,       # fixed time period, 1 hour
              }, 
              { # this is a mailing list, I don&#39;t need frequent notifications
                FOLDER  =&amp;gt; &#39;AList2&#39;,    # directory (mail folder) to scan
                FC      =&amp;gt; 3,           # fixed event count
                FP      =&amp;gt; 20*60,       # fixed time period, 20 minutes
              }
            );
while(4)
  {
  process_folder( $_ ) for @SCAN;
  sleep(10*60); # sleep 10 minutes
  }

sub process_folder
{
  my $hr     = shift; # this is hash reference
  my $fc     = $hr-&amp;gt;{ &#39;FC&#39; };
  my $fp     = $hr-&amp;gt;{ &#39;FP&#39; };
  my $folder = $hr-&amp;gt;{ &#39;FOLDER&#39; };

  my @msg = glob &amp;quot;$MAIL_ROOT/$folder/*&amp;quot;;
  return unless @msg; # no messages found
  for( @msg )
    {
    # there are new messages, so flood check is required
    my  $wait = flood_check( $fc, $fp, $folder );
    if( $wait )
      {
      # skip this pass if non-zero wait time is received for this folder
      print &amp;quot;FLOOD! $wait seconds required.\n&amp;quot;;
      return;
      }
    send_sms( $folder, $_ );
    }
}

sub send_sms
{
  my $folder = shift;
  my $file   = shift;
  # implementation of this function is beyond the scope of this example
  # so I&#39;ll point just that it extracts subject line from the message file
  # and sends (over local sms gateway) text including folder name, time 
  # and subject
  unlink( $file );
  print &amp;quot;SMS: $folder, $file\n&amp;quot;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, this code &amp;ndash; while implementing a totally different task &amp;ndash; has exactly the same flood check as in the previous two examples.&lt;/p&gt;

&lt;h3 id=&#34;span-id-conclusion-conclusion-span&#34;&gt;&lt;span id=&#34;conclusion&#34;&gt;Conclusion&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;I said in the beginning that flood control has a vast field of applications. There are many cases where it is appropriate or even necessary. There is no excuse to avoid such checks; implementing it is not hard at all.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Bloom Filters</title>
      <link>http://localhost:1313/pub/2004/04/08/bloom_filters.html/</link>
      <pubDate>Thu, 08 Apr 2004 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2004/04/08/bloom_filters.html/</guid>
      <description>

&lt;p&gt;Anyone who has used Perl for any length of time is familiar with the lookup hash, a handy idiom for doing existence tests:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;foreach my $e ( @things ) { $lookup{$e}++ }

sub check {
    my ( $key ) = @_;
    print &amp;quot;Found $key!&amp;quot; if exists( $lookup{ $key } );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As useful as the lookup hash is, it can become unwieldy for very large lists or in cases where the keys themselves are large. When a lookup hash grows too big, the usual recourse is to move it to a database or flat file, perhaps keeping a local cache of the most frequently used keys to improve performance.&lt;/p&gt;

&lt;p&gt;Many people don&amp;rsquo;t realize that there is an elegant alternative to the lookup hash, in the form of a venerable algorithm called a &lt;em&gt;Bloom filter&lt;/em&gt;. Bloom filters allow you to perform membership tests in just a fraction of the memory you&amp;rsquo;d need to store a full list of keys, so you can avoid the performance hit of having to use a disk or database to do your lookups. As you might suspect, the savings in space comes at a price: you run an adjustable risk of false positives, and you can&amp;rsquo;t remove a key from a filter once you&amp;rsquo;ve added it in. But in the many cases where those constraints are acceptable, a Bloom filter can make a useful tool.&lt;/p&gt;

&lt;p&gt;For example, imagine you run a high-traffic online music store along the lines of iTunes, and you want to minimize the stress on your database by only fetching song information when you know the song exists in your collection. You can build a Bloom filter at startup, and then use it as a quick existence check before trying to perform an expensive fetching operation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use Bloom::Filter;

my $filter = Bloom::Filter-&amp;gt;new( error_rate =&amp;gt; 0.01, capacity =&amp;gt; $SONG_COUNT );
open my $fh, &amp;quot;enormous_list_of_titles.txt&amp;quot; or die &amp;quot;Failed to open: $!&amp;quot;;

while (&amp;lt;$fh&amp;gt;) {
    chomp;
    $filter-&amp;gt;add( $_ );
}

sub lookup_song {
    my ( $title ) = @_;
    return unless $filter-&amp;gt;check( $title );
    return expensive_db_query( $title ) or undef;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, there&amp;rsquo;s a 1% chance that the test will give a false positive, which means the program will perform the expensive fetch operation and eventually return a null result. Still, you&amp;rsquo;ve managed to avoid the expensive query 99% of the time, using only a fraction of the memory you would have needed for a lookup hash. As we&amp;rsquo;ll see further on, a filter with a 1% error rate requires just under 2 bytes of storage per key. That&amp;rsquo;s far less memory than you would need for a lookup hash.&lt;/p&gt;

&lt;p&gt;Bloom filters are named after Burton Bloom, who first described them in a 1970 paper entitled &lt;a href=&#34;http://portal.acm.org/citation.cfm?id=362692&amp;amp;dl=ACM&amp;amp;coll=portal&#34;&gt;Space/time trade-offs in hash coding with allowable errors&lt;/a&gt;. In those days of limited memory, Bloom filters were prized primarily for their compactness; in fact, one of their earliest applications was in spell checkers. However, there are less obvious features of the algorithm that make it especially well-suited to applications in social software.&lt;/p&gt;

&lt;p&gt;Because Bloom filters use one-way hashing to store their data, it is impossible to reconstruct the list of keys in a filter without doing an exhaustive search of the keyspace. Even that is unlikely to be of much help, since the false positives from an exhaustive search will swamp the list of real keys. Bloom filters therefore make it possible to share information about what you have without broadcasting a complete list of it to the world. For that reason, they may be especially valuable in peer-to-peer applications, where both size and privacy are important constraints.&lt;/p&gt;

&lt;h3 id=&#34;span-id-how-bloom-filters-work-how-bloom-filters-work-span&#34;&gt;&lt;span id=&#34;how_bloom_filters_work&#34;&gt;How Bloom Filters Work&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;A Bloom filter consists of two components: a set of &lt;code&gt;k&lt;/code&gt; hash functions and a bit vector of a given length. We choose the length of the bit vector and the number of hash functions depending on how many keys we want to add to the set and how high an error rate we are willing to put up with &amp;ndash; more on that a little bit further on.&lt;/p&gt;

&lt;p&gt;All of the hash functions in a Bloom filter are configured so that their range matches the length of the bit vector. For example, if a vector is 200 bits long, the hash functions return a value between 1 and 200. It&amp;rsquo;s important to use high-quality hash functions in the filter to guarantee that output is equally distributed over all possible values &amp;ndash; &amp;ldquo;hot spots&amp;rdquo; in a hash function would increase our false-positive rate.&lt;/p&gt;

&lt;p&gt;To enter a key into a Bloom filter, we run it through each one of the &lt;code&gt;k&lt;/code&gt; hash functions and treat the result as an offset into the bit vector, turning on whatever bit we find at that position. If the bit is already set, we leave it on. There&amp;rsquo;s no mechanism for turning bits off in a Bloom filter.&lt;/p&gt;

&lt;p&gt;As an example, let&amp;rsquo;s take a look at a Bloom filter with three hash functions and a bit vector of length 14. We&amp;rsquo;ll use spaces and asterisks to represent the bit vector, to make it easier to follow along. As you might expect, an empty Bloom filter starts out with all the bits turned off, as seen in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_1.gif&#34; alt=&#34;an empty Bloom filter&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 1. An empty Bloom filter.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s now add the string &lt;code&gt;apples&lt;/code&gt; into our filter. To do so, we hash &lt;code&gt;apples&lt;/code&gt; through each of our three hash functions and collect the output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hash1(&amp;quot;apples&amp;quot;) = 3
hash2(&amp;quot;apples&amp;quot;) = 12
hash3(&amp;quot;apples&amp;quot;) = 11
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we turn on the bits at the corresponding positions in the vector &amp;ndash; in this case bits 3, 11, and 12, as shown in Figure 2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_2.gif&#34; alt=&#34;a Bloom filter with three bits enabled&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 2. A Bloom filter with three bits enabled.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To add another key, such as &lt;code&gt;plums&lt;/code&gt;, we repeat the hashing procedure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hash1(&amp;quot;plums&amp;quot;) = 11
hash2(&amp;quot;plums&amp;quot;) = 1
hash3(&amp;quot;plums&amp;quot;) = 8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And again turn on the appropriate bits in the vector, as shown with highlights in Figure 3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_3.gif&#34; alt=&#34;the Bloom filter after adding a second key&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 3. The Bloom filter after adding a second key.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Notice that the bit at position 11 was already turned on &amp;ndash; we had set it when we added &lt;code&gt;apples&lt;/code&gt; in the previous step. Bit 11 now does double duty, storing information for both &lt;code&gt;apples&lt;/code&gt; and &lt;code&gt;plums&lt;/code&gt;. As we add more keys, it may store information for some of them as well. This overlap is what makes Bloom filters so compact &amp;ndash; any one bit may be encoding multiple keys simultaneously. This overlap also means that you can never take a key out of a filter, because you have no guarantee that the bits you turn off don&amp;rsquo;t carry information for other keys. If we tried to remove &lt;code&gt;apples&lt;/code&gt; from the filter by reversing the procedure we used to add it in, we would inadvertently turn off one of the bits that encodes &lt;code&gt;plums&lt;/code&gt;. The only way to strip a key out of a Bloom filter is to rebuild the filter from scratch, leaving out the offending key.&lt;/p&gt;

&lt;p&gt;Checking to see whether a key already exists in a filter is exactly analogous to adding a new key. We run the key through our set of hash functions, and then check to see whether the bits at those offsets are all turned on. If any of the bits is off, we know for certain the key is not in the filter. If all of the bits are on, we know the key is probably there.&lt;/p&gt;

&lt;p&gt;I say &amp;ldquo;probably&amp;rdquo; because there&amp;rsquo;s a certain chance our key might be a false positive. For example, let&amp;rsquo;s see what happens when we test our filter for the string &lt;code&gt;mango&lt;/code&gt;. We run &lt;code&gt;mango&lt;/code&gt; through the set of hash functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hash1(&amp;quot;mango&amp;quot;) = 8
hash2(&amp;quot;mango&amp;quot;) = 3
hash3(&amp;quot;mango&amp;quot;) = 12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then examine the bits at those offsets, as shown in Figure 4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/_pub_2004_04_08_bloom_filters/bloom_4.gif&#34; alt=&#34;a false positive in the Bloom filter&#34; width=&#34;284&#34; height=&#34;24&#34; /&gt;
&lt;em&gt;Figure 4. A false positive in the Bloom filter.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;All of the bits at positions 3, 8, and 12 are on, so our filter will report that &lt;code&gt;mango&lt;/code&gt; is a valid key.&lt;/p&gt;

&lt;p&gt;Of course, &lt;code&gt;mango&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; a valid key &amp;ndash; the filter we built contains only &lt;code&gt;apples&lt;/code&gt; and &lt;code&gt;plums&lt;/code&gt;. The fact that the offsets for &lt;code&gt;mango&lt;/code&gt; point to enabled bits is just coincidence. We have found a false positive &amp;ndash; a key that seems to be in the filter, but isn&amp;rsquo;t really there.&lt;/p&gt;

&lt;p&gt;As you might expect, the false-positive rate depends on the bit vector length and the number of keys stored in the filter. The roomier the bit vector, the smaller the probability that all &lt;code&gt;k&lt;/code&gt; bits we check will be on, unless the key actually exists in the filter. The relationship between the number of hash functions and the false-positive rate is more subtle. If you use too few hash functions, there won&amp;rsquo;t be enough discrimination between keys; but if you use too many, the filter will be very dense, increasing the probability of collisions. You can calculate the false-positive rate for any filter using the formula:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;c = ( 1 - e(-kn/m) )k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;c&lt;/code&gt; is the false positive rate, &lt;code&gt;k&lt;/code&gt; is the number of hash functions, &lt;code&gt;n&lt;/code&gt; is the number of keys in the filter, and &lt;code&gt;m&lt;/code&gt; is the length of the filter in bits.&lt;/p&gt;

&lt;p&gt;When using Bloom filters, we very frequently have a desired false-positive rate in mind and we are also likely to have a rough idea of how many keys we want to add to the filter. We need some way of finding out how large a bit vector is to make sure the false-positive rate never exceeds our limit. The following equation will give us vector length from the error rate and number of keys:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;m = -kn / ( ln( 1 - c ^ 1/k ) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll notice another free variable here: &lt;code&gt;k&lt;/code&gt;, the number of hash functions. It&amp;rsquo;s possible to use calculus to find a minimum for &lt;code&gt;k&lt;/code&gt;, but there&amp;rsquo;s a lazier way to do it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub calculate_shortest_filter_length {
    my ( $num_keys, $error_rate ) = @_;
    my $lowest_m;
    my $best_k = 1;

    foreach my $k ( 1..100 ) {
        my $m = (-1 * $k * $num_keys) /
            ( log( 1 - ($error_rate ** (1/$k))));

        if ( !defined $lowest_m or ($m &amp;lt; $lowest_m) ) {
            $lowest_m = $m;
            $best_k   = $k;
        }
    }
    return ( $lowest_m, $best_k );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To give you a sense of how error rate and number of keys affect the storage size of Bloom filters, Table 1 lists some sample vector sizes for a variety of capacity/error rate combinations.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Error Rate&lt;/th&gt;
&lt;th&gt;Keys&lt;/th&gt;
&lt;th&gt;Required Size&lt;/th&gt;
&lt;th&gt;Bytes/Key&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1%&lt;/td&gt;
&lt;td&gt;1K&lt;/td&gt;
&lt;td&gt;1.87 K&lt;/td&gt;
&lt;td&gt;1.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.1%&lt;/td&gt;
&lt;td&gt;1K&lt;/td&gt;
&lt;td&gt;2.80 K&lt;/td&gt;
&lt;td&gt;2.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;1K&lt;/td&gt;
&lt;td&gt;3.74 K&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;10K&lt;/td&gt;
&lt;td&gt;37.4 K&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;100K&lt;/td&gt;
&lt;td&gt;374 K&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;1M&lt;/td&gt;
&lt;td&gt;3.74 M&lt;/td&gt;
&lt;td&gt;3.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.001%&lt;/td&gt;
&lt;td&gt;1M&lt;/td&gt;
&lt;td&gt;4.68 M&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.0001%&lt;/td&gt;
&lt;td&gt;1M&lt;/td&gt;
&lt;td&gt;5.61 M&lt;/td&gt;
&lt;td&gt;5.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can find further lookup tables for various combinations of error rate, filter size, and number of hash functions at &lt;a href=&#34;http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html#tab:bf-config-1&#34;&gt;Bloom Filters &amp;ndash; the math&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;span-id-building-a-bloom-filter-in-perl-building-a-bloom-filter-in-perl-span&#34;&gt;&lt;span id=&#34;building_a_bloom_filter_in_perl&#34;&gt;Building a Bloom Filter in Perl&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;To make a working Bloom filter, we need a good set of hash functions These are easy to come by &amp;ndash; there are several excellent hashing algorithms available on CPAN. For our purposes, a good choice is &lt;code&gt;Digest::SHA1&lt;/code&gt;, a cryptographically strong hash with a fast C implementation. We can use the module to create as many hash functions as we like by salting the input with a list of distinct values. Here&amp;rsquo;s a subroutine that builds a list of unique hash functions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use Digest::SHA1 qw/sha1/;

sub make_hashing_functions {
    my ( $count ) = @_;
    my @functions;

    for my $salt (1..$count ) {
        push @functions, sub { sha1( $salt, $_[0] ) };
    }

    return @functions;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be able to use these hash functions, we have to find a way to control their range. &lt;code&gt;Digest::SHA1&lt;/code&gt; returns an embarrassingly lavish 160 bits of hashed output, useful only in the unlikely case that our vector is 2&lt;sup&gt;160&lt;/sup&gt; bits long. We&amp;rsquo;ll use a combination of bit chopping and division to scale the output down to a more usable size.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a subroutine that takes a key, runs it through a list of hash functions, and returns a bitmask of length &lt;code&gt;$FILTER_LENGTH&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub make_bitmask {
    my ( $key ) = @_;
    my $mask    = pack( &amp;quot;b*&amp;quot;, &#39;0&#39; x $FILTER_LENGTH);

    foreach my $hash_function ( @functions ){

        my $hash       = $hash_function-&amp;gt;($key);
        my $chopped    = unpack(&amp;quot;N&amp;quot;, $hash );
        my $bit_offset = $result % $FILTER_LENGTH;

        vec( $mask, $bit_offset, 1 ) = 1;
    }
    return $mask;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a dense stretch of code, so let&amp;rsquo;s look at it line by line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $mask = pack( &amp;quot;b*&amp;quot;, &#39;0&#39; x $FILTER_LENGTH);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We start by using Perl&amp;rsquo;s &lt;code&gt;pack&lt;/code&gt; operator to create a zeroed bit vector that is &lt;code&gt;$FILTER_LENGTH&lt;/code&gt; bits long. &lt;code&gt;pack&lt;/code&gt; takes two arguments, a template and a value. The &lt;code&gt;b&lt;/code&gt; in our template tells &lt;code&gt;pack&lt;/code&gt; that we want it to interpret the value as bits, and the &lt;code&gt;*&lt;/code&gt; indicates &amp;ldquo;repeat as often as necessary,&amp;rdquo; just like in a regular expression. Perl will actually pad our bit vector to make its length a multiple of eight, but we&amp;rsquo;ll ignore those superfluous bits.&lt;/p&gt;

&lt;p&gt;With a blank bit vector in hand, we&amp;rsquo;re ready to start running our key through the hash functions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $hash = $hash_function-&amp;gt;($key);
my $chopped = unpack(&amp;quot;N&amp;quot;, $hash );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;re keeping the first 32 bits of the output and discarding the rest. This prevents us from having to require &lt;code&gt;BigInt&lt;/code&gt; support further along. The second line does the actual bit chopping. The &lt;code&gt;N&lt;/code&gt; in the template tells &lt;code&gt;unpack&lt;/code&gt; to extract a 32-bit integer in network byte order. Because we don&amp;rsquo;t provide any quantifier in the template, &lt;code&gt;unpack&lt;/code&gt; will extract just one integer and then stop.&lt;/p&gt;

&lt;p&gt;If you are extra, super paranoid about bit chopping, you could split the hash into five 32-bit pieces and XOR them together, preserving all the information in the original hash:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $chopped = pack( &amp;quot;N&amp;quot;, 0 );
my @pieces  =  map { pack( &amp;quot;N&amp;quot;, $_ ) } unpack(&amp;quot;N*&amp;quot;, $hash );
$chopped    = $_ ^ $chopped foreach @pieces;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But this is probably overkill.&lt;/p&gt;

&lt;p&gt;Now that we have a list of 32-bit integer outputs from our hash functions, all we have to do is scale them down with the modulo operator so they fall in the range (1..&lt;code&gt;$FILTER_LENGTH&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $bit_offset = $chopped % $FILTER_LENGTH;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;ve turned our key into a list of bit offsets, which is exactly what we were after.&lt;/p&gt;

&lt;p&gt;The only thing left to do is to set the bits using &lt;code&gt;vec&lt;/code&gt;, which takes three arguments: the vector itself, a starting position, and the number of bits to set. We can assign a value to &lt;code&gt;vec&lt;/code&gt; like we would to a variable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vec( $mask, $bit_offset, 1 ) = 1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After we&amp;rsquo;ve set all the bits, we wind up with a bitmask that is the same length as our Bloom filter. We can use this mask to add the key into the filter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub add {
    my ( $key, $filter ) = @_;

    my $mask = make_bitmask( $key );
    $filter  = $filter | $mask;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or we can use it to check whether the key is already present:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub check {
    my ( $key, $filter ) = @_;
    my $mask  = make_bitmask( $key );
    my $found = ( ( $filter &amp;amp; $mask ) eq $mask );
    return $found;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that those are the bitwise OR (&lt;code&gt;|&lt;/code&gt;) and AND (&lt;code&gt;&amp;amp;&lt;/code&gt;) operators, not the more commonly used logical OR (&lt;code&gt;||&lt;/code&gt;) and AND ( &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; ) operators. Getting the two mixed up can lead to hours of interesting debugging. The first example ORs the mask against the bit vector, turning on any bits that aren&amp;rsquo;t already set. The second example compares the mask to the corresponding positions in the filter &amp;ndash; if all of the on bits in the mask are also on in the filter, we know we&amp;rsquo;ve found a match.&lt;/p&gt;

&lt;p&gt;Once you get over the intimidation factor of using &lt;code&gt;vec&lt;/code&gt;, &lt;code&gt;pack&lt;/code&gt;, and the bitwise operators, Bloom filters are actually quite straightforward. &lt;a href=&#34;http://localhost:1313/media/_pub_2004_04_08_bloom_filters/Filter.pm&#34;&gt;Listing 1&lt;/a&gt; shows a complete object-oriented implementation called &lt;code&gt;Bloom::Filter&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;span-id-bloom-filters-in-distributed-social-networks-bloom-filters-in-distributed-social-networks-span&#34;&gt;&lt;span id=&#34;bloom_filters_in_distributed_social_networks&#34;&gt;Bloom Filters in Distributed Social Networks&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;One drawback of existing social network schemes is that they require participants to either divulge their list of contacts to a central server (Orkut, Friendster) or publish it to the public Internet (FOAF), in both cases sacrificing a great deal of privacy. By exchanging Bloom filters instead of explicit lists of contacts, users can participate in social networking experiments without having to admit to the world who their friends are. A Bloom filter encoding someone&amp;rsquo;s contact information can be checked to see whether it contains a given name or email address, but it can&amp;rsquo;t be coerced into revealing the full list of keys that were used to build it. It&amp;rsquo;s even possible to turn the false-positive rate, which may not sound like a feature, into a powerful tool.&lt;/p&gt;

&lt;p&gt;Suppose that I am very concerned about people trying to reverse-engineer my social network by running a dictionary attack against my Bloom filter. I can build my filter with a prohibitively high false-positive rate (50%, for example) and then arrange to send multiple copies of my Bloom filter to friends, varying the hash functions I use to build each filter. The more filters my friends collect, the lower the false-positive rate they will see. For example, with five filters the false-positive rate will be (0.5)&lt;sup&gt;5&lt;/sup&gt;, or 3% &amp;ndash; and I can reduce the rate further by sending out more filters.&lt;/p&gt;

&lt;p&gt;If any one of the filters is intercepted, it will register the full 50% false-positive rate. So I am able to hedge my privacy risk across several interactions, and have some control over how accurately other people can see my network. My friends can be sure with a high degree of certainty whether someone is on my contact list, but someone who manages to snag just one or two of my filters will learn almost nothing about me.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a Perl function that checks a key against a set of noisy filters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;use Bloom::Filter;

sub check_noisy_filters {
    my ( $key, @filters ) = @_;
    foreach my $filter ( @filters ) {
        return 0 unless $filter-&amp;gt;check( $key );
    }
    return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you and your friends agree to use the same filter length and set of hash functions, you can also use bitwise comparisons to estimate the degree of overlap between your social networks. The number of shared on bits in two Bloom filters will give a usable measure of the distance between them.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sub shared_on_bits {
    my ( $filter_1, $filter_2 ) = @_;
    return unpack( &amp;quot;%32b*&amp;quot;,  $filter_1 &amp;amp; $filter_2 )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Additionally, you can combine two Bloom filters that have the same length and hash functions with the bitwise OR operator to create a composite filter. For example, if you participate in a small mailing list and want to create a whitelist from the address books of everyone in the group, you can have each participant create a Bloom filter individually and then OR the filters together into a Voltron-like master list. None of the members of the group will know who the other members&amp;rsquo; contacts are, and yet the filter will exhibit the correct behavior.&lt;/p&gt;

&lt;p&gt;There are sure to be other neat Bloom filter tricks with potential applications to social networking and distributed applications. The references below list a few good places to start mining.&lt;/p&gt;

&lt;h3 id=&#34;span-id-references-references-span&#34;&gt;&lt;span id=&#34;references&#34;&gt;References&lt;/span&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2ecs_2ewisc_2eedu_2f_7ecao_2fpape&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html&#34;&gt;Bloom Filters &amp;ndash; the math&lt;/a&gt;&lt;/strong&gt;. A good place to start for an overview of the math behind Bloom filters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2ecap_2dlore_2ecom_2fcode_2fbloom&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.cap-lore.com/code/BloomTheory.html&#34;&gt;Some Motley Bloom Tricks&lt;/a&gt;&lt;/strong&gt;. Handy filter tricks and theory page.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2eeecs_2eharvard_2eedu_2f_7emicha&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.eecs.harvard.edu/~michaelm/NEWWORK/postscripts/BloomFilterSurvey.pdf&#34;&gt;Bloom Filter Survey&lt;/a&gt;&lt;/strong&gt;. A handy survey article on Bloom filter network applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_loaf&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://loaf.cantbedone.org&#34;&gt;LOAF&lt;/a&gt;&lt;/strong&gt;. Our own system for incorporating social networks onto email using Bloom filters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.eecs.harvard.edu/~michaelm/NEWWORK/postscripts/cbf2.pdf&#34;&gt;Compressed Bloom Filters&lt;/a&gt;&lt;/strong&gt;. If you are passing filters around a network, you will want to optimize them for minimum size; this paper gives a good overview of compressed Bloom filters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_bloom16&#34;&gt;&lt;/span&gt;&lt;a href=&#34;https://metacpan.org/pod/Bloom16&#34;&gt;Bloom16&lt;/a&gt;.&lt;/strong&gt; A CPAN module implementing a counting Bloom filter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_bloom&#34;&gt;&lt;/span&gt;&lt;a href=&#34;https://metacpan.org/pod/Text::Bloom&#34;&gt;Text::Bloom&lt;/a&gt;&lt;/strong&gt;. CPAN module for using Bloom filters with text collections.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span id=&#34;item_http_3a_2f_2fwww_2eresearch_2eatt_2ecom_2f_7esmb_2&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.research.att.com/~smb/papers/bloom-encrypt.pdf&#34;&gt;Privacy-Enhanced Searches Using Encryted Bloom Filters&lt;/a&gt;&lt;/strong&gt;. This paper discusses how to use encryption and Bloom filters to set up a query system that prevents the search engine from knowing the query you are running.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.cs.wisc.edu/~cao/papers/summary-cache/node9.html&#34;&gt;Bloom Filters as Summaries&lt;/a&gt;&lt;/strong&gt;. Some performance data on actually using Bloom filters as cache summaries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://www.research.att.com/~smb/papers/draft-bellovin-dnsext-bloomfilt-00.txt&#34;&gt;Using Bloom Filters for Authenticated Yes/No Answers in the DNS&lt;/a&gt;&lt;/strong&gt;. Internet draft for using Bloom filters to implement Secure DNS&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Chromosome at a Time with Perl, Part 2</title>
      <link>http://localhost:1313/pub/2003/10/15/bioinformatics.html/</link>
      <pubDate>Wed, 15 Oct 2003 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2003/10/15/bioinformatics.html/</guid>
      <description>

&lt;p&gt;&lt;em&gt;James D. Tisdall is the author of the recently released&lt;/em&gt; &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/index.html?CMP=IL7015&#34;&gt;Mastering Perl for Bioinformatics&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In my previous article, &lt;a href=&#34;http://localhost:1313/pub/2003/09/10/bioinformatics.html&#34;&gt;A Chromosome at a Time with Perl, Part I&lt;/a&gt;, I showed you some programming &amp;ldquo;tricks&amp;rdquo; that help you avoid the trap of using up all your main memory when coding for very long strings, such as chromosomes and entire genomes.&lt;/p&gt;

&lt;p&gt;The basic approach involved improving your code&amp;rsquo;s running time by limiting the amount of memory space the program uses. The tricks discussed were calling subroutines with references as arguments, and searching for a pattern in a very large file by keeping only a small &amp;ldquo;window&amp;rdquo; of the file in memory at any one time, in a buffer.&lt;/p&gt;

&lt;p&gt;This article will continue that discussion. I&amp;rsquo;ll show you more about how references can greatly speed up a subroutine call by avoiding making copies of very large strings. I&amp;rsquo;ll show you how you can bypass the overhead of subroutine calls entirely. I&amp;rsquo;ll extend the previous example of a buffer window into a large file, making it suited to any situation where you know the minimum and maximum length of a pattern for which you&amp;rsquo;re searching. And I&amp;rsquo;ll show you how to quantify the behavior of your code by measuring its speed and space usage.&lt;/p&gt;

&lt;h3 id=&#34;why-space-puts-a-lower-bound-on-time&#34;&gt;Why Space Puts a Lower Bound on Time&lt;/h3&gt;

&lt;p&gt;In Perl, as in any programming system, the size of the data that the program uses is an absolute lower bound on how fast the program can perform.&lt;/p&gt;

&lt;p&gt;In fact, algorithms are typically classified by how fast they perform on inputs of varying sizes, by giving their speed as a function of the size of the input. So a program that has to do &lt;strong&gt;2&lt;sup&gt;n&lt;/sup&gt;&lt;/strong&gt; computations on an input of size &lt;strong&gt;n&lt;/strong&gt; is a hell of a lot slower than a program that has to do &lt;strong&gt;n&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt; computations on an input of size &lt;strong&gt;n&lt;/strong&gt;. The first is called &lt;em&gt;intractable&lt;/em&gt; and &lt;em&gt;exponential&lt;/em&gt;, or &amp;ldquo;bad&amp;rdquo;; the second is called &lt;em&gt;tractable&lt;/em&gt; and &lt;em&gt;polynomial&lt;/em&gt;, or &amp;ldquo;good.&amp;rdquo; For instance, if &lt;strong&gt;n&lt;/strong&gt;, the size of the input, is 100, then &lt;strong&gt;n&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt; is 10,000, while &lt;strong&gt;2&lt;sup&gt;n&lt;/sup&gt;&lt;/strong&gt; is bigger than the number of atoms in the universe. But who&amp;rsquo;s counting? And is the universe really finite? Oh well &amp;hellip; back to your regularly scheduled program.&lt;/p&gt;

&lt;p&gt;This way of measuring an algorithm is called &lt;em&gt;time complexity&lt;/em&gt;. It&amp;rsquo;s usually written in a shorthand called &lt;em&gt;big Oh&lt;/em&gt; notation. (See the Suggested Reading at the end of this article, if you get that far.)&lt;/p&gt;

&lt;p&gt;In particular, if an algorithm gets an input of size &lt;strong&gt;n&lt;/strong&gt;, and then just to write the answer it must write an output of size &lt;strong&gt;2&lt;sup&gt;n&lt;/sup&gt;&lt;/strong&gt;, then the algorithm is taking &lt;strong&gt;2&lt;sup&gt;n&lt;/sup&gt;&lt;/strong&gt; time, at least. So the space that an algorithm uses is intimately connected to the time it uses. Of course, a program could use just a small, constant amount of space and still use &lt;strong&gt;2&lt;sup&gt;n&lt;/sup&gt;&lt;/strong&gt; time, for instance if it added and subtracted the number one over and over, &lt;strong&gt;2&lt;sup&gt;n&lt;/sup&gt;&lt;/strong&gt; times, for some perverse reason. Still, the amount of space that an algorithm uses establishes a lower bound for how much time the algorithm takes to complete.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s all this got to do with Perl programming in bioinformatics? Quite a lot, if you&amp;rsquo;re writing code that manipulates, say, the 3 gigabases of human DNA.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re new to the field, a base is one of the letters A, C, G, or T that represents one of the four molecules that are the principal building blocks of DNA. Each base is typically represented in a computer language as one ASCII character taking one 8-bit byte, so 3 gigabases equals 3 gigabytes. Of course, you could represent each of the four bases using only 2 bits, so considerable compression is possible; but such space efficiency is not commonly employed. Hmmm &amp;hellip; makes an interesting idea for another article, however! &amp;ldquo;Perl and a Chromosome, Two Bits.&amp;rdquo; Watch this space.&lt;/p&gt;

&lt;p&gt;Just to read in the 3 gigabytes of DNA is going to take you some time. If you&amp;rsquo;re also making copies of the 3 gigabytes in your variables, you&amp;rsquo;re going to need more main memory than most computers have available. So the crafty Perl programmer needs to think of programming solutions that minimize the amount of space used when computing with such large input. If she does, not only will she have a program that fits into her computer&amp;rsquo;s memory (always a wise move); she may also have a program that runs pretty fast, if she does say so herself, with all due humility.&lt;/p&gt;

&lt;h3 id=&#34;subroutines-without-extra-space&#34;&gt;Subroutines Without Extra Space&lt;/h3&gt;

&lt;p&gt;In Part I, I briefly discussed how passing references to subroutines can save you considerable space. I&amp;rsquo;ll just expand a little on that discussion in this section. There are three main ways that references can be used to save space and therefore time in the subroutines of your Perl program.&lt;/p&gt;

&lt;h4 id=&#34;one-collect-data-in-a-subroutine-argument&#34;&gt;One: Collect Data in a Subroutine Argument&lt;/h4&gt;

&lt;p&gt;First, let&amp;rsquo;s say you call a subroutine to get some data. Typically this takes a form such as this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $chromosome1 = get_chromosome( 1 );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Assuming that the data is about 100 megabases long, the Perl programmer can see that the subroutine &amp;ldquo;get_chromosome&amp;rdquo; is collecting 100 megabases of DNA and then &amp;ldquo;returning&amp;rdquo; it, which means that copies of those 100 megabases are being made. And that&amp;rsquo;s a Bad Thing.&lt;/p&gt;

&lt;p&gt;Instead, the wily hacker could pass a reference to the &lt;code&gt;$chromosome1&lt;/code&gt; string into the subroutine, which could be written to &amp;ldquo;fill&amp;rdquo; the string with the 100 megabases without the need for further copying. Then after the subroutine call, say like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;get_chromosome(1, \$chromosome1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the variable &lt;code&gt;$chromosome1&lt;/code&gt; would contain the same data as in the previous version, but it would have gotten there without being copied by means of being &amp;ldquo;returned&amp;rdquo; from a subroutine. And that&amp;rsquo;s a Good Thing. The only gotcha here is that the subroutine call is definitely changing what&amp;rsquo;s in the &lt;code&gt;$chromosome1&lt;/code&gt; variable. No problem as long as you remember that&amp;rsquo;s what&amp;rsquo;s happening.&lt;/p&gt;

&lt;h4 id=&#34;two-pass-the-subroutine-a-reference-to-the-data&#34;&gt;Two: Pass the Subroutine a Reference to the Data&lt;/h4&gt;

&lt;p&gt;Here&amp;rsquo;s a second way to use references as subroutine arguments to good advantage. Let&amp;rsquo;s say you have a chromosome&amp;rsquo;s worth of DNA in a variable&lt;code&gt;$chromosome1&lt;/code&gt;, and you want to pass it to a subroutine that counts how many As, Cs, Gs, and Ts there are in it. (This might be important if you were looking for genes in the chromosome, for instance &amp;ndash; in humans, genes tend to be GC rich.)&lt;/p&gt;

&lt;p&gt;If you write your code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my($a, $c, $g, $t) = countacgt( $chromosome1 );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then the &amp;ldquo;countacgt&amp;rdquo; subroutine is going to make a copy of the data in the argument &lt;code&gt;$chromosome1&lt;/code&gt;. That&amp;rsquo;s a Regrettable Occurence.&lt;/p&gt;

&lt;p&gt;On the other hand, if you pass the subroutine a &lt;em&gt;reference&lt;/em&gt; to the data in &lt;code&gt;$chromosome1&lt;/code&gt;, the data will not be copied, and that&amp;rsquo;s a Fortunate Happenstance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my($a, $c, $g, $t) = countacgt( \$chromosome1 );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, once again you&amp;rsquo;ll have to be aware that the subroutine has the power to change what&amp;rsquo;s in the &lt;code&gt;$chromosome1&lt;/code&gt; variable, and either avoid changing it or take note of the change.&lt;/p&gt;

&lt;p&gt;As another alternative, you could use&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my($a, $c, $g, $t) = countacgt( $chromosome1 );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;but then &lt;em&gt;don&amp;rsquo;t&lt;/em&gt; assign a new variable to the argument within the &lt;code&gt;countacgt&lt;/code&gt; subroutine, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my($chrom) = @_;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Instead, just use &lt;code&gt;$_[0]&lt;/code&gt; to access the chromosome data without copying it. And that&amp;rsquo;s a Perl Idiom. (For readability, you may want to add a comment explaining what kind of data &lt;code&gt;$_[0]&lt;/code&gt; is, since you won&amp;rsquo;t have an informative variable name.)&lt;/p&gt;

&lt;h4 id=&#34;three-return-a-reference-to-the-data&#34;&gt;Three: Return a Reference to the Data&lt;/h4&gt;

&lt;p&gt;Now a third and final way to use references instead of space: if you have a subroutine that collects a large amount of data, you can have it return a reference to the data instead of the data itself; this will also avoid the large string copies, which Ain&amp;rsquo;t Bad:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my $chromosome1ref = get_chromosome( 1 );
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;eliminating-subroutines-altogether&#34;&gt;Eliminating Subroutines Altogether&lt;/h3&gt;

&lt;p&gt;Organizing the code for a computation into a logical set of subroutines can make for clean, easy-to-understand, and elegant programming.&lt;/p&gt;

&lt;p&gt;Unfortunately, it can also make your program perform much slower. Take this small example. (An &lt;em&gt;exon&lt;/em&gt; is a stretch of a chromosome&amp;rsquo;s DNA, transcribed into RNA, that contains part of the code for a gene. In organisms such as humans, most genes are composed of multiple exons separated by non-coding &lt;em&gt;introns&lt;/em&gt;; the exons are &lt;em&gt;spliced&lt;/em&gt; together to get the actual code for the gene):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while ((my $begin, my $end) =  each %exon_endpoints) {
    print get_exon($chromosome, $begin, $end), &amp;quot;\n\n&amp;quot;;
}

sub get_exon {
    my($chromosome, $begin, $end) = @_;

    # The arguments to substr are: string, beginning, length
    return substr($chromosome, $begin - 1, $end - $begin + 1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code takes the information about exon endpoints stored in the hash &lt;code&gt;%exon_endpoints&lt;/code&gt; (key = begin, value = end) to extract the exons from a chromosome and print them. (You may remember from Part I that I translated between the Perl idea of location, where the first location of a string is position 0, and the biologist&amp;rsquo;s idea of location, where the first location is position 1.) The code is short, to the point, and gets the job done. &lt;strong&gt;Unfortunately, it also makes as many copies of the entire chromosome as there are exons to print out&lt;/strong&gt;. &lt;em&gt;Ouch.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In such circumstances, you can save a boatload of pain by eliminating the subroutine entirely, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while ((my $begin, my $end) =  each %exon_endpoints) {
    print substr($chromosome, $begin - 1, $end - $begin + 1), &amp;quot;\n\n&amp;quot;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The bad news: now the details of how to extract the desired exon from the chromosome are right in the loop, instead of being nicely tucked away in the subroutine &lt;code&gt;get_exon&lt;/code&gt;. The good news: the program will finish running before the weekend.&lt;/p&gt;

&lt;h3 id=&#34;sequence-motifs-with-bounded-lengths&#34;&gt;Sequence Motifs with Bounded Lengths&lt;/h3&gt;

&lt;p&gt;In Part I, I showed you how to search for a small pattern in a very large file of DNA data (in &lt;em&gt;FASTA&lt;/em&gt; format) by only keeping at most two lines of the file in the program&amp;rsquo;s memory at any one time.&lt;/p&gt;

&lt;p&gt;Here is some code that generalizes that approach. It is more general because it allows you to declare the maximum and minimum pattern sizes. It uses no more than a certain maximum amount of main memory for the data at any one time. For instance, if you&amp;rsquo;re looking for a pattern and you know that any match must be between 300 and 2,000 bases long, you can use this subroutine to search any amount of DNA while keeping the amount of main memory used for the DNA to within about 4,000 bytes, twice the maximum pattern size. Only matching patterns between 300 and 2,000 bases long will be reported.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl
#
# find_size_bounded_pattern : find a pattern known to be between a min and max length
#   Keep small size of memory but handle arbitrarily large input files
#
#  Copyright (c) 2003 James Tisdall
#

use warnings;
use strict;
use Carp;

my $pattern  = &amp;quot;GGTGGAC[GT].{50,1500}[AC][GT][CG]ATAT&amp;quot;;
my $filename = &amp;quot;Fly.dna&amp;quot;;
my $min      = 65;
my $max      = 1515;

my @locations = find_size_bounded_pattern($pattern, $filename, $min, $max);

print &amp;quot;@locations\n&amp;quot;;

exit;

### End of main program
##################################################

##################################################
### Subroutines:

sub find_size_bounded_pattern {

    ################# Arguments:
    # $pattern   - the pattern to search for, as a regular _expression
    # $filename  - the name of the DNA fasta file (may have multiple records)
    # $min       - the shortest length of a usable match to the pattern
    # $max       - the longest length of a usable match to the pattern
    #################
    my($pattern, $filename, $min, $max) = @_;

    ################# Other variables:
    # $buffer    - a buffer to store the DNA text, usually (2 * $max) in length
    # $position  - the position of the beginning of the buffer in the DNA
    # @locations - the locations where the pattern was found, to be returned
    #              @locations also includes headers for each fasta record
    # $header    - the one-line fasta header for each record in a fasta file
    my $buffer = &#39;&#39;;
    my $position = 0;
    my @locations = ();
    my $header = &#39;&#39;;
    #################

    # Open the DNA file
    open(DNA,&amp;quot;&amp;lt;$filename&amp;quot;) or croak(&amp;quot;Cannot open $filename:$!\n&amp;quot;);

    # Get the input lines and compute
    while(my $newline = &amp;lt;DNA&amp;gt; ) {

        # If new fasta header, reinitialize buffer and location counter
        if($newline =~ /^&amp;gt;/) {
            # Complete previous search in buffer which contains end of fasta record
            while($buffer =~ /$pattern/gi) {
                if($-[0] &amp;lt;= length($buffer) - $min) {
                    unless(($+[0] - $-[0] &amp;lt; $min) or ($+[0] - $-[0] &amp;gt; $max)) {
                        push(@locations, $position + $-[0] + 1);
                    }
                }
            }
            # Reset $header, $buffer, $position for new fasta record
            $header = $newline;
            push(@locations, &amp;quot;\n$header&amp;quot;);
            buffer = &#39;&#39;;
        $position = 0;

            # Get new line from file
            next;
        }

        # If new line of DNA data

        # Add the new line to the buffer
        chomp $newline;
        $buffer .= $newline;

        if(length($buffer) &amp;lt; (2 * $max) ) {
            next;
        }

        # Search for the DNA pattern
        # (Report the character at position 0 as at position 1, as usual in biology)
        while($buffer =~ /$pattern/gi) {
            if($-[0] &amp;lt; $max) {
                unless(($+[0] - $-[0] &amp;lt; $min) or ($+[0] - $-[0] &amp;gt; $max)) {
                    push(@locations, $position + $-[0] + 1);
                }
            }else{
                last;
            }
        }

        # Reset the position counter
        # (will be accurate after you reset the buffer, next)
        $position = $position + $max;

        # Reset the buffer
        # Discard the first $max worth of data in the buffer
        $buffer = substr($buffer, $max);
    }

    # Complete search in final buffer
    while($buffer =~ /$pattern/gi) {
        if($-[0] &amp;lt;= length($buffer) - $min) {
            unless(($+[0] - $-[0] &amp;lt;$min) $-[0] ($+[0] - or&amp;gt; $max)) {
                push(@locations, $position + $-[0] + 1);
            }
        }
    }

    # Computation complete
    return @locations;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;how-the-code-works&#34;&gt;How the Code Works&lt;/h3&gt;

&lt;p&gt;This code gets its DNA from a FASTA-formatted file (FASTA is the most common format for a file of DNA sequence data). It would be fairly easy to rewrite this so that you could give multiple FASTA filenames on a command line and all the files would be processed by this code. As it is, it can handle a single FASTA file that contains multiple FASTA records.&lt;/p&gt;

&lt;p&gt;The subroutine &lt;code&gt;find_size_bounded_pattern&lt;/code&gt; returns an array of all the locations in the DNA that contain the pattern. Since the input may have several FASTA records, the one-line header of each record is also returned, to help identify the start of each new record. For instance, I tested this program on a file, &lt;code&gt;Fly.dna&lt;/code&gt;, that contains all the chromosomes of the fruit fly, &lt;em&gt;Drosophila melanogaster&lt;/em&gt;. In this file, each new chromosome begins with a new FASTA header, which is added to the returned array. The locations reported start afresh from 1 for each chromosome.&lt;/p&gt;

&lt;p&gt;The pattern to be searched for is only reported if it&amp;rsquo;s between a certain minimum and maximum length. Twice the maximum desired pattern length (plus the length of an input line) is the limit of the amount of DNA data that is read into the program&amp;rsquo;s buffer. That way you can search a &lt;code&gt;$max&lt;/code&gt; worth of DNA for the beginning locations of patterns that may be up to &lt;code&gt;$max&lt;/code&gt; long.&lt;/p&gt;

&lt;p&gt;The overall structure of this code is pretty simple, and the comments in the code should do most of the explaining. There are two situations dealt with in the loop as it reads input lines. First is when there is a new FASTA header. Then you have to finish searching in the buffer, and reset the variables to begin a search in a new sequence of DNA from a new FASTA record. Second is when there is a new line of DNA. And finally, after all the lines have been read and you exit the loop, there may still be some unsearched DNA in the buffer, so the subroutine ends by searching the DNA remaining in the last buffer.&lt;/p&gt;

&lt;p&gt;In this code, the devil is in the details of how the specific locations and sizes are set. The intermediate level Perl programmer should be able to puzzle this out given the comments in the code. Note that after a successful pattern match the builtin variable &lt;code&gt;$-[0]&lt;/code&gt; has the offset of the beginning of the match, and &lt;code&gt;$+[0]&lt;/code&gt; has the offset of the end of the match. This avoids the use of the special variable &lt;code&gt;$&amp;amp;&lt;/code&gt;, the use of which causes all manner of space to be used to hold this and several other special variables. But if your regular expression has any parentheses, that&amp;rsquo;s enough to make the special variables and their considerable space get used too. Of course, regular expressions have their own rules of behavior, such as greedy matching and so forth, that are not addressed by this code. (Could you modify this program to find patterns that overlap each other? What happens if &lt;code&gt;$max&lt;/code&gt; is less than the input line size? What other assumptions are made by this code?)&lt;/p&gt;

&lt;h3 id=&#34;profiling-the-speed-of-your-perl-program&#34;&gt;Profiling the Speed of your Perl Program&lt;/h3&gt;

&lt;p&gt;You can profile the speed of your Perl program fairly easily. Let&amp;rsquo;s say I put the program in a file called &lt;em&gt;sizebound.pl&lt;/em&gt;. Then I can get a report on the time the various parts of the program require by running the program like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[tisdall@coltrane]$ perl -d:DProf sizebound.pl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then getting a summary of the report (from the file tmon.out that DProf creates) like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[tisdall@coltrane]$ dprofpp
Total Elapsed Time =  95.1899 Seconds
  User+System Time =  94.9099 Seconds
Exclusive Times
%Time ExclSec CumulS #Calls sec/call Csec/c  Name
 99.9   94.87 94.870      1   94.870 94.870  main::find_size_bounded_pattern
 0.02   0.020  0.020      3   0.0067 0.0067  main::BEGIN
 0.00   0.000 -0.000      1   0.0000      -  warnings::BEGIN
 0.00   0.000 -0.000      2   0.0000      -  Exporter::import
 0.00   0.000 -0.000      1   0.0000      -  warnings::import
 0.00   0.000 -0.000      1   0.0000      -  strict::import
 0.00   0.000 -0.000      1   0.0000      -  strict::bits
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you have lots of subroutines, this can really help you see where the most time is being spent. Here, I&amp;rsquo;m really just getting the information that the program took about a minute and a half to look for the pattern in the DNA of the fruit fly.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also possible to get information about the space usage of a program, but you have to use a version of Perl that was compiled with -DDEBUG, which is not usually the case. If you have such a version, then the following will give you some information:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[tisdall@coltrane]$ perl -DL sizebound.pl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But that&amp;rsquo;s enough for here and now; take a look at the Perl documentation section called perldebguts. And drive safely.&lt;/p&gt;

&lt;h3 id=&#34;suggested-reading&#34;&gt;Suggested Reading&lt;/h3&gt;

&lt;p&gt;Here are some of the many books that you might find useful. I cut my teeth on the Bentley books, but the older ones are hard to find.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Introduction to Algorithms, Second Edition, by Cormen et al, MIT Press, 2001.&lt;/li&gt;
&lt;li&gt;Writing Efficient Programs, by Jon Bentley, Prentice Hall 1982.&lt;/li&gt;
&lt;li&gt;Refactoring: Improving the Design of Existing Code, by Fowler et al, Addison-Wesley, 1999.&lt;/li&gt;
&lt;li&gt;Programming Pearls, Second Edition, by Jon Bentley, Prentice Hall 1999.&lt;/li&gt;
&lt;li&gt;More Programming Pearls: Confessions of a Coder, by Jon Bentley, Pearson Education, 1988.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;O&amp;rsquo;Reilly &amp;amp; Associates recently released (September 2003) &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/index.html?CMP=IL7015&#34;&gt;Mastering Perl for Bioinformatics&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/chapter/index.html?CMP=IL7015&#34;&gt;Sample Chapter 9, Introduction to Bioperl&lt;/a&gt;, is available free online.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can also look at the &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/toc.html?CMP=IL7015&#34;&gt;Table of Contents&lt;/a&gt;, the &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/inx.html?CMP=IL7015&#34;&gt;Index&lt;/a&gt;, and the &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/desc.html?CMP=IL7015&#34;&gt;Full Description&lt;/a&gt; of the book.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For more information, or to order the book, &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/index.html?CMP=IL7015&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Chromosome at a Time with Perl, Part 1</title>
      <link>http://localhost:1313/pub/2003/09/10/bioinformatics.html/</link>
      <pubDate>Thu, 11 Sep 2003 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2003/09/10/bioinformatics.html/</guid>
      <description>

&lt;p&gt;&lt;em&gt;James D. Tisdall is the author of the soon-to-be-released&lt;/em&gt; &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/index.html?CMP=IL7015&#34;&gt;Mastering Perl for Bioinformatics&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For some time now, the use of Perl in biology has been standard practice. Perl remains the most popular language among biologists for a multitude of programming tasks. The same reasons why Perl has been a success story among system administrators, as well as one of the big success stories in the early days of the Web and CGI programming, have also made it the lingua franca of programming in biology, known as &lt;em&gt;bioinformatics&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One of the reasons why Perl has been equally well suited to dealing with things like DNA and protein sequence data is that it&amp;rsquo;s so easy to declare and use a string. You just use it, without worrying about allocating memory, or managing memory as the string shrinks or grows. DNA and proteins and other standard biological data are almost always represented in Perl as strings, so this facility with strings translates directly into a facility with DNA and proteins.&lt;/p&gt;

&lt;p&gt;For example, say you have a subroutine &lt;code&gt;get_chromosome&lt;/code&gt; that returns a string of all the DNA in a human chromosome. In humans, this might be a string about 100Mb in length. This snippet of code calls &lt;code&gt;get_chromosome&lt;/code&gt; to initialize a scalar variable, &lt;code&gt;$chromosome1&lt;/code&gt;, with the string of DNA sequence data that summarizes human chromosome 1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$chromosome1 = get_chromosome( 1 );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This programming is as easy as cake. I mean, simple as pie. Well, you know what I mean.&lt;/p&gt;

&lt;p&gt;But beneath this wonderful ease of programming lurks a problem. It&amp;rsquo;s a problem that can make your wonderful, intuitive code for tossing around chromosomes and genomes&amp;ndash;which looks so elegant in your printout, and which appears so neatly divided into intuitively satisfying, interacting subroutines&amp;ndash;an inefficient mess that barely runs at all, when it&amp;rsquo;s not completely clogging up your computer.&lt;/p&gt;

&lt;p&gt;So, in this short article I&amp;rsquo;ll show you a handful of tricks that enable you to write code for dealing with large amounts of biological sequence data&amp;ndash;in this case, very long strings&amp;ndash;while still getting satisfactory speed from the program.&lt;/p&gt;

&lt;h3 id=&#34;memory-is-the-bottleneck&#34;&gt;Memory is the Bottleneck&lt;/h3&gt;

&lt;p&gt;What is the problem, exactly? It usually comes down to this: by dealing with very large strings, each one of which uses a significant portion of the main memory that your computer uses to hold a running program, you can easily overtax the amount of main memory available.&lt;/p&gt;

&lt;p&gt;When a program on your computer (a &lt;em&gt;process&lt;/em&gt; on your Linux, Unix, or Mac OS X computer) runs out of main memory, its performance starts to seriously degrade. It may try to overcome the lack of fast and efficient main memory by enlisting a portion of disk space to hold the part of the running program that it can no longer fit.&lt;/p&gt;

&lt;p&gt;But when a program starts writing and reading to and from hard disk memory it can get awfully slow awfully fast. And depending on the nature of the computation, the program may start &amp;ldquo;thrashing,&amp;rdquo; that is, repeatedly writing and reading large amounts of data between main memory and hard disk. Your elegant program has turned into a greedy, lazy misanthrope that grabs up all the resources available and then seems to just sit there. You&amp;rsquo;ve created a monster!&lt;/p&gt;

&lt;p&gt;Take the snippet of code above that calls &lt;code&gt;get_chromosome&lt;/code&gt;. Without knowing anything more about the subroutine, it&amp;rsquo;s a pretty good bet that it is fetching the 100Mb of data from somewhere, perhaps a disk file, or a relational database, or a web site. To do so, it must be using at least 100Mb of memory. Then, when it returns the data to be stored in &lt;code&gt;$chromosome1&lt;/code&gt;, the program uses another 100Mb of memory. Now, perhaps you want to do a regular expression search on the chromosome, saving the desired expression with parentheses that set the special variables &lt;code&gt;$1&lt;/code&gt;, &lt;code&gt;$&amp;amp;&lt;/code&gt;, and so on. These special variables can be quite large, and that means use of even more memory by your program.&lt;/p&gt;

&lt;p&gt;And since this is elegant, simple code you&amp;rsquo;ve written, you may well make other copies of the chromosome data or portions of it, in your tenacious hunt for the elusive cure for a deadly disease. The resulting code may be clear, straightforward to understand, and correct&amp;ndash;all good and proper things for code to be&amp;ndash;but the amount of string copies will land you in the soup. Not only does copying a large string take up memory, but the actual copying can itself be slow, especially if there&amp;rsquo;s a lot of it.&lt;/p&gt;

&lt;h3 id=&#34;space-efficiency&#34;&gt;Space Efficiency&lt;/h3&gt;

&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;div class=&#34;secondary&#34;&gt;
&lt;h4 id=&#34;more-from-this-author&#34;&gt;More from this author&lt;/h4&gt;
&lt;p&gt;• &lt;a href=&#34;http://localhost:1313/pub/2001/11/16/perlbio2.html&#34;&gt;Parsing Protein Domains with Perl&lt;/a&gt;&lt;br /&gt;
• &lt;a href=&#34;http://localhost:1313/pub/2002/01/02/bioinf.html&#34;&gt;Beginning Bioinformatics&lt;/a&gt;&lt;br /&gt;
• &lt;a href=&#34;http://www.oreilly.com/news/perlbio_1001.html&#34;&gt;Why Biologists Want to Program Computers&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You may need to add a new constraint to your program design when you&amp;rsquo;ve got a large amount of data in a running program. The constraint is &lt;em&gt;&amp;ldquo;Use minimal memory.&amp;rdquo;&lt;/em&gt; Often, a program that barely runs at all and takes many hours of clogging up the computer, can be rewritten to run in a few minutes by reworking the algorithm so that it uses only a small fraction of the memory.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a case of decreasing time by first decreasing space. (Astrophysicists, take note.)&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s one easy way to cut down on the number of big strings in a program.&lt;/p&gt;

&lt;p&gt;If you need a subroutine to return a large string, as in the &lt;code&gt;get_chromosome&lt;/code&gt; subroutine I&amp;rsquo;ve used as an example, you can use &lt;em&gt;references&lt;/em&gt; to eliminate some of this memory usage.&lt;/p&gt;

&lt;p&gt;The practice of passing references to a subroutine is familiar to experienced Perl programmers. In our example, we can rewrite the subroutine so that the return value is placed into a string that is passed in as an argument. But we don&amp;rsquo;t pass a copy of the string&amp;ndash;we pass a reference to the string, which takes almost no additional space, and which still enables the subroutine to provide the entire chromosome 1 DNA to the calling program. Here&amp;rsquo;s an example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;load_chromosome( 1, \$chromosome1 );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This new subroutine has two arguments. The &lt;code&gt;1&lt;/code&gt; presumably will tell the subroutine which human chromosome we want (we want the biggest human chromosome, chromosome 1).&lt;/p&gt;

&lt;p&gt;The second argument is a reference to a scalar variable. Inside the subroutine, the reference is most likely used to initialize an argument like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my($chromnumber, $chromref) = @_;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then the DNA data is put into the string by calling it &lt;code&gt;$$chromref&lt;/code&gt;, for instance like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$$chromref = &#39;ACGTGTGAACGGA&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;No return value is needed. After the subroutine call, the main program will find that the contents of &lt;code&gt;$chromosome1&lt;/code&gt; have changed, and now consist of &amp;ldquo;ACGTGTGAACGGA.&amp;rdquo; (Of course, a chromosome is much longer than this little fragment.)&lt;/p&gt;

&lt;p&gt;Using references is also a great way to pass a large amount of data &lt;em&gt;into&lt;/em&gt; a subroutine without making copies of it. In this case, however, the fact that the subroutine can change the contents of the referenced data is something to watch out for. Sometimes you just want a subroutine to get to use the data, but you expect the variable containing the data to still have the same data after the subroutine gets a look at it. So you have to watch what you do when you&amp;rsquo;re passing references to data into a subroutine, and make sure you know what you want.&lt;/p&gt;

&lt;h3 id=&#34;managing-memory-with-buffers&#34;&gt;Managing Memory with Buffers&lt;/h3&gt;

&lt;p&gt;One of the most efficient ways to deal with very large strings is to deal with them a little at a time.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of a program for searching an entire chromosome for a particular 12-base pattern, using very little memory. (A &lt;em&gt;base&lt;/em&gt; is one of the four molecules that are the principal building blocks of DNA. The four bases are represented in Perl strings as the characters A, C, G, and T. You&amp;rsquo;ll often hear biologists talking about &amp;ldquo;megabases&amp;rdquo; instead of &amp;ldquo;megabytes&amp;rdquo; in a string. If you hear that, you&amp;rsquo;re probably talking to a bioinformatician.)&lt;/p&gt;

&lt;p&gt;When writing a program that will search for any regular expression in a chromosome, it&amp;rsquo;s hard to see how you could avoid putting the whole chromosome in a string. But very often there&amp;rsquo;s a limit to the size of what you&amp;rsquo;re searching for. In this program, I&amp;rsquo;m looking for the 12-base pattern &amp;ldquo;ACGTACGTACGT.&amp;rdquo; And I&amp;rsquo;m going to get the chromosome data from a disk file.&lt;/p&gt;

&lt;p&gt;My trick is going to be to just read in the chromosome data a line or two at a time, search for the pattern, and then &lt;em&gt;reuse&lt;/em&gt; the memory to read in the next line or two of data.&lt;/p&gt;

&lt;p&gt;The extra work I have to do in programming is, first, I need to keep track myself of how much of the data has been read in, so I can report the locations in the chromosome of successful searches. Second, I need to keep aware that my pattern might start at the end of one line and complete at the beginning of the next line, so I need to make sure I search across line breaks as well as within lines of data from the input file.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a small program that reads in a FASTA file given as an argument on the command line and searches for my pattern in any amount of DNA&amp;ndash;a whole chromosome, a whole genome, even all known genetic data, just assuming that the data is in a FASTA file named in the command line. I&amp;rsquo;ll call my program &lt;code&gt;find_fragment&lt;/code&gt;, and assuming the DNA is in a FASTA file called &lt;code&gt;human.dna&lt;/code&gt;, I&amp;rsquo;ll call it like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[tisdall@coltrane]$ perl find_fragment human.dna
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For testing purposes I made a very short FASTA DNA file, &lt;code&gt;human.dna&lt;/code&gt;, which contains:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; human dna--Not!  The fragment ACGTACGTACGT appears at positions 10, 40, and 98
AAAAAAAAAACGTACGTACGTCCGCGCGCGCGCGCGCGCACGTACGTACG
TGGGGGGGGGGGGGGGCCCCCCCCCCGGGGGGGGGGGGAAAAAAAAAACG
TACGTACGTTTTTTTTTTTTTTTTTTTTTTTTTTT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s the code for the program &lt;code&gt;find_fragment&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl

#
# find_fragment : find &#39;ACGTACGTACGT&#39; in a very large DNA FASTA file
# using minimal memory
#
#  N.B. This example program does no checking of the input to ensure
#       that it is DNA data in FASTA format; it just assumes that
#       it is. This program also assumes there is just one FASTA
#       record in the input file.
#
#  Copyright (c) 2003 James Tisdall
#

use warnings;
use strict;
use Carp;

# Make sure the program is called with one argument, presumably a
# FASTA file of DNA
my $USAGE = &amp;quot;perl find_fragment file.FASTA&amp;quot;;
unless(@ARGV == 1) { croak &amp;quot;$USAGE:$!\n&amp;quot; }

# $fragment: the pattern to search for
# $fraglen:  the length of $fragment
# $buffer:   a buffer to hold the DNA from the input file
# $position: the position of the buffer in the total DNA
my($fragment, $fraglen, $buffer, $position) = (&#39;ACGTACGTACGT&#39;, 12, &#39;&#39;, 0);

# The first line of a FASTA file is a header and begins with &#39;&amp;gt;&#39;
my $header = &amp;lt;&amp;gt;;

# Get the first line of DNA data, to start the ball rolling
$buffer = &amp;lt;&amp;gt;;
chomp $buffer;

# The remaining lines are DNA data ending with newlines
while(my $newline = &amp;lt;&amp;gt;) {

    # Add the new line to the buffer
    chomp $newline;
    $buffer .= $newline;

    # Search for the DNA fragment, which has a length of 12
    # (Report the character at string position 0 as being at position 1,
    # as usual in biology)
    while($buffer =~ /$fragment/gi) {
        print &amp;quot;Found $fragment at position &amp;quot;, $position + $-[0] + 1, &amp;quot;\n&amp;quot;;
    }

    # Reset the position counter (will be true after you reset the buffer, next)
    $position = $position + length($buffer) - $fraglen + 1;

    # Discard the data in the buffer, except for a portion at the end
    # so patterns that appear across line breaks are not missed
    $buffer = substr($buffer, length($buffer) - $fraglen + 1, $fraglen - 1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s the output of running the command &lt;code&gt;perl find_fragment human.dna&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Found ACGTACGTACGT at position 10
Found ACGTACGTACGT at position 40
Found ACGTACGTACGT at position 98
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;how-the-code-works&#34;&gt;How the Code Works&lt;/h3&gt;

&lt;p&gt;After the highly recommended &lt;code&gt;use strict&lt;/code&gt; and &lt;code&gt;use warnings&lt;/code&gt; are turned on, and the Carp module is loaded so the program can &amp;ldquo;croak&amp;rdquo; when needed, the program variables are declared and initialized.&lt;/p&gt;

&lt;p&gt;The first line of the FASTA file is a header and is not needed here, so it&amp;rsquo;s read and not used. Then the first line of DNA data is read into the buffer and its &lt;code&gt;newline&lt;/code&gt; character is removed. I start with this because I want to search for the fragment even if it is broken by new lines, so I&amp;rsquo;ll have to look at least at the first two lines; here I get the first line, and in the while loop that follows I&amp;rsquo;ll start by adding the second line to the buffer.&lt;/p&gt;

&lt;p&gt;Then the while loop, which does the main work of the program, starts reading in the next line of the FASTA file named on the command line, in this case the FASTA file &lt;code&gt;human.dna&lt;/code&gt;. The &lt;code&gt;newline&lt;/code&gt; is removed with &amp;ldquo;chomp,&amp;rdquo; and the new line is added to the buffer.&lt;/p&gt;

&lt;p&gt;Then comes the short while loop that does the regular expression pattern match of the &lt;code&gt;$fragment&lt;/code&gt; in the &lt;code&gt;$buffer&lt;/code&gt;. It has modifiers &amp;ldquo;g&amp;rdquo; for *g*lobal search (the fragment may appear more than once in the buffer); and &amp;ldquo;i&amp;rdquo; for case *i*nsensitive search, that is, either uppercase or lowercase DNA data (e.g. ACGT or acgt).&lt;/p&gt;

&lt;p&gt;When the fragment is found the program simply prints out the position. &lt;code&gt;$position&lt;/code&gt; holds the position of the beginning of the buffer in the total DNA, and is something I have to keep track of. &lt;code&gt;$-[0]&lt;/code&gt; is a special variable that gives the offset of the last successful pattern match in the string. I also add 1, because biologists always say that the first base in a sequence of DNA is at position 1, whereas Perl says that the first character in a string is at position 0. So I add 1 to the Perl position to get the biologist&amp;rsquo;s position.&lt;/p&gt;

&lt;p&gt;The last two lines of code reset the buffer by eliminating the beginning part of it, and then adjust the position counter accordingly. The buffer is shortened so that it just keeps the part at the very end that might be part of a pattern match that crosses over the lines of the input file. This would be the tail part of the buffer that is just one base shorter than the length of the fragment.&lt;/p&gt;

&lt;p&gt;In this way, the program keeps at most two lines&amp;rsquo; worth of DNA in &lt;code&gt;$buffer&lt;/code&gt;, but still manages to search the entire genome (or chromosome or whatever is in the FASTA file) for the fragment. It performs very quickly, compared to a program that reads in a whole genome and blows out the memory in the process.&lt;/p&gt;

&lt;h3 id=&#34;when-you-should-bother&#34;&gt;When You Should Bother&lt;/h3&gt;

&lt;p&gt;A space-inefficient program might well work fine on your better computers, but it won&amp;rsquo;t work well at all when you need to run it on another computer with less main memory installed. Or, it might work fine on the fly genome, but slow to a crawl on the human genome.&lt;/p&gt;

&lt;p&gt;The rule of thumb is that if you know you&amp;rsquo;ll be dealing with large data sets, consider the amount of space your program uses as an important constraint when designing and coding. Then you won&amp;rsquo;t have to go back and redo the entire program when a large amount of DNA gets thrown at you.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Editor&amp;rsquo;s note: Stay tuned for part two in this two-part series later this month. In it, James will take a more in-depth look at space efficiency, and include a more general version of a program that uses a buffer. In particular, part two will cover running subroutines with minimal space, eliminating subroutines altogether, and sequence motifs with bounded lengths.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;O&amp;rsquo;Reilly &amp;amp; Associates will soon release (September 2003) &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/index.html?CMP=IL7015&#34;&gt;Mastering Perl for Bioinformatics&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/chapter/index.html?CMP=IL7015&#34;&gt;Sample Chapter 9, Introduction to Bioperl&lt;/a&gt;, is available free online.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can also look at the &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/toc.html?CMP=IL7015&#34;&gt;Table of Contents&lt;/a&gt;, the &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/inx.html?CMP=IL7015&#34;&gt;Index&lt;/a&gt;, and the &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/desc.html?CMP=IL7015&#34;&gt;Full Description&lt;/a&gt; of the book.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For more information, or to order the book, &lt;a href=&#34;http://www.oreilly.com/catalog/mperlbio/index.html?CMP=IL7015&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Genomic Perl</title>
      <link>http://localhost:1313/pub/2003/02/27/review.html/</link>
      <pubDate>Thu, 27 Feb 2003 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2003/02/27/review.html/</guid>
      <description>&lt;p&gt;This is a book I have been looking forward to for a long time. Back when James Tisdall had just finished his &lt;a href=&#34;http://www.oreilly.com/catalog/begperlbio/&#34;&gt;Beginning Perl for Bioinformatics&lt;/a&gt;, I asked him to write an article about &lt;a href=&#34;http://localhost:1313/pub/2002/01/02/bioinf.html&#34;&gt;how to get into bioinf&lt;/a&gt; from a Perl programmer&amp;rsquo;s perspective. With bioinformatics being a recently booming sphere and many scientists finding themselves in need of computer programmers to help them process their data, I thought it would be good if there was more information for programmers about the genomic side of things.&lt;/p&gt;

&lt;p&gt;Rex Dwyer has produced a book, &lt;a href=&#34;http://books.cambridge.org/052180177X.htm&#34;&gt;Genomic Perl&lt;/a&gt;, which bridges the gap. As well as teaching basic Perl programming techniques to biologists, it introduces many useful genetic concepts to those already familiar with programming. Of course, as a programmer and not a biologist, I&amp;rsquo;m by no means qualified to assess the quality of that side of the book, but I certainly learned a lot from it.&lt;/p&gt;

&lt;p&gt;The first chapter, for instance, taught the basics of DNA transcription and translation, the basics of Perl string handling, and tied the two concepts together with an example of transcription in Perl. This is typical of the format of the book - each chapter introduces a genetic principle and a related problem, a Perl principle which can be used to solve the problem, and a program illustrates both. It&amp;rsquo;s a well thought-out approach which deftly caters for both sides of the audience.&lt;/p&gt;

&lt;p&gt;However, it should be stressed that the book is a substitute neither for a good introductory book on Perl nor a good textbook on genetics; and indeed, I think it will turn out to be better for programmers who need an over-arching idea of some of the problems involved with bioinformatics than for biologists who need to turn out working code. For instance, when it states that a hash is the most convenient data structure for looking up amino acids by their codons, it doesn&amp;rsquo;t say why, or even what a hash is. On the other hand, amino acids and codons are both explained in detail.&lt;/p&gt;

&lt;p&gt;The book covers a wide range of biological areas - from the structure of DNA to building predictive models of species, exploring the available databases of genetic sequences including readers of the GenBank database and an implementation of the BLAST algorithm, phylology, protein databases, DNA sequence assembly and prediction, restriction mapping, and a lot more besides. In all, it&amp;rsquo;s a good overview of the common areas in which biologists need computer programs.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a significant but non-threatening amount of math in there, particularly in dealing with probabilities of mutation and determining whether or not events are significant, but I was particularly encouraged to see discussion of algorithmic running time; as the author is primarily a computer science professor and secondarily a bioinformaticists, this should not be too surprising. However, a significant number of bioinformaticists tend to produce code which works&amp;hellip; eventually. Stopping to say &amp;ldquo;well, this is order n-to-the-6 and we can probably do better than that&amp;rdquo; is most welcome.&lt;/p&gt;

&lt;p&gt;Onto the code itself. The first thing any reader will notice about the book is that the code isn&amp;rsquo;t monospaced. Instead, the code is &amp;ldquo;ground&amp;rdquo;, pretty-printed, as in days of old. This means you&amp;rsquo;ll see code snippets like:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;next unless&lt;/strong&gt; $succReadSite; &lt;em&gt;## dummy sinks have no successor&lt;/em&gt;
&lt;strong&gt;my&lt;/strong&gt; $succContigSite = $classNames-&amp;gt;find($succReadSite);&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, I have to admit I really like this, but others may find it difficult to read, and those who know slightly less Perl may find it confusing - the distinction between &amp;ldquo; and &amp;rdquo; (that&amp;rsquo;s two single quotes and a double quote) can be quite subtle, and if you&amp;rsquo;re going to grind Perl code, regular expressions really, really ought to be monospaced. &amp;ldquo;&lt;span style=&#34;font-family:sans-serif&#34;&gt;$elem =~ /^[(^\[(]*)(\(.*\))?$/;&lt;/span&gt;&amp;rdquo; is just plain awkward.&lt;/p&gt;

&lt;p&gt;The code is more idiomatic than much bioinformatic code that I&amp;rsquo;ve seen, but still feels a little unPerlish; good use is made of references, callbacks and object oriented programming, but three-argument &lt;code&gt;for&lt;/code&gt; is used more widely than the fluent Perl programmer would have it, and things like&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    main();
    sub main {
        ...
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;worry me somewhat. But it works, it&amp;rsquo;s efficient, and it&amp;rsquo;s certainly enough to get the point across.&lt;/p&gt;

&lt;p&gt;The appendices were enlightening and well thought-out: the first turns an earlier example, RNA structure folding, into a practical problem of drawing diagrams of folded RNA for publication; the other two tackle matters of how to make some of the algorithms in the text more efficient.&lt;/p&gt;

&lt;p&gt;All in all, I came away from this book not just with more knowledge about genetics and biology - indeed, some of what I learned has been directly applicable to some work I have - but also with an understanding of some of the complexity of the problems geneticists face. It fully satisfies its goals, expressed in the preface: teaching computer scientists the biological underpinnings of bioinformatics, providing real, working code for biologists without boring the programmers, and providing an elementary handling of the statistical considerations of the subject matter. While it will end up being more used by programmers getting into the field, it&amp;rsquo;s still useful for the biologists already there, particularly when combined with something like James Tisdall&amp;rsquo;s book or &lt;a href=&#34;http://www.oreilly.com/catalog/lperl3&#34;&gt;Learning Perl&lt;/a&gt;. But for the programmer like me, interested in what biologists do and how we can help them do it, it&amp;rsquo;s by far the clearest introduction available, and I would heartily recommend it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Beginning Bioinformatics</title>
      <link>http://localhost:1313/pub/2002/01/02/bioinf.html/</link>
      <pubDate>Wed, 02 Jan 2002 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2002/01/02/bioinf.html/</guid>
      <description>

&lt;p&gt;Bioinformatics, the use of computers in biology research, has been increasing in importance during the past decade as the Human Genome Project went from its beginning to the announcement last year of a &amp;ldquo;draft&amp;rdquo; of the complete sequence of human DNA.&lt;/p&gt;

&lt;p&gt;The importance of programming in biology stretches back before the previous decade. And it certainly has a significant future now that it is a recognized part of research into many areas of medicine and basic biological research. This may not be news to biologists. But Perl programmers may be surprised to find that their handsome language has become one of the most - if not &lt;em&gt;the&lt;/em&gt; most popular - of computer languages used in bioinformatics.&lt;/p&gt;

&lt;p&gt;My new book &lt;a href=&#34;http://www.oreilly.com/catalog/begperlbio/&#34;&gt;&lt;strong&gt;Beginning Perl for Bioinformatics&lt;/strong&gt;&lt;/a&gt; from &lt;em&gt;O&amp;rsquo;Reilly &amp;amp; Associates&lt;/em&gt; addresses the needs of biologists who want to learn Perl programming. In this article, I&amp;rsquo;m going to approach the subject from another, almost opposite, angle. I want to address the needs of Perl programmers who want to learn biology and bioinformatics.&lt;/p&gt;

&lt;p&gt;First, let me talk about ways to go from Perl programmer to &amp;ldquo;bioinformatician&amp;rdquo;. I&amp;rsquo;ll describe my experience, and give some ideas for making the jump. Then, I&amp;rsquo;ll try to give you a taste of modern biology by talking about some of the technology used in the sequencing of genomes.&lt;/p&gt;

&lt;h3 id=&#34;my-experience&#34;&gt;My Experience&lt;/h3&gt;

&lt;p&gt;Bioinformaticians generally have either a biology or programming background, and then receive additional training in the other field. The common wisdom is that it&amp;rsquo;s easier for biologists to pick up programming than the other way around; but, of course, it depends on the individual. How does one take the skills learned while programming in, say, the telecommunications industry, and bring them to a job programming for biology?&lt;/p&gt;

&lt;p&gt;I used to work at Bell Labs in Murray Hill, N.J., in the Speech Research Department. It was my first computer programming job; I got to do stuff with computer sound, and learn about speech science and linguistics as well. I also got to do some computer music on the side, which was fantastic for me. I became interested in the theory of computer science, and entered academia full time for a few years.&lt;/p&gt;

&lt;p&gt;When it became time for me to get back to a regular salary, the Human Genome Project had just started a bioinformatics lab at the university where I was studying. I had a year of molecular biology some years before as an undergraduate, but that was before the PCR technique revolutionized the field. At that time, I read Watson&amp;rsquo;s classic &amp;ldquo;The Molecular Biology of the Gene&amp;rdquo; and so I had an inkling about DNA, which probably helped, and I knew I liked the subject. I went over to meet the directors and leveraged my Unix and C and Bell Labs background to get a job as the systems manager. (PCR, the polymerase chain reaction, is the way we make enough copies (&amp;ldquo;clones&amp;rdquo;) of a stretch of DNA to be able to do experiments on it. After learning the basics of DNA &amp;ndash; keep reading! &amp;ndash; PCR would be a great topic to start learning about molecular biology techniques. I&amp;rsquo;ll explain how in just a bit.)&lt;/p&gt;

&lt;p&gt;In my new job I started working with bioinformatics software, both supporting and writing it. In previous years, I&amp;rsquo;d done practically no programming, having concentrated on complexity theory and parallel algorithms. Now I was plunged into a boatload of programming &amp;ndash; C, Prolog, Unix shell and FORTRAN were the principal languages we used. At that time, just as I was starting the job, a friend at the university pressed his copy of &lt;a href=&#34;http://oreilly.com/catalog/pperl3/&#34;&gt;Programming Perl&lt;/a&gt; into my hands. It made a strong impression on me, and in short order I was turning to Perl for most of the programming jobs I did.&lt;/p&gt;

&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://conferences.oreilly.com/biocon/&#34;&gt;O&amp;#39;Reilly Bioinformatics Technology Conference&lt;/a&gt;
&lt;p&gt;Don&#39;t miss the &lt;a href=&#34;http://conferences.oreillynet.com/cs/bio2002/view/e_sess/1935&#34;&gt;Beginning Perl for Bioinformatics session&lt;/a&gt;, Monday, January 28, 2002, at the &lt;a href=&#34;http://conferences.oreilly.com/biocon/&#34;&gt;O&#39;Reilly Bioinformatics Technology Conference&lt;/a&gt;.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I also started hanging out with the genome project people. I took some graduate courses in human genetics and molecular biology, which helped me a lot in understanding what everyone around me was doing.&lt;/p&gt;

&lt;p&gt;After a few years, when the genome project closed down at my university, I went to other organizations to do bioinformatics, first at a biotech startup, then at a national comprehensive cancer center, and now consulting for biology researchers. So that&amp;rsquo;s my story in a nutshell, which I offer as one man&amp;rsquo;s path from programming to bioinformatics.&lt;/p&gt;

&lt;h3 id=&#34;bringing-programming-to-biology&#34;&gt;Bringing Programming to Biology&lt;/h3&gt;

&lt;p&gt;Especially now that bioinformatics is seen as an important field, many biology researchers are adding bioinformatics to their grant proposals and research programs. I believe the kind of path that I took is even more possible now than then, simply due to the amount of bioinformatics funding and jobs that are now staffed. Find biology research organizations that are advertising for programmers, and let them know you have the programming skills and the interest in biology that would make you an asset to their work.&lt;/p&gt;

&lt;p&gt;But what about formal training? It&amp;rsquo;s true that the ideal bioinformatician has graduate degrees in both computer science and biology. But such people are extremely rare. Most workers in the field have a good mix of computer and biology skills, but their degrees tend to come from one or the other. Still, formal training in biology is a good way for a computer programmer to learn about bioinformatics, either preceding or concurrently with a job in the field.&lt;/p&gt;

&lt;p&gt;I can understand the reluctance to face another degree. (I earned my degrees with a job and a family to support, and it was stressful at times.) Yes, it is best to get a degree if you&amp;rsquo;re going to be working in biology. A masters degree is OK, but most of the best jobs go to those who have their doctrate degree. They are, however, in ample supply and often get relatively low pay, as in postdoc positions that are frequently inhabited for many years. So the economic benefit of formal training in biology is not great, compared to what you may be earning as a computer expert. But at present bioinformatics pays OK.&lt;/p&gt;

&lt;p&gt;On the other hand, to really work in biology, training is a good thing. It&amp;rsquo;s a deep subject, and in many ways quite dissimilar to computer science or electrical engineering or similar fields. It has many surprises, and the whole &amp;ldquo;wet lab&amp;rdquo; experimental approach is hard to get out of books.&lt;/p&gt;

&lt;p&gt;For self-study, there&amp;rsquo;s one book that I think is a real gem for Perl programmers who want to learn about modern biology research. The book is called &amp;ldquo;Recombinant DNA,&amp;rdquo; by the co-discoverer of the structure of DNA, James Watson, and his co-authors Gilman, Witkowski, Zoller, and Witkowski. The book was deliberately written for a wide audience, so you can start at the beginning with an explanation of what, exactly, are DNA and proteins, the two most important types of molecules in biology. But it goes on to introduce a wide range of fundamental topics in biology research, including explanations of the molecular biology laboratory techniques that form the basis of the revolution and the golden age in biology that we&amp;rsquo;re now experiencing. I particularly like the use of illustrations to explain the techniques and the biology &amp;ndash; they&amp;rsquo;re outstanding. In my jobs as manager of bioinformatics, I&amp;rsquo;ve always strongly urged the programmers to keep the book around and to dip into it often.&lt;/p&gt;

&lt;p&gt;The book does have one drawback, however. It was published in 1992. Ten years is as long in biology as it is in computer technology; so &amp;ldquo;Recombinant DNA&amp;rdquo; will not go into newer stuff such as microarrays or SNPs. (And don&amp;rsquo;t get the even earlier &amp;ldquo;Recombinant DNA: A Short Course&amp;rdquo; &amp;ndash; the 1992 edition is the one to get for now.) But what it does give you is a chance to really understand the fundamental techniques of modern molecular biology; and if you want to bring your Perl programming expertise to a biology research setting, then this is a great way to get a good start getting the general idea.&lt;/p&gt;

&lt;p&gt;There are a few other good books out, and several more coming during the next year, in the bioinformatics field. Waterman; Mount; Grant and Ewens; Baxevanis et al, and Pevzner are a few of the most popular books (some more theoretical than others). My book, although for beginning programmers, may be helpful in the later chapters to get an idea of basic biological data and programs. Gibas and Jambeck&amp;rsquo;s book &lt;a href=&#34;http://www.oreilly.com/catalog/bioskills/&#34;&gt;Developing Bioinformatics Computer Skills&lt;/a&gt; gives a good overview of much of the software and the general computational approach that&amp;rsquo;s used in bioinformatics, although it also includes some beginning topics unsuitable for the experienced programmer.&lt;/p&gt;

&lt;p&gt;Of all the bioinformatics programs that one might want to learn about, the Perl programmer will naturally gravitate toward the Bioperl project. This is an open-source, international collaborative effort to write useful Perl bioinformatics modules, and it has reached a point during the past few years where it is quite useful stuff. The 1.0 release may be available by the time you read this. Exploring this software, available at &lt;a href=&#34;http://www.bioperl.org&#34;&gt;http://www.bioperl.org&lt;/a&gt;, is highly recommended, with one caveat: It does not include much tutorial material, certainly not for teaching basic biology concepts. Still, you&amp;rsquo;ll find lots of great stuff to explore and use in Bioperl. It&amp;rsquo;s a must for the Perl bioinformatician.&lt;/p&gt;

&lt;p&gt;Apart from self-study, you may also want to try to get into some seminars or reading groups at the local university or biotech firm, or generally meet people. If you&amp;rsquo;re job hunting, then you may want to go introduce yourself to the head of the biology department at the U, and let her (yes, there are a lot of women working in biology research, a much better situation than in programming) &amp;ndash; know that you want a bioinformatics job and that you are a wizard at 1) programming in general, 2) Web programming, and 3) getting a lot out of computers for minimal money. But be prepared for them to have sticker shock when it comes to salaries. Maybe it&amp;rsquo;s getting a little better now, but I&amp;rsquo;ve often found that biologists want to pay you about half of what you&amp;rsquo;re worth on the market. Their pay level is just lower than that in computer programming. When you get to that point, you might have to be a bit hardnosed during salary negotiations to maintain your children&amp;rsquo;s nutritional requirements.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t know of a book or training program that&amp;rsquo;s specifically targeted at programmers interested in learning about biology. However, many universities have started offering bioinformatics courses, training programs, and even degrees, and some of their course offerings are designed for the experienced programmer. You might consider attending one of the major bioinformatics conferences. However, there will be a tutorial aimed at you in the &lt;a href=&#34;http://conferences.oreilly.com/biocon/&#34;&gt;upcoming O&amp;rsquo;Reilly bioinformatics conference&lt;/a&gt; &amp;ndash; indeed, the main focus of that conference is from the programming side more than the biology side.&lt;/p&gt;

&lt;p&gt;Apart from the upcoming O&amp;rsquo;Reilly conference already mentioned, there is the &lt;a href=&#34;http://www.ismb.org&#34;&gt;ISMB conference&lt;/a&gt;, the largest in the bioinformatics field, which is in Calgary this coming summer; a good place to meet people and learn. It will also play host to the Bioperl yearly meeting, which is directly on target. Actually, if you check out the presenters at the ISMB, RECOMB or O&amp;rsquo;Reilly conferences, then you will find computer science people who are specializing in biology-related problems, as well as biologists specializing in infomatics, and many of these will be many of these will be lab heads or managers who maintain staffs of programmers.
The thing about biology is that it&amp;rsquo;s a very large area. Most researchers stake out a claim to some particular system &amp;ndash; say, the regulation of nervous system development in the fly &amp;ndash; and work there. So it&amp;rsquo;s hard to really prepare yourself for the particular biology you might find on the job. The &amp;ldquo;Recombinant DNA&amp;rdquo; book will give you an overview of some of the more important techniques that are commonly used in most labs.&lt;/p&gt;

&lt;h3 id=&#34;a-taste-of-molecular-biology&#34;&gt;A Taste of Molecular Biology&lt;/h3&gt;

&lt;p&gt;Now that I&amp;rsquo;ve given you my general take on how a Perl programmer could move into biology research, I&amp;rsquo;ll turn my attention to two basic molecular biology techniques that are fundamental in biology research, as for instance in the Human Genome Project: restriction enzymes and cloning with PCR.&lt;/p&gt;

&lt;p&gt;First, we have to point out that the two most important biological molecules, DNA and proteins, are both polymers, which are chains of smaller building block molecules. DNA is made out of four building blocks, the nucleotides or &amp;ldquo;bases&amp;rdquo;; proteins are made from 20 amino acids. DNA has a regular structure, usually the famous double helix of two complementary strands intertwined; whereas proteins fold up in a wide variety of ways that have an important effect on what the proteins are able to do inside the cell. DNA is the primary genetic material that transmits traits to succeeding generations. Finally, DNA contains the coded templates from which proteins are made; and proteins accomplish much of the work of the cell.&lt;/p&gt;

&lt;p&gt;One important class of proteins are &lt;em&gt;enzymes&lt;/em&gt;, which promote certain specific chemical reactions in the cell. In 1978 the Nobel Prize was awarded to Werner Arber, Daniel Nathans, and Hamilton Smith for their discovery and work on &lt;em&gt;restriction enzymes&lt;/em&gt; in the 1960s and early 1970s. Restriction enzymes are a large group of enzymes that have the useful property of cutting DNA at specific locations called &lt;em&gt;restriction sites&lt;/em&gt;. This has been exploited in several important ways. It has been an important technique in &lt;em&gt;fingerprinting&lt;/em&gt; DNA, as is used in forensic science to identify individuals. It has been instrumental in developing &lt;em&gt;physical maps&lt;/em&gt;, which are known positions along DNA and are used to zero in on the location of genes, and also serve as a reference point for the lengthy process of the determination of the entire sequence of bases in DNA.&lt;/p&gt;

&lt;p&gt;Restriction enzymes are fundamental to modern biological research. To learn more about them, you could go to the &lt;a href=&#34;http://www.neb.com/rebase%20&#34;&gt;REBASE&lt;/a&gt; restriction enzyme database where detailed information about all known restriction enzymes is collected. Many of them are easily ordered from supply houses for use in the lab.&lt;/p&gt;

&lt;p&gt;One of the most common restriction enzymes is called EcoRI. When it finds the six bases GAATTC along a strand of DNA, it cleaves the DNA.&lt;/p&gt;

&lt;p&gt;The other main technique I want to introduce is one already mentioned: PCR, or the polymerase chain reaction. This is the most important way that DNA samples are cloned, that is, have copies made. PCR is very powerful at this; in a short time many millions of copies of a stretch of DNA can be created, at which point there is enough of a &amp;ldquo;sample&amp;rdquo; of the DNA to perform other molecular biology experiments, such as determining what exactly is the sequence of bases in the DNA (as has been accomplished for humans in the Human Genome Project.)&lt;/p&gt;

&lt;p&gt;PCR also won a Nobel prize for its invention, by Kary Mullis in 1983. The basic idea is quite simple. We&amp;rsquo;ve mentioned that the two intertwined strands of the double helix of DNA are complementary. They are different, but given one strand we know what the other strand is, as they always pair in a specific way. PCR exploits this property.&lt;/p&gt;

&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s clear that a short article is not going to get very far in introducing a major science such as biology. But I hope I&amp;rsquo;ve given you enough pointers to enable you to make a good start at learning about this explosive science, and about how a Perl programmer might be able to bring needed skills to the great challenge of understanding life and curing disease.&lt;/p&gt;

&lt;p&gt;In the 10 years I&amp;rsquo;ve been working in biology, I&amp;rsquo;ve found it to be a really exciting field, very stimulating intellectually; and I&amp;rsquo;ve found that going to work to try to help to cure cancer, Alzheimer&amp;rsquo;s disease, and others, has been very satisfying emotionally.&lt;/p&gt;

&lt;p&gt;I wish you the very best of luck. If you make it to the &lt;a href=&#34;http://conferences.oreilly.com/biocon/&#34;&gt;O&amp;rsquo;Reilly conference&lt;/a&gt;, please look me up!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parsing Protein Domains with Perl</title>
      <link>http://localhost:1313/pub/2001/11/16/perlbio2.html/</link>
      <pubDate>Fri, 16 Nov 2001 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2001/11/16/perlbio2.html/</guid>
      <description>

&lt;p&gt;The Perl programming language is popular with biologists because of its practicality. In my book, &lt;a href=&#34;http://www.oreilly.com/catalog/begperlbio/&#34;&gt;Beginning Perl for Bioinformatics&lt;/a&gt;, I demonstrate how many of the things biologists want to write programs for are readily&amp;ndash;even enjoyably&amp;ndash;accomplished with Perl.&lt;/p&gt;

&lt;p&gt;My book teaches biologists how to program in Perl, even if they have never programmed before. This article will use Perl at the level found in the middle-to-late chapters in my book, after some of the basics have been learned. However, this article can be read by biologists who do not (yet) know any programming. They should be able to skim the program code in this article, only reading the comments, to get a general feel for how Perl is used in practical applications, using real biological data.&lt;/p&gt;

&lt;p&gt;Biological data on computers tends to be either in structured ASCII flat files&amp;ndash;that is to say, in plain-text files&amp;ndash;or in relational databases. Both of these data sources are easy to handle with Perl programs. For this article, I will discuss one of the flat-file data sources, the &lt;a href=&#34;http://ca.expasy.org/prosite/&#34;&gt;Prosite database&lt;/a&gt;, which contains valuable biological information about protein domains. I will demonstrate how to use Perl to extract and use the protein domain information. In &lt;em&gt;Beginning Perl for Bioinformatics&lt;/em&gt; I also show how to work with several other similar data sources, including GenBank (Genetic Data Bank), PDB (Protein DataBank), BLAST (Basic Local Alignment Search Tool) output files, and REBASE (Restriction Enzyme Database).&lt;/p&gt;

&lt;h3 id=&#34;what-is-prosite&#34;&gt;What is Prosite?&lt;/h3&gt;

&lt;p&gt;Prosite stands for &amp;ldquo;A Dictionary of Protein Sites and Patterns.&amp;rdquo; To learn more about the fascinating biology behind Prosite, visit the &lt;a href=&#34;http://ca.expasy.org/cgi-bin/lists?prosuser.txt&#34;&gt;Prosite User Manual&lt;/a&gt;. Here&amp;rsquo;s an introductory description of Prosite from the user manual:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Prosite is a method of determining what is the function of uncharacterized proteins translated from genomic or cDNA sequences. It consists of a database of biologically significant sites and patterns formulated in such a way that with appropriate computational tools it can rapidly and reliably identify to which known family of protein (if any) the new sequence belongs.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;In some cases, the sequence of an unknown protein is too distantly related to any protein of known structure to detect its resemblance by overall sequence alignment. However, it can be identified by the occurrence in its sequence of a particular cluster of residue types, variously known as a pattern, a motif, a signature, or a fingerprint. These motifs arise because of particular requirements on the structure of specific regions of a protein, which may be important, for example, for their binding properties, or for their enzymatic activity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prosite is available as a set of plain-text files that provide the data, plus documentation. The &lt;a href=&#34;http://www.expasy.ch/prosite&#34;&gt;Prosite home page&lt;/a&gt; provides a user interface that allows you to query the database and examine the documentation. The database can also be obtained for local installation from the &lt;a href=&#34;ftp://www.expasy.ch/databases/prosite&#34;&gt;Prosite ftp site&lt;/a&gt;. Its use is free of charge for noncommercial users.
There is some fascinating and important biology involved here; and in the programs that follow there are interesting and useful Perl programming techniques. See the Prosite User Manual for the biology background, and &lt;em&gt;Beginning Perl for Bioinformatics&lt;/em&gt; for the programming background. Or just keep reading to get a taste for what is possible when you combine programming skills with biological data.&lt;/p&gt;

&lt;h3 id=&#34;prosite-data&#34;&gt;Prosite Data&lt;/h3&gt;

&lt;p&gt;The Prosite data can be downloaded to your computer. It is in the ASCII flat file called &lt;a href=&#34;ftp://ca.expasy.org/databases/prosite/release_with_updates/prosite.dat&#34;&gt;prosite.dat&lt;/a&gt; and is more than 4MB in size. A small version of this file created for this article, called &lt;em&gt;prosmall.dat&lt;/em&gt;, is available &lt;a href=&#34;http://perl.com/2001/11/16/examples/prosmall.dat&#34;&gt;here&lt;/a&gt;. This version of the data has just the first few records from the complete file, making it easier for you to download and test, and it&amp;rsquo;s the file that we&amp;rsquo;ll use in the code discussed later in this article.&lt;/p&gt;

&lt;p&gt;Prosite also provides an accompanying data file, &lt;a href=&#34;ftp://ca.expasy.org/databases/prosite/release_with_updates/prosite.doc&#34;&gt;prosite.doc&lt;/a&gt;, which contains documentation for all the records in &lt;em&gt;prosite.dat&lt;/em&gt;. Though we will not use it for this article, I do recommend you look at it and think about how to use the information along with the code presented here if you plan on doing more with Prosite.&lt;/p&gt;

&lt;blockquote&gt;
&lt;hr /&gt;

&lt;blockquote&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;p&gt;&lt;a href=&#34;http://conferences.oreilly.com/biocon/&#34;&gt;&lt;img src=&#34;http://oreilly.com/graphics_new/biocon_logo.gif&#34; alt=&#34;O&amp;#39;Reilly Bioinformatics Technology Conference&#34; width=&#34;120&#34; height=&#34;70&#34; /&gt;&lt;/a&gt; James Tisdall will be speaking at O&#39;Reilly&#39;s first Bioinformatics Technology Conference, January 28-31, 2002, in Tuscon, Arizona. For more information visit &lt;a href=&#34;http://conferences.oreilly.com/biocon/&#34;&gt;Bioinformatics Conference Web site&lt;/a&gt;.&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Prosite data in &lt;a href=&#34;ftp://ca.expasy.org/databases/prosite/release_with_updates/prosite.dat&#34;&gt;prosite.dat&lt;/a&gt; (or our much smaller test file &lt;em&gt;prosmall.dat&lt;/em&gt;) is organized in &amp;ldquo;records,&amp;rdquo; each of which consists of several lines, and which always include an ID line and a termination line containing &amp;ldquo;//&amp;rdquo;. The Prosite lines all begin with a two-character code that specifies the kind of data that appears on that line. Here&amp;rsquo;s a breakdown of all the possible line types that a record may contain from the &lt;a href=&#34;http://ca.expasy.org/cgi-bin/lists?prosuser.txt&#34;&gt;Prosite User Manual&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;**
ID
Identification (Begins each entry; one per entry)&lt;/p&gt;

&lt;p&gt;AC
Accession number (one per entry)&lt;/p&gt;

&lt;p&gt;DT
Date (one per entry)&lt;/p&gt;

&lt;p&gt;DE
Short description (one per entry)&lt;/p&gt;

&lt;p&gt;PA
Pattern (&amp;gt;=0 per entry)&lt;/p&gt;

&lt;p&gt;MA
Matrix/profile (&amp;gt;=0 per entry)&lt;/p&gt;

&lt;p&gt;RU
Rule (&amp;gt;=0 per entry)&lt;/p&gt;

&lt;p&gt;NR
Numerical results (&amp;gt;=0 per entry)&lt;/p&gt;

&lt;p&gt;CC
Comments (&amp;gt;=0 per entry)&lt;/p&gt;

&lt;p&gt;DR
Cross references to SWISS-PROT (&amp;gt;=0 per entry)&lt;/p&gt;

&lt;p&gt;3D
Cross references to PDB (&amp;gt;=0 per entry)&lt;/p&gt;

&lt;p&gt;DO
Pointer to the documentation file (one per entry)&lt;/p&gt;

&lt;p&gt;//
Termination line (Ends each entry; one per entry)&lt;/p&gt;

&lt;p&gt;Each of these line types has certain kinds of information that are formatted in a specific manner, as is detailed in the Prosite documentation.&lt;/p&gt;

&lt;h3 id=&#34;prosite-patterns&#34;&gt;Prosite Patterns&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s look specifically at the Prosite patterns. These are presented in a kind of mini-language that describes a set of short stretches of protein that may be a region of known biological activity. Here&amp;rsquo;s the description of the pattern &amp;ldquo;language&amp;rdquo; from the &lt;a href=&#34;http://ca.expasy.org/cgi-bin/lists?prosuser.txt&#34;&gt;Prosite User Manual&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The PA (PAttern) lines contains the definition of a Prosite pattern. The patterns are described using the following conventions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The standard IUPAC one-letter codes for the amino acids are used.&lt;/li&gt;
&lt;li&gt;The symbol `x&amp;rsquo; is used for a position where any amino acid is accepted.&lt;/li&gt;
&lt;li&gt;Ambiguities are indicated by listing the acceptable amino acids for a given position, between square parentheses `[ ]&amp;rsquo;. For example: [ALT] stands for Ala or Leu or Thr.&lt;/li&gt;
&lt;li&gt;Ambiguities are also indicated by listing between a pair of curly brackets `{ }&amp;rsquo; the amino acids that are not accepted at a given position. For example: {AM} stands for any amino acid except Ala and Met.&lt;/li&gt;
&lt;li&gt;Each element in a pattern is separated from its neighbor by a `-&amp;lsquo;.&lt;/li&gt;
&lt;li&gt;Repetition of an element of the pattern can be indicated by following that element with a numerical value or a numerical range between parenthesis. Examples: x(3) corresponds to x-x-x, x(2,4) corresponds to x-x or x-x-x or x-x-x-x.&lt;/li&gt;
&lt;li&gt;When a pattern is restricted to either the N- or C-terminal of a sequence, that pattern either starts with a `&amp;lt;&amp;rsquo; ` a ends or respectively symbol with&amp;gt;&amp;rsquo; symbol.&lt;/li&gt;
&lt;li&gt;A period ends the pattern.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;perl-subroutine-to-translate-prosite-patterns-into-perl-regular-expressions&#34;&gt;Perl Subroutine to Translate Prosite Patterns into Perl Regular Expressions&lt;/h4&gt;

&lt;p&gt;In order to use this pattern data in our Perl program, we need to translate the Prosite patterns into Perl regular expressions, which are the main way that you search for patterns in data in Perl. For the sake of this article I will assume that you know the basic regular expression syntax. (If not, just read the program comments, and skip the Perl regular expressions.) As an example of what the following subroutine does, it will translate the Prosite pattern &lt;code&gt;[AC]-x-V-x(4)-{ED}.&lt;/code&gt; into the equivalent Perl regular expression &lt;code&gt;[AC].V.{4}[^ED]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Here, then, is our first Perl code, the subroutine &lt;code&gt;PROSITE_2_regexp&lt;/code&gt;, to translate the Prosite patterns to Perl regular expressions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#
# Calculate a Perl regular expression
#  from a PROSITE pattern
#
sub PROSITE_2_regexp {

  #
  # Collect the PROSITE pattern
  #
  my($pattern) = @_;

  #
  # Copy the pattern to a regular expression
  #
  my $regexp = $pattern;

  #
  # Now start translating the pattern to an
  #  equivalent regular expression
  #

  #
  # Remove the period at the end of the pattern
  #
  $regexp =~ s/.$//;

  #
  # Replace &#39;x&#39; with a dot &#39;.&#39;
  #
  $regexp =~ s/x/./g;

  #
  # Leave an ambiguity such as &#39;[ALT]&#39; as is.
  #   However, there are two patterns [G&amp;gt;] that need
  #   special treatment (and the PROSITE documentation
  #   is a bit vague, perhaps).
  #
  $regexp =~ s/\[G\&amp;gt;\]/(G|\$)/;

  #
  # Ambiguities such as {AM} translate to [^AM].
  #
  $regexp =~ s/{([A-Z]+)}/[^$1]/g;

  #
  # Remove the &#39;-&#39; between elements in a pattern
  #
  $regexp =~ s/-//g;

  #
  # Repetitions such as x(3) translate as x{3}
  #
  $regexp =~ s/\((\d+)\)/{$1}/g;

  #
  # Repetitions such as x(2,4) translate as x{2,4}
  #
  $regexp =~ s/\((\d+,\d+)\)/{$1}/g;

  #
  # &#39;&amp;lt;&#39; &amp;quot;beginning # $regexp=&amp;quot;~&amp;quot; &#39; &#39;^&#39; ; \&amp;lt; ^ becomes for of s sequence&amp;quot;&amp;gt;&#39; becomes &#39;$&#39; for &amp;quot;end of sequence&amp;quot;
  #
  $regexp =~ s/\&amp;gt;/\$/;

  #
  # Return the regular expression
  #
  return $regexp;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Subroutine &lt;code&gt;PROSITE_2_regexp&lt;/code&gt; takes the Prosite pattern and translates its parts step by step into the equivalent Perl regular expression, as explained in the comments for the subroutine. If you do not know Perl regular expression syntax at this point, just read the comments&amp;ndash;that is, the lines that start with the # character. That will give you the general idea of the subroutine, even if you don&amp;rsquo;t know any Perl at all.&lt;/p&gt;

&lt;blockquote&gt;
&lt;hr /&gt;

&lt;blockquote&gt;
&lt;p&gt;Learn more about the power of regular expressions from O&amp;rsquo;Reilly&amp;rsquo;s &lt;a href=&#34;http://www.oreilly.com/catalog/regex/&#34;&gt;Mastering Regular Expressions: Powerful Techniques for Perl and Other Tools&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;perl-subroutine-to-parse-prosite-records-into-their-line-types&#34;&gt;Perl Subroutine to Parse Prosite Records into Their Line Types&lt;/h4&gt;

&lt;p&gt;The other task we need to accomplish is to parse the various types of lines, so that, for instance, we can get the ID and the PA pattern lines easily. The next subroutine accomplishes this task: given a Prosite record, it returns a hash with the lines of each type indexed by a key that is the two-character &amp;ldquo;line type&amp;rdquo;. The keys we&amp;rsquo;ll be interested in are the ID key for the line that has the identification information; and the PA key for the line(s) that have the pattern information.&lt;/p&gt;

&lt;p&gt;This &amp;ldquo;get_line_types&amp;rdquo; subroutine does more than we need. It makes a hash index on all the line types, not just the ID and PA lines that we&amp;rsquo;ll actually use here. But that&amp;rsquo;s OK. The subroutine is short and simple enough, and we may want to use it later to do things with some of the other types of lines in a Prosite record.&lt;/p&gt;

&lt;p&gt;By building our hash to store the lines of a record, we can extract any of the data lines from the record that we like, just by giving the line type code (such as ID for identification number). We can use this hash to extract two line types that will interest us here, the ID identifier line and the PA pattern line. Then, by translating the Prosite pattern into a Perl regular expression (using our first subroutine), we will be in a position to actually look for all the patterns in a protein sequence. In other words, we will have extracted the pattern information and made it available for use in our Perl program, so we can search for the patterns in the protein sequence.&lt;/p&gt;

&lt;blockquote&gt;
&lt;hr /&gt;

&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re interested in learning Perl, don&amp;rsquo;t miss O&amp;rsquo;Reilly&amp;rsquo;s best-selling &lt;a href=&#34;http://www.oreilly.com/catalog/lperl3/&#34;&gt;Learning Perl, 3rd Edition&lt;/a&gt;, which has been updated to cover Perl version 5.6 and rewritten to reflect the needs of programmers learning Perl today. For a complete list of O&amp;rsquo;Reilly&amp;rsquo;s books on Perl, go to &lt;a href=&#34;http://perl.oreilly.com&#34;&gt;perl.oreilly.com&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here, then, is our second subroutine, which accepts a Prosite record, and returns a hash which has the lines of the record indexed by their line types:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#
# Parse a PROSITE record into &amp;quot;line types&amp;quot; hash
#
sub get_line_types {

  #
  # Collect the PROSITE record
  #
  my($record) = @_;

  #
  # Initialize the hash
  #   key   = line type
  #   value = lines
  #
  my %line_types_hash = ();

  #
  # Split the PROSITE record to an array of lines
  #
  my @records = split(/\n/,$record);

  #
  # Loop through the lines of the PROSITE record
  #
  foreach my $line (@records) {

    #
    # Extract the 2-character name
    # of the line type
    #
    my $line_type = substr($line,0,2);

    #
    # Append the line to the hash
    # indexed by this line type
    #
    (defined $line_types_hash{$line_type})
    ?  ($line_types_hash{$line_type} .= $line)
    :  ($line_types_hash{$line_type} = $line);
  }

  #
  # Return the hash
  #
  return %line_types_hash;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;main-program&#34;&gt;Main Program&lt;/h3&gt;

&lt;p&gt;Now let&amp;rsquo;s see the code at work. The following program uses the subroutines we&amp;rsquo;ve just defined to read in the Prosite records one at a time from the database in the flat file prosmall.txt. It then separates the different kinds of lines (such as &amp;ldquo;PA&amp;rdquo; for pattern), and translates the patterns into regular expressions, using the subroutine PROSITE_2_regexp we already wrote. Finally, it searches for the regular expressions in the protein sequence, and reports the position of the matched pattern in the sequence.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/perl
#
# Parse patterns from the PROSITE database, and
# search for them in a protein sequence
#

#
# Turn on useful warnings and constraints
#
use strict;
use warnings;

#
# Declare variables
#

#
# The PROSITE database
#
my $prosite_file = &#39;prosmall.dat&#39;;

#
# A &amp;quot;handle&amp;quot; for the opened PROSITE file
#
my $prosite_filehandle;

#
# Store each PROSITE record that is read in
#
my $record = &#39;&#39;;

#
# The protein sequence to search
# (use &amp;quot;join&amp;quot; and &amp;quot;qw&amp;quot; to keep line length short)
#
my $protein = join &#39;&#39;, qw(
MNIDDKLEGLFLKCGGIDEMQSSRTMVVMGGVSG
QSTVSGELQDSVLQDRSMPHQEILAADEVLQESE
MRQQDMISHDELMVHEETVKNDEEQMETHERLPQ
);

#
# open the PROSITE database or exit the program
#
open($prosite_filehandle, $prosite_file)
 or die &amp;quot;Cannot open PROSITE file $prosite_file&amp;quot;;

#
# set input separator to termination line //
#
$/ = &amp;quot;//\n&amp;quot;;

#
# Loop through the PROSITE records
#
while($record = &amp;lt;$prosite_filehandle&amp;gt;) {

  #
  # Parse the PROSITE record into its &amp;quot;line types&amp;quot;
  #
  my %line_types = get_line_types($record);

  #
  # Skip records without an ID (the first record)
  #
  defined $line_types{&#39;ID&#39;} or next;

  #
  # Skip records that are not PATTERN
  # (such as MATRIX or RULE)
  #
  $line_types{&#39;ID&#39;} =~ /PATTERN/ or next;

  #
  # Get the ID of this record
  #
  my $id = $line_types{&#39;ID&#39;};
  $id =~ s/^ID   //;
  $id =~ s/; .*//;

  #
  # Get the PROSITE pattern from the PA line(s)
  #
  my $pattern = $line_types{&#39;PA&#39;};
  # Remove the PA line type tag(s)
  $pattern =~ s/PA   //g;

  #
  # Calculate the Perl regular expression
  # from the PROSITE pattern
  #
  my $regexp =  PROSITE_2_regexp($pattern);

  #
  # Find the PROSITE regular expression patterns
  # in the protein sequence, and report
  #
  while ($protein =~ /$regexp/g) {
    my $position = (pos $protein) - length($&amp;amp;) +1;
    print &amp;quot;Found $id at position $position\n&amp;quot;;
    print &amp;quot;   match:   $&amp;amp;\n&amp;quot;;
    print &amp;quot;   pattern: $pattern\n&amp;quot;;
    print &amp;quot;   regexp:  $regexp\n\n&amp;quot;;
  }
}

#
# Exit the program
#
exit;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This program is available online as the file &lt;a href=&#34;http://perl.com/2001/11/16/examples/parse_prosite&#34;&gt;parse_prosite&lt;/a&gt;. The tiny example Prosite database is available as the file &lt;a href=&#34;http://perl.com/2001/11/16/examples/prosmall.dat&#34;&gt;prosmall.dat&lt;/a&gt;. If you save these files on your (Unix, Linux, Macintosh, or Windows) computer, you can enter the following command at your command-line prompt (in the same folder in which you saved the two files):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;% perl parse_prosite
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and it will produce the following output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Found PKC_PHOSPHO_SITE at position 22
   match:   SSR
   pattern: [ST]-x-[RK].
   regexp:  [ST].[RK]

Found PKC_PHOSPHO_SITE at position 86
   match:   TVK
   pattern: [ST]-x-[RK].
   regexp:  [ST].[RK]

Found CK2_PHOSPHO_SITE at position 76
   match:   SHDE
   pattern: [ST]-x(2)-[DE].
   regexp:  [ST].{2}[DE]

Found MYRISTYL at position 30
   match:   GGVSGQ
   pattern: G-{EDRKHPFYW}-x(2)-[STAGCN]-{P}.
   regexp:  G[^EDRKHPFYW].{2}[STAGCN][^P]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you see, our short program goes through the Prosite database one record at a time, parsing each record according to the types of lines within it. If the record has an ID and a pattern, it then extracts them, creates a Perl regular expression from the pattern, and finally searches in a protein sequence for the regular expression, reporting on the patterns found.&lt;/p&gt;

&lt;h3 id=&#34;the-next-step&#34;&gt;The Next Step&lt;/h3&gt;

&lt;p&gt;This article has shown you how to take biological data from the Prosite database and use it in your own programs. With this ability, you can write programs specific to your particular research needs.&lt;/p&gt;

&lt;p&gt;Many kinds of data discovery are possible: you could combine searches for Prosite patterns with some other computation. For instance, you may want to also search the associated genomic DNA or cDNA for restriction sites surrounding a particular Prosite pattern in the translated protein, in preparation for cloning.&lt;/p&gt;

&lt;blockquote&gt;
&lt;hr /&gt;

&lt;blockquote&gt;
&lt;p&gt;James Tisdall has also written &lt;a href=&#34;http://www.oreilly.com/news/perlbio_1001.html&#34;&gt;Why Biologists Want to Program Computers&lt;/a&gt; for &lt;em&gt;oreilly.com&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;/blockquote&gt;

&lt;p&gt;While such programs are interesting in their own right, their importance in laboratory research really lies in the fact that their use can save enormous amounts of time; time which can then be used for other, less routine, tasks on which biological research critically depends.&lt;/p&gt;

&lt;p&gt;This article gives an example of using Perl to extract and use data from a flat file database, of which there are many in biological research. In fact, some of the most important biological databases are in flat file format, including GenBank and PDB, the primary databases for DNA sequence information and for protein structures.&lt;/p&gt;

&lt;p&gt;With the ability to write your own programs, the true power of bioinformatics can be applied in your lab. Learning the Perl programming language can give you a direct entry into this valuable new laboratory technique.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;O&amp;rsquo;Reilly &amp;amp; Associates recently released (October 2001) &lt;a href=&#34;http://oreilly.com/catalog/begperlbio/&#34;&gt;Beginning Perl for Bioinformatics&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.oreilly.com/catalog/begperlbio/chapter/ch10.html&#34;&gt;Sample Chapter 10, GenBank&lt;/a&gt;, is available free online.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can also look at the &lt;a href=&#34;http://www.oreilly.com/catalog/begperlbio/toc.html&#34;&gt;Table of Contents&lt;/a&gt;, the &lt;a href=&#34;http://www.oreilly.com/catalog/begperlbio/toc.html&#34;&gt;Index&lt;/a&gt;, and the &lt;a href=&#34;http://www.oreilly.com/catalog/begperlbio/desc.html&#34;&gt;Full Description&lt;/a&gt; of the book.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For more information, or to order the book, &lt;a href=&#34;http://oreilly.com/catalog/begperlbio/&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Quantum::Entanglement</title>
      <link>http://localhost:1313/pub/2001/08/08/quantum.html/</link>
      <pubDate>Wed, 08 Aug 2001 00:00:00 -0800</pubDate>
      
      <guid>http://localhost:1313/pub/2001/08/08/quantum.html/</guid>
      <description>

&lt;h3 id=&#34;span-id-there-is-more-than-one-world-in-which-to-do-it-there-is-more-than-one-world-in-which-to-do-it-span&#34;&gt;&lt;span id=&#34;there is more than one world (in which) to do it&#34;&gt;There Is More Than One World (In Which) To Do It&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;With the possible exception of many physicists, quantum mechanics is one of the stranger things to have emerged from science over the last hundred years. It has led the way to new understanding of a diverse range of fundamental physical phenomena and, should recent developments prove fruitful, could also lead to an entirely new mode of computation where previously intractable problems find themselves open to easy solution.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;Quantum::Entanglement&lt;/code&gt; module attempts to port some of the functionality of the universe into Perl. Variables can be prepared in a superposition of states, where they take many values at once, and when observed during the course of a program, will collapse to have a single value. If variables interact then their fates are linked so that when one is observed and forced to collapse the others will also collapse at the moment of observation.&lt;/p&gt;

&lt;p&gt;It is quite hard to provide a complete version of quantum mechanics in Perl, so we need to make some simplifications. Instead of solving thousands of equations each time we want to do something, we will forget entirely about eigen-functions, Hermitian operators and other mathematical hurdles. This still leaves us with plenty of ways to make Perl behave in a thoroughly unpredictable fashion.&lt;/p&gt;

&lt;h3 id=&#34;span-id-the-entangle-function-the-entangle-function-span&#34;&gt;&lt;span id=&#34;the entangle() function&#34;&gt;The &lt;code&gt;entangle()&lt;/code&gt; function&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;Quantum::Entanglement&lt;/code&gt; module adds an &lt;code&gt;entangle()&lt;/code&gt; function to Perl, this takes a list of amplitudes and values and returns a scalar in a superposition of values; saying&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $die = entangle( 1=&amp;gt;1, 1=&amp;gt;2, 1=&amp;gt;3, 1=&amp;gt;4, 1=&amp;gt;5, 1=&amp;gt;6);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;creates a superposition of the values &lt;code&gt;1..6&lt;/code&gt;. From now on, &lt;code&gt;$die&lt;/code&gt; acts as if it has every one of those values at the same time as long as we do not try to find out exactly which one.&lt;/p&gt;

&lt;h3 id=&#34;span-id-observation-and-collapse-in-perl-observation-and-collapse-in-perl-span&#34;&gt;&lt;span id=&#34;observation and collapse in perl&#34;&gt;Observation and Collapse in Perl&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;We now need to decide what happens when we observe our variable, and what we mean by &lt;em&gt;observe&lt;/em&gt;? Taking a broad definition as being ``anything that reveals the values which a variable has&amp;rdquo; seems about right. Perl provides us with many ways of doing this, there are the obvious acts of printing out a variable or testing it for truth, but even operators such as &lt;code&gt;eq&lt;/code&gt; or &lt;code&gt;&amp;lt;=&lt;/code&gt; tell us something.&lt;/p&gt;

&lt;p&gt;How do we decide which way a variable collapses? Well, each possible value has an associated probability amplitude, so all we need to do is build up a list of distinct outcomes, add up the amplitudes for each one, square the result, then use this to bias the value (or values) to which the variable collapses.&lt;/p&gt;

&lt;p&gt;As every coefficient of the superposition in &lt;code&gt;$die&lt;/code&gt; is equal to 1,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; print &amp;quot;You rolled a $die.\n&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;will output &lt;code&gt;You rolled a 1.&lt;/code&gt; or &lt;code&gt;You rolled a 2.&lt;/code&gt; and so on, each for one sixth of the time.&lt;/p&gt;

&lt;h3 id=&#34;span-id-entanglement-and-simple-complex-logic-entanglement-and-simple-complex-logic-span&#34;&gt;&lt;span id=&#34;entanglement and simple complex logic&#34;&gt;Entanglement and Simple Complex Logic&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;Whenever superposed variables interact, or are involved in calculations, the results of these as well as the variables themselves become entangled. This means that they will all collapse at the same time, so as to remain consistent with their history. This emulates the entanglement, or ``spooky action at a distance&amp;rdquo;, which so worried Einstein.&lt;/p&gt;

&lt;h3 id=&#34;span-id-complex-amplitudes-and-entanglement-in-perl-complex-amplitudes-and-entanglement-in-perl-span&#34;&gt;&lt;span id=&#34;complex amplitudes and entanglement in perl&#34;&gt;Complex Amplitudes and Entanglement in Perl&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;If we can have plain numbers as the coefficients of our superpositions it seems sensible that we could also use complex numbers. Although instead of just squaring the number when working out our probability, we need to square the &lt;em&gt;size&lt;/em&gt; of the number. (eg. &lt;code&gt;|1+2i|**2 == 5 == |1-2i|**2&lt;/code&gt;.)&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;Quantum::Entanglement&lt;/code&gt; module allows subroutines to create new states (amplitude-value pairs) based on the current set of states by using the function &lt;code&gt;q_logic&lt;/code&gt;. This takes as an argument a subroutine which is presented each state in turn and must return a new set of states constructed from these.&lt;/p&gt;

&lt;p&gt;Starting our program with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; #!/usr/bin/perl -w

 use Quantum::Entanglement qw(:DEFAULT :complex);

 $Quantum::Entanglement::destroy = 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;so that we have access to the constants defined by &lt;code&gt;Math::Complex&lt;/code&gt; and turn off the memory management performed by the module (as this causes some information to be lost, which will be important later). We then define a subroutine to return the value it receives and its logical negation, their coefficients are those of the original state multiplied by &lt;code&gt;i/sqrt(2)&lt;/code&gt; and &lt;code&gt;1/sqrt(2)&lt;/code&gt; respectively:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; sub root_not {

   my ($prob, $val) = @_;

   return( $prob * i / sqrt(2) , $val,

               $prob / sqrt(2) , !$val );

 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then create a superposition which we &lt;em&gt;know&lt;/em&gt; is equal to 0 and feed it through our &lt;code&gt;root_not()&lt;/code&gt; once:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; my $var = entangle(1 =&amp;gt; 0);

 $var = q_logic(\&amp;amp;root_not, $var);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the variable is now in a superposition of two possible values, 0 and 1, with coefficients of &lt;code&gt;i/sqrt(2)&lt;/code&gt; and &lt;code&gt;1/sqrt(2)&lt;/code&gt; respectively. We now make our variable interact, storing the result in &lt;code&gt;$peek&lt;/code&gt;. As &lt;code&gt;$var&lt;/code&gt; is in a superposition, every possible value it has participates in the calculation and contributes to the result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; my $peek  = 12*$var;   # $peek and $var become entangled

 $var = q_logic(\&amp;amp;root_not, $var);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then feed &lt;code&gt;$var&lt;/code&gt; through &lt;code&gt;root_not()&lt;/code&gt; one more time and test it for truth. What will happen and what will be the value of &lt;code&gt;$peek&lt;/code&gt;?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; if ($var) { print &amp;quot;\$var is true!\n&amp;quot;; }

 else      { print &amp;quot;\$var is false\n&amp;quot;; }




 print &amp;quot;\$peek is equal to: $peek.\n&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output is always &lt;code&gt;$var is true!&lt;/code&gt; as &lt;code&gt;$var&lt;/code&gt; is in a final superposition of &lt;code&gt;(1/2=&lt;/code&gt;0, i/2=&amp;gt;1, -&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;=&amp;gt;0, i/2=&amp;gt;1)&amp;gt;. You can convince yourself of this by running through the math. What about &lt;code&gt;$peek&lt;/code&gt;? Well, because it interacted with &lt;code&gt;$var&lt;/code&gt; before $&amp;lt;var&amp;gt; collapsed and both possible values that &lt;code&gt;$var&lt;/code&gt; had at that time contributed to its eventual truthfulness, both values of &lt;code&gt;$peek&lt;/code&gt; are still present, we get 0 or 12 each for half the time.&lt;/p&gt;

&lt;p&gt;If we reverse the order in which we examine the variables:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; print &amp;quot;\$peek is equal to: $peek.\n&amp;quot;;




 if ($var) { print &amp;quot;\$var is true!\n&amp;quot;; }

 else      { print &amp;quot;\$var is false\n&amp;quot;; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;we still see &lt;code&gt;peek&lt;/code&gt; being &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;12&lt;/code&gt; but as we collapsed &lt;code&gt;$peek&lt;/code&gt; we must also collapse &lt;code&gt;$var&lt;/code&gt; at the same time. This causes &lt;code&gt;$var&lt;/code&gt; to be in a superposition of &lt;code&gt;(1/2=&lt;/code&gt;0,i/2=&amp;gt;1)&amp;gt; or a superposition of &lt;code&gt;(-1/2=&lt;/code&gt;0,i/2=&amp;gt;1)&amp;gt;, both of which will collapse to 0 half of the time and 1 the other half of the time so that (on average) we see both phrases printed.&lt;/p&gt;

&lt;p&gt;If we try to find the value that &lt;code&gt;$var&lt;/code&gt; had while it was `between&amp;rsquo; the subroutines we force it to have a single value so that after two passes though &lt;code&gt;root_not()&lt;/code&gt; we get random noise, even if we test this after the event. If, on the other hand, we leave it alone it emerges from repeated application of &lt;code&gt;root_not()&lt;/code&gt; as the logical negation of its original value, thus the name of our subroutine.&lt;/p&gt;

&lt;h3 id=&#34;span-id-beneath-the-veil-beneath-the-veil-span&#34;&gt;&lt;span id=&#34;beneath the veil&#34;&gt;Beneath the Veil&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;Although the module is intended to be used as a black box which does the Right Thing (or some close approximation to it), the internals of the code are interesting and reveal many features of Perl which may be useful elsewhere.&lt;/p&gt;

&lt;p&gt;Writing entangled behaviour into Perl presents an interesting challenge; a means of representing a superposition is required, as is some way of allowing different variables to know about each other without creating a twisty maze of references which would stand in the way of garbage collection and lead to a certain programming headache. We also need a means to cause collapse, as well as a robust mechanism for dealing with both real and complex numbers. Thankfully Perl provides a rich set of ingredients which can more than satisfy these requirements without making the job so hard that it becomes impossible.&lt;/p&gt;

&lt;h3 id=&#34;span-id-objective-reality-objective-reality-span&#34;&gt;&lt;span id=&#34;objective reality&#34;&gt;Objective Reality&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;We want to represent something which has many values (and store these somewhere) while making it look like there&amp;rsquo;s only one value present. Objects in Perl are nothing more than scalars that know slightly more than usual. When a new entanglement is created, we create a new object, and return that to the calling program. Deep within the module we have a routine which is similar to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; sub entangle {

   my $self = [ ~ data goes in here ~ ];

   return bless $self, &#39;Quantum::Entanglement&#39;;

 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;exactly how we store the data is covered below. We then turn this into a &amp;lsquo;core&amp;rsquo; function by importing it into the namespace which asked for it.&lt;/p&gt;

&lt;h3 id=&#34;span-id-when-worlds-collide-when-worlds-collide-span&#34;&gt;&lt;span id=&#34;when worlds collide&#34;&gt;When Worlds Collide&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve created a superposition of values and sent it back to our user. What needs to happen when they write something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $talk = entangle( 1=&amp;gt;&#39;Ships&#39;,    1=&amp;gt;&#39;Sealing Wax&#39;,
                   1=&amp;gt;&#39;Cabbages&#39;, 1=&amp;gt;&#39;Kings&#39;        );
 $more = $talk . &#39; yada yada yada&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We want to redefine the meaning of concatenation when an entangled object is involved. Perl lets us do this using the &lt;code&gt;overload&lt;/code&gt; module. Within the &lt;code&gt;Quantum::Entanglement&lt;/code&gt; module we say:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; use overload
        &#39;+&#39;  =&amp;gt; sub { binop(@_, sub{$_[0] + $_[1]} ) },
     # more ...
        &#39;.&#39;  =&amp;gt; sub { binop(@_, sub{$_[0] . $_[1]} ) },
     # yet more ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Whenever someone applies the &amp;lsquo;.&amp;rsquo; operator to our object, a subroutine (in this case an anonymous one) is called to handle the operation, the result of this subroutine is then used as the result of the operation. Because the module provides new behaviours for all of Perl&amp;rsquo;s operations, we write a generic routine to handle &lt;strong&gt;Bi&lt;/strong&gt;nary &lt;strong&gt;N&lt;/strong&gt;on-observational &lt;strong&gt;Op&lt;/strong&gt;erations and pass this the values to operate on along with another anonymous routine (which it will see as a code-ref) so that it knows which operation to perform. This allows us to re-use the code which works out if both operands are objects and if they are reversed and pieces together the data structures we use. &lt;code&gt;binop&lt;/code&gt; is described below.&lt;/p&gt;

&lt;h2 id=&#34;span-id-data-structures-with-hair-data-structures-with-hair-span&#34;&gt;&lt;span id=&#34;data structures with hair&#34;&gt;Data Structures with Hair&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;This module lives and dies on the strength of its data structures. We need to ensure that every variable (or, more correctly, object) knows about all the other superpositions it has been involved with throughout the course of the program without having any direct pointers between them.&lt;/p&gt;

&lt;p&gt;When we create a new variable, we give it the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; sub entangle {
   my $universe = [ [ @_[0,1] ], # amp1, val1
                    [ @_[2,3] ], ...  ];
   my $offsets  = [];
   $var = [ \$universe, 1, \$offsets];
   $offsets-&amp;gt;[0] = \ $var-&amp;gt;[1];
   return bless $var, &#39;Quantum::Entanglement&#39;;
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;there&amp;rsquo;s a lot going on here, so pay attention. &lt;code&gt;$universe&lt;/code&gt; is a list of lists (lol), essentially a two dimensional table with the first two columns holding the amplitudes and values of our superposition. &lt;code&gt;$var&lt;/code&gt; contains a reference which points at a scalar which then points at the universe, rather like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ($var-&amp;gt;[0]) ---&amp;gt; (anonymous scalar) ---&amp;gt; $universe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The second value in &lt;code&gt;$var&lt;/code&gt; is a number which indicates the column in the universe that we need to look at to find the values of our superposition. The last field of &lt;code&gt;$var&lt;/code&gt; again points to a pointer to an array. This array though contains a scalar which points directly at the scalar which holds the number representing the offset of the values in the universe, something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; $var[ (-&amp;gt;X-&amp;gt;universe), (number), (-&amp;gt;Y-&amp;gt;offsets[  ])  ]
                            \------&amp;lt;----&amp;lt;-------/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, when we want this object to interact with another object, all we need to do is make &lt;code&gt;$var-&amp;gt;[0]&lt;/code&gt; and &lt;code&gt;$var-&amp;gt;[1]&lt;/code&gt; for each object end up refering to the same universe. Easy, you might say, given that we have both objects around. But what if one had already interacted with another variable, which we cannot directly access anymore? This is where our extra level of indirection is required. Because each variable contains something which points at something else which then points at their set of values, we merely need to make sure that the &amp;lsquo;something else&amp;rsquo; ends up pointing at the same thing for everything. So, we delve into each object&amp;rsquo;s universe, choosing one which will contain the data for both objects (and thus for all those which have interacted in the past) and move all the data from the other object&amp;rsquo;s universe into it. We then make our middle reference the same for each object.&lt;/p&gt;

&lt;p&gt;Initially,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; universe1 = [[a1,av1],      [a2,av2]      ,... ]

 universe2 = [[b1,bv1,c1,cv1],[b2,bv2,c1,cv1],... ] 

 $var1[ (-&amp;gt;X-&amp;gt;universe1), 1,... ] # we have this object

 $var2[ (-&amp;gt;Y-&amp;gt;universe2), 1,... ] #  and this object

 $var3[ (-&amp;gt;Y-&amp;gt;universe2), 3,... ] # but not this one
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then by pointing Y at universe1 the whole structure of our objects becomes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; universe1 = [[a1,av1,b1,bv1,c1,cv1],[a2,v2,b1,bv1,c1,cv1] ,... ]

 $var1[ (-&amp;gt;X-&amp;gt;universe1), 1,... ] # we have this object

 $var2[ (-&amp;gt;Y-&amp;gt;universe1), 3,... ] #  and this object

 $var3[ (-&amp;gt;Y-&amp;gt;universe1), 5,... ] # but not this one
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To allow every possible value of one variable to interact with every possible value of our other variables, we need to follow a crossing rule so that the rows of our merged universe look like this:&lt;/p&gt;

&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;pre&gt;&lt;code&gt;
 universe1   universe2            result

 a1 av1      b1 bv1 c1 cv1      a1 av9  ]

                            \------&amp;lt;----&amp;lt;-------/&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now, when we want this object to interact with another object, all we need to do is make &lt;code&gt;$var-&amp;gt;[0]&lt;/code&gt; and &lt;code&gt;$var-&amp;gt;[1]&lt;/code&gt; for each object end up refering to the same universe. Easy, you might say, given that we have both objects around. But what if one had already interacted with another variable, which we cannot directly access anymore? This is where our extra level of indirection is required. Because each variable contains something which points at something else which then points at their set of values, we merely need to make sure that the &amp;lsquo;something else&amp;rsquo; ends up pointing at the same thing for everything. So, we delve into each object&amp;rsquo;s universe, choosing one which will contain the data for both objects (and thus for all those which have interacted in the past) and move all the data from the other object&amp;rsquo;s universe into it. We then make our middle reference the same for each object.&lt;/p&gt;

&lt;p&gt;Initially,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; universe1 = [[a1,av1],      [a2,av2]      ,... ]

 universe2 = [[b1,bv1,c1,cv1],[b2,bv2,c1,cv1],... ] 

 $var1[ (-&amp;gt;X-&amp;gt;universe1), 1,... ] # we have this object

 $var2[ (-&amp;gt;Y-&amp;gt;universe2), 1,... ] #  and this object

 $var3[ (-&amp;gt;Y-&amp;gt;universe2), 3,... ] # but not this one
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;then by pointing Y at universe1 the whole structure of our objects becomes&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; universe1 = [[a1,av1,b1,bv1,c1,cv1],[a2,v2,b1,bv1,c1,cv1] ,... ]

 $var1[ (-&amp;gt;X-&amp;gt;universe1), 1,... ] # we have this object

 $var2[ (-&amp;gt;Y-&amp;gt;universe1), 3,... ] #  and this object

 $var3[ (-&amp;gt;Y-&amp;gt;universe1), 5,... ] # but not this one
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To allow every possible value of one variable to interact with every possible value of our other variables, we need to follow a crossing rule so that the rows of our merged universe look like this:&lt;/p&gt;

&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;100%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;pre&gt;&lt;code&gt;
 universe1   universe2            result

 a1 av1      b1 bv1 c1 cv1      a1 av1  b1 bv1  c1 cv1

 a2 av2    * b1 bv1 c2 cv2  ==&amp;gt; a1 av1  b1 bv1  c2 cv2

                                a2 av2  b1 bv1  c1 cv1

                                a2 av2  b1 bv1  c2 cv2&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;so that every row in the first universe is paired with every row of the second. We then need to update the offsets for each variable which has had data moved from one universe to another. As the offsets array contains pointers back to these values, it is easy to increase each one by the correct amount. So, given two entanglements in @_, and a bit of cheating with &lt;code&gt;map&lt;/code&gt;, we can say&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  my $offsets1 = ${$_[0]-&amp;gt;[2]}; # middle-man reference

  my $offsets2 = ${$_[1]-&amp;gt;[2]};

  my $extra = scalar(@{ ${$_[0]-&amp;gt;[0]} });

  push @$offsets1, map {$$_+=$extra; $_} @$offsets2;

  ${$_[1]-&amp;gt;[2]} = $offsets1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and you can&amp;rsquo;t get clearer than that.&lt;/p&gt;

&lt;p&gt;So &lt;code&gt;binop&lt;/code&gt; is written like so (assuming that we can only be given two entangled variables in the correct order, for the full story, read the source):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; sub binop {

    my ($obj1,$obj2,$r,$code) = @_;

    _join($obj1,$obj2);   # ensure universes shared

    my ($os1, $os2) = ($obj1-&amp;gt;[1],$obj2-&amp;gt;[1]);

    my $new = $obj1-&amp;gt;_add(); # new var also shares universe

    foreach my $state (@{${$obj1-&amp;gt;[0]}}) {

       push( @$state, $state-&amp;gt;[$os1-1]*$state-&amp;gt;[$os2-1],

                      &amp;amp;$code( $state-&amp;gt;[$os1], $state-&amp;gt;[$os2] );

    }

    return $new;

 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or, in English: make sure each variable is in the same universe then create a new variable in the same universe. For every row of the universe: add two extra values, the first is the product of the two input amplitudes, the second is the result of our operation on our two input values. Here you see the tremendous value of code reuse, no sane man would write such a routine more than once. Or, more correctly, no man would remain sane if they tried.&lt;/p&gt;

&lt;h3 id=&#34;span-id-london-bridge-is-falling-down-london-bridge-is-falling-down-span&#34;&gt;&lt;span id=&#34;london bridge is falling down&#34;&gt;London Bridge is Falling Down&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;How do we collapse our superpositions so that every entangled variable is affected even though we can only access one of them at once? When we perform an observational operation (&lt;code&gt;if ($var){...}&lt;/code&gt;, say) we simply need to split our universe (table of values) into two groups, those which lead to our operator returning a true value and those that do not. We add up the probability amplitudes for each value in each group, square these to get two numbers and use these to decide which group to keep. To cause our collapse we merely need to delete all the rows of the universe which form the other group which will remove any value of any variable in that row.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;span-id-getting-the-module-getting-the-module-span&#34;&gt;&lt;span id=&#34;getting the module&#34;&gt;Getting the Module&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;The module distribution, like all good things, is available from the CPAN and includes a few short demonstrations of what the module can do, along with plenty of explanation (including Shor&amp;rsquo;s algorithm and the square root of NOT gate outlined above). The source of this, and any other module on the CPAN, is available for inspection. If you have a burning desire to find out how the mystical wheel was first invented, Perl, and its community, will gladly show you.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

